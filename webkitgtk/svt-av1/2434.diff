diff --git a/Source/Lib/ASM_NEON/aom_convolve8_neon.c b/Source/Lib/ASM_NEON/aom_convolve8_neon.c
index 96ab76a59cd3d34e6f03e81f43cc1bf9f787031d..3e5ed3f4124387d2e746ee391f5af28d3c0c6802 100644
--- a/Source/Lib/ASM_NEON/aom_convolve8_neon.c
+++ b/Source/Lib/ASM_NEON/aom_convolve8_neon.c
@@ -188,10 +188,10 @@ static INLINE void convolve8_horiz_4tap_neon(const uint8_t *src, ptrdiff_t src_s
         do {
             uint8x8_t t01[4];
 
-            t01[0] = load_unaligned_u8(src + 0, (int)src_stride);
-            t01[1] = load_unaligned_u8(src + 1, (int)src_stride);
-            t01[2] = load_unaligned_u8(src + 2, (int)src_stride);
-            t01[3] = load_unaligned_u8(src + 3, (int)src_stride);
+            t01[0] = load_u8_4x2(src + 0, (int)src_stride);
+            t01[1] = load_u8_4x2(src + 1, (int)src_stride);
+            t01[2] = load_u8_4x2(src + 2, (int)src_stride);
+            t01[3] = load_u8_4x2(src + 3, (int)src_stride);
 
             int16x8_t s01[4];
             s01[0] = vreinterpretq_s16_u16(vmovl_u8(t01[0]));
diff --git a/Source/Lib/ASM_NEON/aom_convolve8_neon.h b/Source/Lib/ASM_NEON/aom_convolve8_neon.h
index aef605520274703c891965d94cf2a9d97b88f923..7d7b19ad0481a24a73511814754e7e0ed8ed04f9 100644
--- a/Source/Lib/ASM_NEON/aom_convolve8_neon.h
+++ b/Source/Lib/ASM_NEON/aom_convolve8_neon.h
@@ -78,10 +78,10 @@ static INLINE void convolve8_horiz_2tap_neon(const uint8_t *src, ptrdiff_t src_s
 
     if (w == 4) {
         do {
-            uint8x8_t s0 = load_unaligned_u8(src + 0 * src_stride + 0, src_stride);
-            uint8x8_t s1 = load_unaligned_u8(src + 0 * src_stride + 1, src_stride);
-            uint8x8_t s2 = load_unaligned_u8(src + 2 * src_stride + 0, src_stride);
-            uint8x8_t s3 = load_unaligned_u8(src + 2 * src_stride + 1, src_stride);
+            uint8x8_t s0 = load_u8_4x2(src + 0 * src_stride + 0, src_stride);
+            uint8x8_t s1 = load_u8_4x2(src + 0 * src_stride + 1, src_stride);
+            uint8x8_t s2 = load_u8_4x2(src + 2 * src_stride + 0, src_stride);
+            uint8x8_t s3 = load_u8_4x2(src + 2 * src_stride + 1, src_stride);
 
             uint16x8_t sum0 = vmull_u8(s0, f0);
             sum0            = vmlal_u8(sum0, s1, f1);
@@ -168,8 +168,8 @@ static INLINE void convolve8_vert_4tap_neon(const uint8_t *src, ptrdiff_t src_st
     const int16x4_t filter = vshr_n_s16(vld1_s16(filter_y + 2), 1);
 
     if (w == 4) {
-        uint8x8_t t01 = load_unaligned_u8(src + 0 * src_stride, (int)src_stride);
-        uint8x8_t t12 = load_unaligned_u8(src + 1 * src_stride, (int)src_stride);
+        uint8x8_t t01 = load_u8_4x2(src + 0 * src_stride, (int)src_stride);
+        uint8x8_t t12 = load_u8_4x2(src + 1 * src_stride, (int)src_stride);
 
         int16x8_t s01 = vreinterpretq_s16_u16(vmovl_u8(t01));
         int16x8_t s12 = vreinterpretq_s16_u16(vmovl_u8(t12));
@@ -177,10 +177,10 @@ static INLINE void convolve8_vert_4tap_neon(const uint8_t *src, ptrdiff_t src_st
         src += 2 * src_stride;
 
         do {
-            uint8x8_t t23 = load_unaligned_u8(src + 0 * src_stride, (int)src_stride);
-            uint8x8_t t34 = load_unaligned_u8(src + 1 * src_stride, (int)src_stride);
-            uint8x8_t t45 = load_unaligned_u8(src + 2 * src_stride, (int)src_stride);
-            uint8x8_t t56 = load_unaligned_u8(src + 3 * src_stride, (int)src_stride);
+            uint8x8_t t23 = load_u8_4x2(src + 0 * src_stride, (int)src_stride);
+            uint8x8_t t34 = load_u8_4x2(src + 1 * src_stride, (int)src_stride);
+            uint8x8_t t45 = load_u8_4x2(src + 2 * src_stride, (int)src_stride);
+            uint8x8_t t56 = load_u8_4x2(src + 3 * src_stride, (int)src_stride);
 
             int16x8_t s23 = vreinterpretq_s16_u16(vmovl_u8(t23));
             int16x8_t s34 = vreinterpretq_s16_u16(vmovl_u8(t34));
@@ -252,10 +252,10 @@ static INLINE void convolve8_vert_2tap_neon(const uint8_t *src, ptrdiff_t src_st
 
     if (w == 4) {
         do {
-            uint8x8_t s0 = load_unaligned_u8(src + 0 * src_stride, (int)src_stride);
-            uint8x8_t s1 = load_unaligned_u8(src + 1 * src_stride, (int)src_stride);
-            uint8x8_t s2 = load_unaligned_u8(src + 2 * src_stride, (int)src_stride);
-            uint8x8_t s3 = load_unaligned_u8(src + 3 * src_stride, (int)src_stride);
+            uint8x8_t s0 = load_u8_4x2(src + 0 * src_stride, (int)src_stride);
+            uint8x8_t s1 = load_u8_4x2(src + 1 * src_stride, (int)src_stride);
+            uint8x8_t s2 = load_u8_4x2(src + 2 * src_stride, (int)src_stride);
+            uint8x8_t s3 = load_u8_4x2(src + 3 * src_stride, (int)src_stride);
 
             uint16x8_t sum0 = vmull_u8(s0, f0);
             sum0            = vmlal_u8(sum0, s1, f1);
diff --git a/Source/Lib/ASM_NEON/av1_k_means_neon.c b/Source/Lib/ASM_NEON/av1_k_means_neon.c
index 6a8f2b3f0638021497fd8f8e058f711ee18ed5ad..536ee22521ceefa208ea610a35d6a650e96589d5 100644
--- a/Source/Lib/ASM_NEON/av1_k_means_neon.c
+++ b/Source/Lib/ASM_NEON/av1_k_means_neon.c
@@ -14,7 +14,7 @@
 #include "common_dsp_rtcd.h"
 #include "definitions.h"
 
-static inline int32x4_t k_means_multiply_add_neon(const int16x8_t a) {
+static INLINE int32x4_t k_means_multiply_add_neon(const int16x8_t a) {
     const int32x4_t l = vmull_s16(vget_low_s16(a), vget_low_s16(a));
     const int32x4_t h = vmull_s16(vget_high_s16(a), vget_high_s16(a));
     return vpaddq_s32(l, h);
diff --git a/Source/Lib/ASM_NEON/av1_quantize_neon.c b/Source/Lib/ASM_NEON/av1_quantize_neon.c
index ee8f578d80220a929c9b2f695d5d9154f359d26b..ad9f39b53935a69e48640cbcbdd4275c6ae3ce3c 100644
--- a/Source/Lib/ASM_NEON/av1_quantize_neon.c
+++ b/Source/Lib/ASM_NEON/av1_quantize_neon.c
@@ -229,11 +229,12 @@ static INLINE int16x8_t qm_mull_shift(int16x8_t x0, uint16x8_t x1) {
                   vshrq_n_u16(vmulq_u16(vreinterpretq_u16_s16(x0), x1), AOM_QM_BITS)));
 }
 
-static void aom_quantize_b_helper_16x16_neon(const TranLow *coeff_ptr, intptr_t n_coeffs, const int16_t *zbin_ptr,
-                                             const int16_t *round_ptr, const int16_t *quant_ptr,
-                                             const int16_t *quant_shift_ptr, TranLow *qcoeff_ptr, TranLow *dqcoeff_ptr,
-                                             const int16_t *dequant_ptr, uint16_t *eob_ptr, const int16_t *iscan,
-                                             const QmVal *qm_ptr, const QmVal *iqm_ptr) {
+static INLINE void aom_quantize_b_helper_16x16_neon(const TranLow *coeff_ptr, intptr_t n_coeffs,
+                                                    const int16_t *zbin_ptr, const int16_t *round_ptr,
+                                                    const int16_t *quant_ptr, const int16_t *quant_shift_ptr,
+                                                    TranLow *qcoeff_ptr, TranLow *dqcoeff_ptr,
+                                                    const int16_t *dequant_ptr, uint16_t *eob_ptr, const int16_t *iscan,
+                                                    const QmVal *qm_ptr, const QmVal *iqm_ptr) {
     uint16x8_t vwt, viwt;
     const int  zbins[2] = {zbin_ptr[0], zbin_ptr[1]};
 
@@ -350,11 +351,12 @@ static void aom_quantize_b_helper_16x16_neon(const TranLow *coeff_ptr, intptr_t
     *eob_ptr = get_max_eob(v_eobmax_76543210) + 1;
 }
 
-static void aom_quantize_b_helper_32x32_neon(const TranLow *coeff_ptr, intptr_t n_coeffs, const int16_t *zbin_ptr,
-                                             const int16_t *round_ptr, const int16_t *quant_ptr,
-                                             const int16_t *quant_shift_ptr, TranLow *qcoeff_ptr, TranLow *dqcoeff_ptr,
-                                             const int16_t *dequant_ptr, uint16_t *eob_ptr, const int16_t *iscan,
-                                             const QmVal *qm_ptr, const QmVal *iqm_ptr) {
+static INLINE void aom_quantize_b_helper_32x32_neon(const TranLow *coeff_ptr, intptr_t n_coeffs,
+                                                    const int16_t *zbin_ptr, const int16_t *round_ptr,
+                                                    const int16_t *quant_ptr, const int16_t *quant_shift_ptr,
+                                                    TranLow *qcoeff_ptr, TranLow *dqcoeff_ptr,
+                                                    const int16_t *dequant_ptr, uint16_t *eob_ptr, const int16_t *iscan,
+                                                    const QmVal *qm_ptr, const QmVal *iqm_ptr) {
     uint16x8_t vwt, viwt;
     const int  log_scale = 1;
     const int  zbins[2]  = {ROUND_POWER_OF_TWO(zbin_ptr[0], log_scale), ROUND_POWER_OF_TWO(zbin_ptr[1], log_scale)};
@@ -476,11 +478,12 @@ static void aom_quantize_b_helper_32x32_neon(const TranLow *coeff_ptr, intptr_t
     *eob_ptr = get_max_eob(v_eobmax_76543210) + 1;
 }
 
-static void aom_quantize_b_helper_64x64_neon(const TranLow *coeff_ptr, intptr_t n_coeffs, const int16_t *zbin_ptr,
-                                             const int16_t *round_ptr, const int16_t *quant_ptr,
-                                             const int16_t *quant_shift_ptr, TranLow *qcoeff_ptr, TranLow *dqcoeff_ptr,
-                                             const int16_t *dequant_ptr, uint16_t *eob_ptr, const int16_t *iscan,
-                                             const QmVal *qm_ptr, const QmVal *iqm_ptr) {
+static INLINE void aom_quantize_b_helper_64x64_neon(const TranLow *coeff_ptr, intptr_t n_coeffs,
+                                                    const int16_t *zbin_ptr, const int16_t *round_ptr,
+                                                    const int16_t *quant_ptr, const int16_t *quant_shift_ptr,
+                                                    TranLow *qcoeff_ptr, TranLow *dqcoeff_ptr,
+                                                    const int16_t *dequant_ptr, uint16_t *eob_ptr, const int16_t *iscan,
+                                                    const QmVal *qm_ptr, const QmVal *iqm_ptr) {
     uint16x8_t      vwt, viwt;
     const int       log_scale   = 2;
     const int16x8_t v_log_scale = vreinterpretq_s16_s64(vdupq_n_s64(0xFFFEFFFEFFFEFFFE));
diff --git a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
index e745dcdf49666ebe136ee444b5e82aa467178e85..776929d7551768722a136cadb68d48d85c4c21d4 100644
--- a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
@@ -87,10 +87,10 @@ void svt_aom_blend_a64_hmask_neon(uint8_t *dst, uint32_t dst_stride, const uint8
             dst += dst_stride;
         } while (--h != 0);
     } else if (w == 4) {
-        const uint8x8_t m0 = load_unaligned_dup_u8_4x2(mask);
+        const uint8x8_t m0 = load_dup_u8_4x2(mask);
         do {
-            uint8x8_t s0 = load_unaligned_u8_4x2(src0, src0_stride);
-            uint8x8_t s1 = load_unaligned_u8_4x2(src1, src1_stride);
+            uint8x8_t s0 = load_u8_4x2(src0, src0_stride);
+            uint8x8_t s1 = load_u8_4x2(src1, src1_stride);
 
             uint8x8_t blend = alpha_blend_a64_u8x8(m0, s0, s1);
 
@@ -105,8 +105,8 @@ void svt_aom_blend_a64_hmask_neon(uint8_t *dst, uint32_t dst_stride, const uint8
         assert(w == 2);
         const uint8x8_t m0 = vreinterpret_u8_u16(vld1_dup_u16((uint16_t *)mask));
         do {
-            uint8x8_t s0 = load_unaligned_u8_2x2(src0, src0_stride);
-            uint8x8_t s1 = load_unaligned_u8_2x2(src1, src1_stride);
+            uint8x8_t s0 = load_u8_2x2(src0, src0_stride);
+            uint8x8_t s1 = load_u8_2x2(src1, src1_stride);
 
             uint8x8_t blend = alpha_blend_a64_u8x8(m0, s0, s1);
 
@@ -170,8 +170,8 @@ void svt_aom_blend_a64_vmask_neon(uint8_t *dst, uint32_t dst_stride, const uint8
             const uint16x4_t m0 = vdup_n_u16((uint16_t)mask[0]);
             const uint16x4_t m1 = vdup_n_u16((uint16_t)mask[1]);
             const uint8x8_t  m  = vmovn_u16(vcombine_u16(m0, m1));
-            uint8x8_t        s0 = load_unaligned_u8_4x2(src0, src0_stride);
-            uint8x8_t        s1 = load_unaligned_u8_4x2(src1, src1_stride);
+            uint8x8_t        s0 = load_u8_4x2(src0, src0_stride);
+            uint8x8_t        s1 = load_u8_4x2(src1, src1_stride);
 
             uint8x8_t blend = alpha_blend_a64_u8x8(m, s0, s1);
 
@@ -189,8 +189,8 @@ void svt_aom_blend_a64_vmask_neon(uint8_t *dst, uint32_t dst_stride, const uint8
             uint16x4_t m0 = vdup_n_u16(0);
             m0            = vld1_lane_u16((uint16_t *)mask, m0, 0);
             uint8x8_t m   = vzip_u8(vreinterpret_u8_u16(m0), vreinterpret_u8_u16(m0)).val[0];
-            uint8x8_t s0  = load_unaligned_u8_2x2(src0, src0_stride);
-            uint8x8_t s1  = load_unaligned_u8_2x2(src1, src1_stride);
+            uint8x8_t s0  = load_u8_2x2(src0, src0_stride);
+            uint8x8_t s1  = load_u8_2x2(src1, src1_stride);
 
             uint8x8_t blend = alpha_blend_a64_u8x8(m, s0, s1);
 
@@ -266,9 +266,9 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
             } while (--h != 0);
         } else {
             do {
-                uint16x8_t m0 = vmovl_u8(load_unaligned_u8_4x2(mask, mask_stride));
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t m0 = vmovl_u8(load_u8_4x2(mask, mask_stride));
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint8x8_t blend = alpha_blend_a64_d16_u16x8(m0, s0, s1, offset_vec);
 
@@ -312,8 +312,8 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                 uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);
                 uint8x8_t  m2 = vld1_u8(mask + 2 * mask_stride);
                 uint8x8_t  m3 = vld1_u8(mask + 3 * mask_stride);
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);
                 uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
@@ -353,8 +353,8 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
             do {
                 uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);
                 uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));
                 uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
@@ -392,10 +392,10 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
             } while (--h != 0);
         } else {
             do {
-                uint8x8_t  m0_2 = load_unaligned_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
-                uint8x8_t  m1_3 = load_unaligned_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
-                uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);
+                uint8x8_t  m0_2 = load_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
+                uint8x8_t  m1_3 = load_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
+                uint16x8_t s0   = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1   = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0_2, m1_3));
                 uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
@@ -462,13 +462,13 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
             } while (--h != 0);
         } else {
             do {
-                const uint8x8_t m0 = load_unaligned_u8_4x2(mask, mask_stride);
-                const uint8x8_t s0 = load_unaligned_u8_4x2(src0, src0_stride);
-                const uint8x8_t s1 = load_unaligned_u8_4x2(src1, src1_stride);
+                const uint8x8_t m0 = load_u8_4x2(mask, mask_stride);
+                const uint8x8_t s0 = load_u8_4x2(src0, src0_stride);
+                const uint8x8_t s1 = load_u8_4x2(src1, src1_stride);
 
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m0, s0, s1);
 
-                store_unaligned_u8_4x2(dst, dst_stride, blend);
+                store_u8x4_strided_x2(dst, dst_stride, blend);
 
                 mask += 2 * mask_stride;
                 src0 += 2 * src0_stride;
@@ -527,13 +527,13 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                 const uint8x8_t m1 = vld1_u8(mask + 1 * mask_stride);
                 const uint8x8_t m2 = vld1_u8(mask + 2 * mask_stride);
                 const uint8x8_t m3 = vld1_u8(mask + 3 * mask_stride);
-                const uint8x8_t s0 = load_unaligned_u8_4x2(src0, src0_stride);
-                const uint8x8_t s1 = load_unaligned_u8_4x2(src1, src1_stride);
+                const uint8x8_t s0 = load_u8_4x2(src0, src0_stride);
+                const uint8x8_t s1 = load_u8_4x2(src1, src1_stride);
 
                 const uint8x8_t m_avg = avg_blend_pairwise_u8x8_4(m0, m1, m2, m3);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
-                store_unaligned_u8_4x2(dst, dst_stride, blend);
+                store_u8x4_strided_x2(dst, dst_stride, blend);
 
                 mask += 4 * mask_stride;
                 src0 += 2 * src0_stride;
@@ -587,13 +587,13 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
             do {
                 const uint8x8_t m0 = vld1_u8(mask + 0 * mask_stride);
                 const uint8x8_t m1 = vld1_u8(mask + 1 * mask_stride);
-                const uint8x8_t s0 = load_unaligned_u8_4x2(src0, src0_stride);
-                const uint8x8_t s1 = load_unaligned_u8_4x2(src1, src1_stride);
+                const uint8x8_t s0 = load_u8_4x2(src0, src0_stride);
+                const uint8x8_t s1 = load_u8_4x2(src1, src1_stride);
 
                 const uint8x8_t m_avg = vrshr_n_u8(vpadd_u8(m0, m1), 1);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
-                store_unaligned_u8_4x2(dst, dst_stride, blend);
+                store_u8x4_strided_x2(dst, dst_stride, blend);
 
                 mask += 2 * mask_stride;
                 src0 += 2 * src0_stride;
@@ -644,15 +644,15 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
             } while (--h != 0);
         } else {
             do {
-                const uint8x8_t m0_2 = load_unaligned_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
-                const uint8x8_t m1_3 = load_unaligned_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
-                const uint8x8_t s0   = load_unaligned_u8_4x2(src0, src0_stride);
-                const uint8x8_t s1   = load_unaligned_u8_4x2(src1, src1_stride);
+                const uint8x8_t m0_2 = load_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
+                const uint8x8_t m1_3 = load_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
+                const uint8x8_t s0   = load_u8_4x2(src0, src0_stride);
+                const uint8x8_t s1   = load_u8_4x2(src1, src1_stride);
 
                 const uint8x8_t m_avg = vrhadd_u8(m0_2, m1_3);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
-                store_unaligned_u8_4x2(dst, dst_stride, blend);
+                store_u8x4_strided_x2(dst, dst_stride, blend);
 
                 mask += 4 * mask_stride;
                 src0 += 2 * src0_stride;
diff --git a/Source/Lib/ASM_NEON/cfl_neon.c b/Source/Lib/ASM_NEON/cfl_neon.c
index 0210cd1edb5bc159c46c901a975c3bfc12c780cc..9acac14db7e5b5c5e6dc82ab0105e09af94b60a4 100644
--- a/Source/Lib/ASM_NEON/cfl_neon.c
+++ b/Source/Lib/ASM_NEON/cfl_neon.c
@@ -26,7 +26,7 @@ Notes:
   corresponding elements in b are zero. Because vsign is used twice in a
   row, with b in the first call becoming a in the second call, there's no
   impact from not zeroing out. */
-static int16x4_t vsign_s16(int16x4_t a, int16x4_t b) {
+static INLINE int16x4_t vsign_s16(int16x4_t a, int16x4_t b) {
     const int16x4_t mask = vshr_n_s16(b, 15);
     return veor_s16(vadd_s16(a, mask), mask);
 }
@@ -40,7 +40,7 @@ Notes:
   corresponding elements in b are zero. Because vsignq is used twice in a
   row, with b in the first call becoming a in the second call, there's no
   impact from not zeroing out. */
-static int16x8_t vsignq_s16(int16x8_t a, int16x8_t b) {
+static INLINE int16x8_t vsignq_s16(int16x8_t a, int16x8_t b) {
     const int16x8_t mask = vshrq_n_s16(b, 15);
     return veorq_s16(vaddq_s16(a, mask), mask);
 }
@@ -197,8 +197,8 @@ void svt_cfl_luma_subsampling_420_lbd_neon(const uint8_t *input, int input_strid
     const int      luma_stride = input_stride << 1;
     if (width == 4) {
         do {
-            const uint8x8_t top = load_unaligned_u8(input, luma_stride);
-            const uint8x8_t bot = load_unaligned_u8(input + input_stride, luma_stride);
+            const uint8x8_t top = load_u8_4x2(input, luma_stride);
+            const uint8x8_t bot = load_u8_4x2(input + input_stride, luma_stride);
             uint16x4_t      sum = vpaddl_u8(top);
             sum                 = vpadal_u8(sum, bot);
             sum                 = vadd_u16(sum, sum);
@@ -264,8 +264,8 @@ void svt_cfl_luma_subsampling_420_hbd_neon(const uint16_t *input, int input_stri
     const int      luma_stride = input_stride << 1;
     if (width == 4) {
         do {
-            const uint16x8_t top = load_unaligned_u16_4x2(input, luma_stride);
-            const uint16x8_t bot = load_unaligned_u16_4x2(input + input_stride, luma_stride);
+            const uint16x8_t top = load_u16_4x2(input, luma_stride);
+            const uint16x8_t bot = load_u16_4x2(input + input_stride, luma_stride);
 
             uint16x8_t sum = vaddq_u16(top, bot);
             sum            = vpaddq_u16(sum, sum);
diff --git a/Source/Lib/ASM_NEON/compound_convolve_neon.c b/Source/Lib/ASM_NEON/compound_convolve_neon.c
index 4e55168d5e80f03de34b097a3397f0fe939982a4..5ef94d16da51704d2a7ef6216f87c72523b7ccdc 100644
--- a/Source/Lib/ASM_NEON/compound_convolve_neon.c
+++ b/Source/Lib/ASM_NEON/compound_convolve_neon.c
@@ -322,10 +322,8 @@ static INLINE void dist_wtd_convolve_2d_copy_dist_wtd_avg_neon(const uint8_t *sr
                                      &d01,
                                      &d23);
 
-            store_u8_4x1(dst8 + 0 * dst8_stride, d01, 0);
-            store_u8_4x1(dst8 + 1 * dst8_stride, d01, 1);
-            store_u8_4x1(dst8 + 2 * dst8_stride, d23, 0);
-            store_u8_4x1(dst8 + 3 * dst8_stride, d23, 1);
+            store_u8x4_strided_x2(dst8 + 0 * dst8_stride, dst8_stride, d01);
+            store_u8x4_strided_x2(dst8 + 2 * dst8_stride, dst8_stride, d23);
 
             src += 4 * src_stride;
             dst += 4 * dst_stride;
@@ -416,10 +414,8 @@ static INLINE void dist_wtd_convolve_2d_copy_avg_neon(const uint8_t *src, int sr
             compute_basic_avg_4x4(
                 dd0, dd1, dd2, dd3, d0, d1, d2, d3, vreinterpretq_s16_u16(round_offset_vec), &d01, &d23);
 
-            store_u8_4x1(dst8 + 0 * dst8_stride, d01, 0);
-            store_u8_4x1(dst8 + 1 * dst8_stride, d01, 1);
-            store_u8_4x1(dst8 + 2 * dst8_stride, d23, 0);
-            store_u8_4x1(dst8 + 3 * dst8_stride, d23, 1);
+            store_u8x4_strided_x2(dst8 + 0 * dst8_stride, dst8_stride, d01);
+            store_u8x4_strided_x2(dst8 + 2 * dst8_stride, dst8_stride, d23);
 
             src += 4 * src_stride;
             dst += 4 * dst_stride;
@@ -654,7 +650,7 @@ static INLINE void dist_wtd_convolve_x_dist_wtd_avg_neon(const uint8_t *src, int
             uint8x8_t d01;
             compute_dist_wtd_avg_4x1(dd0, d0, fwd_offset, bck_offset, vget_low_s16(round_offset_vec), &d01);
 
-            store_u8_4x1(dst8_ptr, d01, 0);
+            store_u8_4x1(dst8_ptr, d01);
 
             src_ptr += src_stride;
             dst_ptr += dst_stride;
@@ -875,7 +871,7 @@ static INLINE void dist_wtd_convolve_x_avg_neon(const uint8_t *src, int src_stri
             uint8x8_t d01;
             compute_basic_avg_4x1(dd0, d0, vget_low_s16(round_offset_vec), &d01);
 
-            store_u8_4x1(dst8_ptr, d01, 0);
+            store_u8_4x1(dst8_ptr, d01);
 
             src_ptr += src_stride;
             dst_ptr += dst_stride;
@@ -1277,11 +1273,11 @@ static INLINE void dist_wtd_convolve_y_6tap_dist_wtd_avg_neon(const uint8_t *src
             uint8_t       *d_u8   = dst8_ptr;
             int            height = h;
 
-            uint8x8_t t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-            uint8x8_t t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-            uint8x8_t t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-            uint8x8_t t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
-            uint8x8_t t4 = load_unaligned_u8_4x1(s + 4 * src_stride);
+            uint8x8_t t0 = load_u8_4x1(s + 0 * src_stride);
+            uint8x8_t t1 = load_u8_4x1(s + 1 * src_stride);
+            uint8x8_t t2 = load_u8_4x1(s + 2 * src_stride);
+            uint8x8_t t3 = load_u8_4x1(s + 3 * src_stride);
+            uint8x8_t t4 = load_u8_4x1(s + 4 * src_stride);
 
             int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
             int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1292,10 +1288,10 @@ static INLINE void dist_wtd_convolve_y_6tap_dist_wtd_avg_neon(const uint8_t *src
             s += 5 * src_stride;
 
             do {
-                t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-                t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-                t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-                t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
+                t0 = load_u8_4x1(s + 0 * src_stride);
+                t1 = load_u8_4x1(s + 1 * src_stride);
+                t2 = load_u8_4x1(s + 2 * src_stride);
+                t3 = load_u8_4x1(s + 3 * src_stride);
 
                 int16x4_t s5 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
                 int16x4_t s6 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1314,10 +1310,8 @@ static INLINE void dist_wtd_convolve_y_6tap_dist_wtd_avg_neon(const uint8_t *src
                 compute_dist_wtd_avg_4x4(
                     dd0, dd1, dd2, dd3, d0, d1, d2, d3, fwd_offset, bck_offset, round_offset_vec, &d01, &d23);
 
-                store_u8_4x1(d_u8 + 0 * dst8_stride, d01, 0);
-                store_u8_4x1(d_u8 + 1 * dst8_stride, d01, 1);
-                store_u8_4x1(d_u8 + 2 * dst8_stride, d23, 0);
-                store_u8_4x1(d_u8 + 3 * dst8_stride, d23, 1);
+                store_u8x4_strided_x2(d_u8 + 0 * dst8_stride, dst8_stride, d01);
+                store_u8x4_strided_x2(d_u8 + 2 * dst8_stride, dst8_stride, d23);
 
                 s0 = s4;
                 s1 = s5;
@@ -1455,11 +1449,11 @@ static INLINE void dist_wtd_convolve_y_6tap_avg_neon(const uint8_t *src_ptr, int
             uint8_t       *d_u8   = dst8_ptr;
             int            height = h;
 
-            uint8x8_t t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-            uint8x8_t t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-            uint8x8_t t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-            uint8x8_t t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
-            uint8x8_t t4 = load_unaligned_u8_4x1(s + 4 * src_stride);
+            uint8x8_t t0 = load_u8_4x1(s + 0 * src_stride);
+            uint8x8_t t1 = load_u8_4x1(s + 1 * src_stride);
+            uint8x8_t t2 = load_u8_4x1(s + 2 * src_stride);
+            uint8x8_t t3 = load_u8_4x1(s + 3 * src_stride);
+            uint8x8_t t4 = load_u8_4x1(s + 4 * src_stride);
 
             int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
             int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1470,10 +1464,10 @@ static INLINE void dist_wtd_convolve_y_6tap_avg_neon(const uint8_t *src_ptr, int
             s += 5 * src_stride;
 
             do {
-                t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-                t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-                t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-                t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
+                t0 = load_u8_4x1(s + 0 * src_stride);
+                t1 = load_u8_4x1(s + 1 * src_stride);
+                t2 = load_u8_4x1(s + 2 * src_stride);
+                t3 = load_u8_4x1(s + 3 * src_stride);
 
                 int16x4_t s5 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
                 int16x4_t s6 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1491,10 +1485,8 @@ static INLINE void dist_wtd_convolve_y_6tap_avg_neon(const uint8_t *src_ptr, int
                 uint8x8_t d01, d23;
                 compute_basic_avg_4x4(dd0, dd1, dd2, dd3, d0, d1, d2, d3, round_offset_vec, &d01, &d23);
 
-                store_u8_4x1(d_u8 + 0 * dst8_stride, d01, 0);
-                store_u8_4x1(d_u8 + 1 * dst8_stride, d01, 1);
-                store_u8_4x1(d_u8 + 2 * dst8_stride, d23, 0);
-                store_u8_4x1(d_u8 + 3 * dst8_stride, d23, 1);
+                store_u8x4_strided_x2(d_u8 + 0 * dst8_stride, dst8_stride, d01);
+                store_u8x4_strided_x2(d_u8 + 2 * dst8_stride, dst8_stride, d23);
 
                 s0 = s4;
                 s1 = s5;
@@ -1604,11 +1596,11 @@ static INLINE void dist_wtd_convolve_y_6tap_neon(const uint8_t *src_ptr, int src
             CONV_BUF_TYPE *d      = dst_ptr;
             int            height = h;
 
-            uint8x8_t t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-            uint8x8_t t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-            uint8x8_t t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-            uint8x8_t t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
-            uint8x8_t t4 = load_unaligned_u8_4x1(s + 4 * src_stride);
+            uint8x8_t t0 = load_u8_4x1(s + 0 * src_stride);
+            uint8x8_t t1 = load_u8_4x1(s + 1 * src_stride);
+            uint8x8_t t2 = load_u8_4x1(s + 2 * src_stride);
+            uint8x8_t t3 = load_u8_4x1(s + 3 * src_stride);
+            uint8x8_t t4 = load_u8_4x1(s + 4 * src_stride);
 
             int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
             int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1619,10 +1611,10 @@ static INLINE void dist_wtd_convolve_y_6tap_neon(const uint8_t *src_ptr, int src
             s += 5 * src_stride;
 
             do {
-                t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-                t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-                t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-                t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
+                t0 = load_u8_4x1(s + 0 * src_stride);
+                t1 = load_u8_4x1(s + 1 * src_stride);
+                t2 = load_u8_4x1(s + 2 * src_stride);
+                t3 = load_u8_4x1(s + 3 * src_stride);
 
                 int16x4_t s5 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
                 int16x4_t s6 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1772,13 +1764,13 @@ static INLINE void dist_wtd_convolve_y_8tap_dist_wtd_avg_neon(const uint8_t *src
             __builtin_prefetch(s + 2 * src_stride);
             __builtin_prefetch(s + 3 * src_stride);
 
-            uint8x8_t t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-            uint8x8_t t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-            uint8x8_t t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-            uint8x8_t t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
-            uint8x8_t t4 = load_unaligned_u8_4x1(s + 4 * src_stride);
-            uint8x8_t t5 = load_unaligned_u8_4x1(s + 5 * src_stride);
-            uint8x8_t t6 = load_unaligned_u8_4x1(s + 6 * src_stride);
+            uint8x8_t t0 = load_u8_4x1(s + 0 * src_stride);
+            uint8x8_t t1 = load_u8_4x1(s + 1 * src_stride);
+            uint8x8_t t2 = load_u8_4x1(s + 2 * src_stride);
+            uint8x8_t t3 = load_u8_4x1(s + 3 * src_stride);
+            uint8x8_t t4 = load_u8_4x1(s + 4 * src_stride);
+            uint8x8_t t5 = load_u8_4x1(s + 5 * src_stride);
+            uint8x8_t t6 = load_u8_4x1(s + 6 * src_stride);
 
             int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
             int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1796,10 +1788,10 @@ static INLINE void dist_wtd_convolve_y_8tap_dist_wtd_avg_neon(const uint8_t *src
             s += 7 * src_stride;
 
             do {
-                t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-                t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-                t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-                t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
+                t0 = load_u8_4x1(s + 0 * src_stride);
+                t1 = load_u8_4x1(s + 1 * src_stride);
+                t2 = load_u8_4x1(s + 2 * src_stride);
+                t3 = load_u8_4x1(s + 3 * src_stride);
 
                 int16x4_t s7  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
                 int16x4_t s8  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -1829,10 +1821,8 @@ static INLINE void dist_wtd_convolve_y_8tap_dist_wtd_avg_neon(const uint8_t *src
                 compute_dist_wtd_avg_4x4(
                     dd0, dd1, dd2, dd3, d0, d1, d2, d3, fwd_offset, bck_offset, round_offset_vec, &d01, &d23);
 
-                store_u8_4x1(d_u8 + 0 * dst8_stride, d01, 0);
-                store_u8_4x1(d_u8 + 1 * dst8_stride, d01, 1);
-                store_u8_4x1(d_u8 + 2 * dst8_stride, d23, 0);
-                store_u8_4x1(d_u8 + 3 * dst8_stride, d23, 1);
+                store_u8x4_strided_x2(d_u8 + 0 * dst8_stride, dst8_stride, d01);
+                store_u8x4_strided_x2(d_u8 + 2 * dst8_stride, dst8_stride, d23);
 
                 s0 = s4;
                 s1 = s5;
@@ -2002,13 +1992,13 @@ static INLINE void dist_wtd_convolve_y_8tap_avg_neon(const uint8_t *src_ptr, int
             __builtin_prefetch(s + 2 * src_stride);
             __builtin_prefetch(s + 3 * src_stride);
 
-            uint8x8_t t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-            uint8x8_t t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-            uint8x8_t t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-            uint8x8_t t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
-            uint8x8_t t4 = load_unaligned_u8_4x1(s + 4 * src_stride);
-            uint8x8_t t5 = load_unaligned_u8_4x1(s + 5 * src_stride);
-            uint8x8_t t6 = load_unaligned_u8_4x1(s + 6 * src_stride);
+            uint8x8_t t0 = load_u8_4x1(s + 0 * src_stride);
+            uint8x8_t t1 = load_u8_4x1(s + 1 * src_stride);
+            uint8x8_t t2 = load_u8_4x1(s + 2 * src_stride);
+            uint8x8_t t3 = load_u8_4x1(s + 3 * src_stride);
+            uint8x8_t t4 = load_u8_4x1(s + 4 * src_stride);
+            uint8x8_t t5 = load_u8_4x1(s + 5 * src_stride);
+            uint8x8_t t6 = load_u8_4x1(s + 6 * src_stride);
 
             int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
             int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -2026,10 +2016,10 @@ static INLINE void dist_wtd_convolve_y_8tap_avg_neon(const uint8_t *src_ptr, int
             s += 7 * src_stride;
 
             do {
-                t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-                t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-                t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-                t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
+                t0 = load_u8_4x1(s + 0 * src_stride);
+                t1 = load_u8_4x1(s + 1 * src_stride);
+                t2 = load_u8_4x1(s + 2 * src_stride);
+                t3 = load_u8_4x1(s + 3 * src_stride);
 
                 int16x4_t s7  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
                 int16x4_t s8  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -2058,10 +2048,8 @@ static INLINE void dist_wtd_convolve_y_8tap_avg_neon(const uint8_t *src_ptr, int
                 uint8x8_t d01, d23;
                 compute_basic_avg_4x4(dd0, dd1, dd2, dd3, d0, d1, d2, d3, round_offset_vec, &d01, &d23);
 
-                store_u8_4x1(d_u8 + 0 * dst8_stride, d01, 0);
-                store_u8_4x1(d_u8 + 1 * dst8_stride, d01, 1);
-                store_u8_4x1(d_u8 + 2 * dst8_stride, d23, 0);
-                store_u8_4x1(d_u8 + 3 * dst8_stride, d23, 1);
+                store_u8x4_strided_x2(d_u8 + 0 * dst8_stride, dst8_stride, d01);
+                store_u8x4_strided_x2(d_u8 + 2 * dst8_stride, dst8_stride, d23);
 
                 s0 = s4;
                 s1 = s5;
@@ -2203,13 +2191,13 @@ static INLINE void dist_wtd_convolve_y_8tap_neon(const uint8_t *src_ptr, int src
             __builtin_prefetch(s + 2 * src_stride);
             __builtin_prefetch(s + 3 * src_stride);
 
-            uint8x8_t t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-            uint8x8_t t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-            uint8x8_t t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-            uint8x8_t t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
-            uint8x8_t t4 = load_unaligned_u8_4x1(s + 4 * src_stride);
-            uint8x8_t t5 = load_unaligned_u8_4x1(s + 5 * src_stride);
-            uint8x8_t t6 = load_unaligned_u8_4x1(s + 6 * src_stride);
+            uint8x8_t t0 = load_u8_4x1(s + 0 * src_stride);
+            uint8x8_t t1 = load_u8_4x1(s + 1 * src_stride);
+            uint8x8_t t2 = load_u8_4x1(s + 2 * src_stride);
+            uint8x8_t t3 = load_u8_4x1(s + 3 * src_stride);
+            uint8x8_t t4 = load_u8_4x1(s + 4 * src_stride);
+            uint8x8_t t5 = load_u8_4x1(s + 5 * src_stride);
+            uint8x8_t t6 = load_u8_4x1(s + 6 * src_stride);
 
             int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
             int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -2227,10 +2215,10 @@ static INLINE void dist_wtd_convolve_y_8tap_neon(const uint8_t *src_ptr, int src
             s += 7 * src_stride;
 
             do {
-                t0 = load_unaligned_u8_4x1(s + 0 * src_stride);
-                t1 = load_unaligned_u8_4x1(s + 1 * src_stride);
-                t2 = load_unaligned_u8_4x1(s + 2 * src_stride);
-                t3 = load_unaligned_u8_4x1(s + 3 * src_stride);
+                t0 = load_u8_4x1(s + 0 * src_stride);
+                t1 = load_u8_4x1(s + 1 * src_stride);
+                t2 = load_u8_4x1(s + 2 * src_stride);
+                t3 = load_u8_4x1(s + 3 * src_stride);
 
                 int16x4_t s7  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
                 int16x4_t s8  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
diff --git a/Source/Lib/ASM_NEON/compound_convolve_neon.h b/Source/Lib/ASM_NEON/compound_convolve_neon.h
index 1b062344038d4aae673b00526a88366e00896f21..60732df642da5bb27f4eae6ff46d10329945af50 100644
--- a/Source/Lib/ASM_NEON/compound_convolve_neon.h
+++ b/Source/Lib/ASM_NEON/compound_convolve_neon.h
@@ -261,10 +261,8 @@ static INLINE void dist_wtd_convolve_2d_vert_6tap_dist_wtd_avg_neon(int16_t *src
             compute_dist_wtd_avg_4x4(
                 dd0, dd1, dd2, dd3, d0, d1, d2, d3, fwd_offset, bck_offset, round_offset_vec, &d01_u8, &d23_u8);
 
-            store_u8_4x1(dst8_ptr + 0 * dst8_stride, d01_u8, 0);
-            store_u8_4x1(dst8_ptr + 1 * dst8_stride, d01_u8, 1);
-            store_u8_4x1(dst8_ptr + 2 * dst8_stride, d23_u8, 0);
-            store_u8_4x1(dst8_ptr + 3 * dst8_stride, d23_u8, 1);
+            store_u8x4_strided_x2(dst8_ptr + 0 * dst8_stride, dst8_stride, d01_u8);
+            store_u8x4_strided_x2(dst8_ptr + 2 * dst8_stride, dst8_stride, d23_u8);
             dst8_ptr += 4 * dst8_stride;
 
             s0 = s4;
@@ -369,10 +367,8 @@ static INLINE void dist_wtd_convolve_2d_vert_6tap_avg_neon(int16_t *src_ptr, con
             uint8x8_t d01_u8, d23_u8;
             compute_basic_avg_4x4(dd0, dd1, dd2, dd3, d0, d1, d2, d3, round_offset_vec, &d01_u8, &d23_u8);
 
-            store_u8_4x1(dst8_ptr + 0 * dst8_stride, d01_u8, 0);
-            store_u8_4x1(dst8_ptr + 1 * dst8_stride, d01_u8, 1);
-            store_u8_4x1(dst8_ptr + 2 * dst8_stride, d23_u8, 0);
-            store_u8_4x1(dst8_ptr + 3 * dst8_stride, d23_u8, 1);
+            store_u8x4_strided_x2(dst8_ptr + 0 * dst8_stride, dst8_stride, d01_u8);
+            store_u8x4_strided_x2(dst8_ptr + 2 * dst8_stride, dst8_stride, d23_u8);
             dst8_ptr += 4 * dst8_stride;
 
             s0 = s4;
@@ -591,10 +587,8 @@ static INLINE void dist_wtd_convolve_2d_vert_8tap_dist_wtd_avg_neon(int16_t *src
             compute_dist_wtd_avg_4x4(
                 dd0, dd1, dd2, dd3, d0, d1, d2, d3, fwd_offset, bck_offset, round_offset_vec, &d01_u8, &d23_u8);
 
-            store_u8_4x1(dst8_ptr + 0 * dst8_stride, d01_u8, 0);
-            store_u8_4x1(dst8_ptr + 1 * dst8_stride, d01_u8, 1);
-            store_u8_4x1(dst8_ptr + 2 * dst8_stride, d23_u8, 0);
-            store_u8_4x1(dst8_ptr + 3 * dst8_stride, d23_u8, 1);
+            store_u8x4_strided_x2(dst8_ptr + 0 * dst8_stride, dst8_stride, d01_u8);
+            store_u8x4_strided_x2(dst8_ptr + 2 * dst8_stride, dst8_stride, d23_u8);
             dst8_ptr += 4 * dst8_stride;
 
             s0 = s4;
@@ -703,10 +697,8 @@ static INLINE void dist_wtd_convolve_2d_vert_8tap_avg_neon(int16_t *src_ptr, con
             uint8x8_t d01_u8, d23_u8;
             compute_basic_avg_4x4(dd0, dd1, dd2, dd3, d0, d1, d2, d3, round_offset_vec, &d01_u8, &d23_u8);
 
-            store_u8_4x1(dst8_ptr + 0 * dst8_stride, d01_u8, 0);
-            store_u8_4x1(dst8_ptr + 1 * dst8_stride, d01_u8, 1);
-            store_u8_4x1(dst8_ptr + 2 * dst8_stride, d23_u8, 0);
-            store_u8_4x1(dst8_ptr + 3 * dst8_stride, d23_u8, 1);
+            store_u8x4_strided_x2(dst8_ptr + 0 * dst8_stride, dst8_stride, d01_u8);
+            store_u8x4_strided_x2(dst8_ptr + 2 * dst8_stride, dst8_stride, d23_u8);
             dst8_ptr += 4 * dst8_stride;
 
             s0 = s4;
diff --git a/Source/Lib/ASM_NEON/compute_sad_neon.c b/Source/Lib/ASM_NEON/compute_sad_neon.c
index 56cd569f40b3bff4ff4baaf2c0822ecc19bfab8a..88e5175a62b5d0b866bb70515dd273596ec3ad79 100644
--- a/Source/Lib/ASM_NEON/compute_sad_neon.c
+++ b/Source/Lib/ASM_NEON/compute_sad_neon.c
@@ -329,8 +329,8 @@ static INLINE uint32_t sad4xh_neon(const uint8_t *src_ptr, uint32_t src_stride,
     sum = vdupq_n_u16(0);
     i   = h / 2;
     do {
-        s = load_unaligned_u8(src_ptr, src_stride);
-        r = load_unaligned_u8(ref_ptr, ref_stride);
+        s = load_u8_4x2(src_ptr, src_stride);
+        r = load_u8_4x2(ref_ptr, ref_stride);
 
         sum = vabal_u8(sum, s, r); // add and accumulate
 
diff --git a/Source/Lib/ASM_NEON/convolve_neon.c b/Source/Lib/ASM_NEON/convolve_neon.c
index 943e1027a7ee632dd3891faccbefd0485c81d57b..fe84cc520670e5eec3bf34fcadbca59285de32e7 100644
--- a/Source/Lib/ASM_NEON/convolve_neon.c
+++ b/Source/Lib/ASM_NEON/convolve_neon.c
@@ -47,10 +47,10 @@ static INLINE void convolve_x_sr_4tap_neon(const uint8_t *src_ptr, int src_strid
     if (w == 4) {
         do {
             uint8x8_t t01[4];
-            t01[0] = load_unaligned_u8(src_ptr + 0, src_stride);
-            t01[1] = load_unaligned_u8(src_ptr + 1, src_stride);
-            t01[2] = load_unaligned_u8(src_ptr + 2, src_stride);
-            t01[3] = load_unaligned_u8(src_ptr + 3, src_stride);
+            t01[0] = load_u8_4x2(src_ptr + 0, src_stride);
+            t01[1] = load_u8_4x2(src_ptr + 1, src_stride);
+            t01[2] = load_u8_4x2(src_ptr + 2, src_stride);
+            t01[3] = load_u8_4x2(src_ptr + 3, src_stride);
 
             int16x8_t s01[4];
             s01[0] = vreinterpretq_s16_u16(vmovl_u8(t01[0]));
@@ -291,8 +291,8 @@ static INLINE void convolve_y_sr_4tap_neon(const uint8_t *src, const int src_str
     const int16x4_t filter = vshr_n_s16(vld1_s16(filter_y + 2), 1);
 
     if (w == 4) {
-        uint8x8_t t01 = load_unaligned_u8(src + 0 * src_stride, src_stride);
-        uint8x8_t t12 = load_unaligned_u8(src + 1 * src_stride, src_stride);
+        uint8x8_t t01 = load_u8_4x2(src + 0 * src_stride, src_stride);
+        uint8x8_t t12 = load_u8_4x2(src + 1 * src_stride, src_stride);
 
         int16x8_t s01 = vreinterpretq_s16_u16(vmovl_u8(t01));
         int16x8_t s12 = vreinterpretq_s16_u16(vmovl_u8(t12));
@@ -300,10 +300,10 @@ static INLINE void convolve_y_sr_4tap_neon(const uint8_t *src, const int src_str
         src += 2 * src_stride;
 
         do {
-            uint8x8_t t23 = load_unaligned_u8(src + 0 * src_stride, src_stride);
-            uint8x8_t t34 = load_unaligned_u8(src + 1 * src_stride, src_stride);
-            uint8x8_t t45 = load_unaligned_u8(src + 2 * src_stride, src_stride);
-            uint8x8_t t56 = load_unaligned_u8(src + 3 * src_stride, src_stride);
+            uint8x8_t t23 = load_u8_4x2(src + 0 * src_stride, src_stride);
+            uint8x8_t t34 = load_u8_4x2(src + 1 * src_stride, src_stride);
+            uint8x8_t t45 = load_u8_4x2(src + 2 * src_stride, src_stride);
+            uint8x8_t t56 = load_u8_4x2(src + 3 * src_stride, src_stride);
 
             int16x8_t s23 = vreinterpretq_s16_u16(vmovl_u8(t23));
             int16x8_t s34 = vreinterpretq_s16_u16(vmovl_u8(t34));
@@ -436,11 +436,11 @@ static INLINE uint8x8_t convolve6_8_y(const int16x8_t s0, const int16x8_t s1, co
 static INLINE void convolve_y_sr_6tap_neon(const uint8_t *src_ptr, int src_stride, uint8_t *dst_ptr,
                                            const int dst_stride, int w, int h, const int16x8_t y_filter) {
     if (w <= 4) {
-        uint8x8_t t0 = load_unaligned_u8_4x1(src_ptr + 0 * src_stride);
-        uint8x8_t t1 = load_unaligned_u8_4x1(src_ptr + 1 * src_stride);
-        uint8x8_t t2 = load_unaligned_u8_4x1(src_ptr + 2 * src_stride);
-        uint8x8_t t3 = load_unaligned_u8_4x1(src_ptr + 3 * src_stride);
-        uint8x8_t t4 = load_unaligned_u8_4x1(src_ptr + 4 * src_stride);
+        uint8x8_t t0 = load_u8_4x1(src_ptr + 0 * src_stride);
+        uint8x8_t t1 = load_u8_4x1(src_ptr + 1 * src_stride);
+        uint8x8_t t2 = load_u8_4x1(src_ptr + 2 * src_stride);
+        uint8x8_t t3 = load_u8_4x1(src_ptr + 3 * src_stride);
+        uint8x8_t t4 = load_u8_4x1(src_ptr + 4 * src_stride);
 
         int16x4_t s0 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
         int16x4_t s1 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
@@ -451,10 +451,10 @@ static INLINE void convolve_y_sr_6tap_neon(const uint8_t *src_ptr, int src_strid
         src_ptr += 5 * src_stride;
 
         do {
-            uint8x8_t t5 = load_unaligned_u8_4x1(src_ptr + 0 * src_stride);
-            uint8x8_t t6 = load_unaligned_u8_4x1(src_ptr + 1 * src_stride);
-            uint8x8_t t7 = load_unaligned_u8_4x1(src_ptr + 2 * src_stride);
-            uint8x8_t t8 = load_unaligned_u8_4x1(src_ptr + 3 * src_stride);
+            uint8x8_t t5 = load_u8_4x1(src_ptr + 0 * src_stride);
+            uint8x8_t t6 = load_u8_4x1(src_ptr + 1 * src_stride);
+            uint8x8_t t7 = load_u8_4x1(src_ptr + 2 * src_stride);
+            uint8x8_t t8 = load_u8_4x1(src_ptr + 3 * src_stride);
 
             int16x4_t s5 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t5)));
             int16x4_t s6 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t6)));
@@ -470,10 +470,8 @@ static INLINE void convolve_y_sr_6tap_neon(const uint8_t *src_ptr, int src_strid
             uint8x8_t d01 = vqrshrun_n_s16(vcombine_s16(d0, d1), FILTER_BITS - 1);
             uint8x8_t d23 = vqrshrun_n_s16(vcombine_s16(d2, d3), FILTER_BITS - 1);
 
-            store_u8_4x1(dst_ptr + 0 * dst_stride, d01, 0);
-            store_u8_4x1(dst_ptr + 1 * dst_stride, d01, 1);
-            store_u8_4x1(dst_ptr + 2 * dst_stride, d23, 0);
-            store_u8_4x1(dst_ptr + 3 * dst_stride, d23, 1);
+            store_u8x4_strided_x2(dst_ptr + 0 * dst_stride, dst_stride, d01);
+            store_u8x4_strided_x2(dst_ptr + 2 * dst_stride, dst_stride, d23);
 
             s0 = s4;
             s1 = s5;
@@ -574,13 +572,13 @@ static INLINE uint8x8_t convolve8_8_y(const int16x8_t s0, const int16x8_t s1, co
 static INLINE void convolve_y_sr_8tap_neon(const uint8_t *src_ptr, int src_stride, uint8_t *dst_ptr,
                                            const int dst_stride, int w, int h, const int16x8_t y_filter) {
     if (w <= 4) {
-        uint8x8_t t0 = load_unaligned_u8_4x1(src_ptr + 0 * src_stride);
-        uint8x8_t t1 = load_unaligned_u8_4x1(src_ptr + 1 * src_stride);
-        uint8x8_t t2 = load_unaligned_u8_4x1(src_ptr + 2 * src_stride);
-        uint8x8_t t3 = load_unaligned_u8_4x1(src_ptr + 3 * src_stride);
-        uint8x8_t t4 = load_unaligned_u8_4x1(src_ptr + 4 * src_stride);
-        uint8x8_t t5 = load_unaligned_u8_4x1(src_ptr + 5 * src_stride);
-        uint8x8_t t6 = load_unaligned_u8_4x1(src_ptr + 6 * src_stride);
+        uint8x8_t t0 = load_u8_4x1(src_ptr + 0 * src_stride);
+        uint8x8_t t1 = load_u8_4x1(src_ptr + 1 * src_stride);
+        uint8x8_t t2 = load_u8_4x1(src_ptr + 2 * src_stride);
+        uint8x8_t t3 = load_u8_4x1(src_ptr + 3 * src_stride);
+        uint8x8_t t4 = load_u8_4x1(src_ptr + 4 * src_stride);
+        uint8x8_t t5 = load_u8_4x1(src_ptr + 5 * src_stride);
+        uint8x8_t t6 = load_u8_4x1(src_ptr + 6 * src_stride);
 
         int16x4_t s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t0)));
         int16x4_t s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t1)));
@@ -593,10 +591,10 @@ static INLINE void convolve_y_sr_8tap_neon(const uint8_t *src_ptr, int src_strid
         src_ptr += 7 * src_stride;
 
         do {
-            uint8x8_t t7  = load_unaligned_u8_4x1(src_ptr + 0 * src_stride);
-            uint8x8_t t8  = load_unaligned_u8_4x1(src_ptr + 1 * src_stride);
-            uint8x8_t t9  = load_unaligned_u8_4x1(src_ptr + 2 * src_stride);
-            uint8x8_t t10 = load_unaligned_u8_4x1(src_ptr + 3 * src_stride);
+            uint8x8_t t7  = load_u8_4x1(src_ptr + 0 * src_stride);
+            uint8x8_t t8  = load_u8_4x1(src_ptr + 1 * src_stride);
+            uint8x8_t t9  = load_u8_4x1(src_ptr + 2 * src_stride);
+            uint8x8_t t10 = load_u8_4x1(src_ptr + 3 * src_stride);
 
             int16x4_t s7  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t7)));
             int16x4_t s8  = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(t8)));
@@ -612,10 +610,8 @@ static INLINE void convolve_y_sr_8tap_neon(const uint8_t *src_ptr, int src_strid
             uint8x8_t d01 = vqrshrun_n_s16(vcombine_s16(d0, d1), FILTER_BITS - 1);
             uint8x8_t d23 = vqrshrun_n_s16(vcombine_s16(d2, d3), FILTER_BITS - 1);
 
-            store_u8_4x1(dst_ptr + 0 * dst_stride, d01, 0);
-            store_u8_4x1(dst_ptr + 1 * dst_stride, d01, 1);
-            store_u8_4x1(dst_ptr + 2 * dst_stride, d23, 0);
-            store_u8_4x1(dst_ptr + 3 * dst_stride, d23, 1);
+            store_u8x4_strided_x2(dst_ptr + 0 * dst_stride, dst_stride, d01);
+            store_u8x4_strided_x2(dst_ptr + 2 * dst_stride, dst_stride, d23);
 
             s0 = s4;
             s1 = s5;
@@ -753,10 +749,10 @@ static INLINE void convolve_2d_sr_horiz_4tap_neon(const uint8_t *src, ptrdiff_t
     if (w == 4) {
         do {
             uint8x8_t t01[4];
-            t01[0] = load_unaligned_u8(src + 0, (int)src_stride);
-            t01[1] = load_unaligned_u8(src + 1, (int)src_stride);
-            t01[2] = load_unaligned_u8(src + 2, (int)src_stride);
-            t01[3] = load_unaligned_u8(src + 3, (int)src_stride);
+            t01[0] = load_u8_4x2(src + 0, (int)src_stride);
+            t01[1] = load_u8_4x2(src + 1, (int)src_stride);
+            t01[2] = load_u8_4x2(src + 2, (int)src_stride);
+            t01[3] = load_u8_4x2(src + 3, (int)src_stride);
 
             int16x8_t s01[4];
             s01[0] = vreinterpretq_s16_u16(vmovl_u8(t01[0]));
diff --git a/Source/Lib/ASM_NEON/convolve_neon.h b/Source/Lib/ASM_NEON/convolve_neon.h
index 9457a8c4289cc8dd803f84a9a7cd44a04abf6f02..9ab64dc651cd58fae3dd37777036a049ed055ccf 100644
--- a/Source/Lib/ASM_NEON/convolve_neon.h
+++ b/Source/Lib/ASM_NEON/convolve_neon.h
@@ -222,10 +222,8 @@ static INLINE void convolve_2d_sr_vert_6tap_neon(int16_t *src_ptr, int src_strid
             uint8x8_t d01 = vqmovun_s16(vsubq_s16(vcombine_s16(d0, d1), sub_const));
             uint8x8_t d23 = vqmovun_s16(vsubq_s16(vcombine_s16(d2, d3), sub_const));
 
-            store_u8_4x1(dst_ptr + 0 * dst_stride, d01, 0);
-            store_u8_4x1(dst_ptr + 1 * dst_stride, d01, 1);
-            store_u8_4x1(dst_ptr + 2 * dst_stride, d23, 0);
-            store_u8_4x1(dst_ptr + 3 * dst_stride, d23, 1);
+            store_u8x4_strided_x2(dst_ptr + 0 * dst_stride, dst_stride, d01);
+            store_u8x4_strided_x2(dst_ptr + 2 * dst_stride, dst_stride, d23);
 
             s0 = s4;
             s1 = s5;
@@ -345,10 +343,8 @@ static INLINE void convolve_2d_sr_vert_8tap_neon(int16_t *src_ptr, int src_strid
             uint8x8_t d01 = vqmovun_s16(vsubq_s16(vcombine_s16(d0, d1), sub_const));
             uint8x8_t d23 = vqmovun_s16(vsubq_s16(vcombine_s16(d2, d3), sub_const));
 
-            store_u8_4x1(dst_ptr + 0 * dst_stride, d01, 0);
-            store_u8_4x1(dst_ptr + 1 * dst_stride, d01, 1);
-            store_u8_4x1(dst_ptr + 2 * dst_stride, d23, 0);
-            store_u8_4x1(dst_ptr + 3 * dst_stride, d23, 1);
+            store_u8x4_strided_x2(dst_ptr + 0 * dst_stride, dst_stride, d01);
+            store_u8x4_strided_x2(dst_ptr + 2 * dst_stride, dst_stride, d23);
 
             s0 = s4;
             s1 = s5;
diff --git a/Source/Lib/ASM_NEON/convolve_scale_neon.c b/Source/Lib/ASM_NEON/convolve_scale_neon.c
index cd7bd99b7131236c6f66bc908fd3b148bf859d7e..51375bd56290abf527b13357630b2ccf6b4fe849 100644
--- a/Source/Lib/ASM_NEON/convolve_scale_neon.c
+++ b/Source/Lib/ASM_NEON/convolve_scale_neon.c
@@ -20,7 +20,7 @@
 #include "mem_neon.h"
 #include "transpose_neon.h"
 
-static inline int16x4_t convolve8_4_h(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
+static INLINE int16x4_t convolve8_4_h(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
                                       const int16x4_t s4, const int16x4_t s5, const int16x4_t s6, const int16x4_t s7,
                                       const int16x8_t filter, const int32x4_t horiz_const) {
     int16x4_t filter_lo = vget_low_s16(filter);
@@ -39,7 +39,7 @@ static inline int16x4_t convolve8_4_h(const int16x4_t s0, const int16x4_t s1, co
     return vshrn_n_s32(sum, ROUND0_BITS);
 }
 
-static inline int16x8_t convolve8_8_h(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
+static INLINE int16x8_t convolve8_8_h(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
                                       const int16x8_t s4, const int16x8_t s5, const int16x8_t s6, const int16x8_t s7,
                                       const int16x8_t filter, const int16x8_t horiz_const) {
     int16x4_t filter_lo = vget_low_s16(filter);
@@ -58,7 +58,7 @@ static inline int16x8_t convolve8_8_h(const int16x8_t s0, const int16x8_t s1, co
     return vshrq_n_s16(sum, ROUND0_BITS - 1);
 }
 
-static inline void convolve_horiz_scale_8tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_8tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                   int w, int h, const int16_t *x_filter, const int subpel_x_qn,
                                                   const int x_step_qn) {
     DECLARE_ALIGNED(16, int16_t, temp[8 * 8]);
@@ -171,7 +171,7 @@ static inline void convolve_horiz_scale_8tap_neon(const uint8_t *src, int src_st
     }
 }
 
-static inline int16x4_t convolve6_4_h(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
+static INLINE int16x4_t convolve6_4_h(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
                                       const int16x4_t s4, const int16x4_t s5, const int16x8_t filter,
                                       const int32x4_t horiz_const) {
     int16x4_t filter_lo = vget_low_s16(filter);
@@ -189,7 +189,7 @@ static inline int16x4_t convolve6_4_h(const int16x4_t s0, const int16x4_t s1, co
     return vshrn_n_s32(sum, ROUND0_BITS);
 }
 
-static inline int16x8_t convolve6_8_h(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
+static INLINE int16x8_t convolve6_8_h(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
                                       const int16x8_t s4, const int16x8_t s5, const int16x8_t filter,
                                       const int16x8_t horiz_const) {
     int16x4_t filter_lo = vget_low_s16(filter);
@@ -208,7 +208,7 @@ static inline int16x8_t convolve6_8_h(const int16x8_t s0, const int16x8_t s1, co
     return vshrq_n_s16(sum, ROUND0_BITS - 1);
 }
 
-static inline void convolve_horiz_scale_6tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_6tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                   int w, int h, const int16_t *x_filter, const int subpel_x_qn,
                                                   const int x_step_qn) {
     DECLARE_ALIGNED(16, int16_t, temp[8 * 8]);
@@ -317,7 +317,7 @@ static inline void convolve_horiz_scale_6tap_neon(const uint8_t *src, int src_st
     }
 }
 
-static inline void convolve_horiz_scale_2_8tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_2_8tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                     int w, int h, const int16_t *x_filter) {
     const int bd = 8;
 
@@ -452,7 +452,7 @@ static inline void convolve_horiz_scale_2_8tap_neon(const uint8_t *src, int src_
     }
 }
 
-static inline void convolve_horiz_scale_2_6tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_2_6tap_neon(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                     int w, int h, const int16_t *x_filter) {
     const int bd = 8;
 
diff --git a/Source/Lib/ASM_NEON/convolve_scale_neon.h b/Source/Lib/ASM_NEON/convolve_scale_neon.h
index e92cce4ba8f689692912c255237025ff28a75248..145abd4c6fbad0c980039f41ddef8781136774a7 100644
--- a/Source/Lib/ASM_NEON/convolve_scale_neon.h
+++ b/Source/Lib/ASM_NEON/convolve_scale_neon.h
@@ -17,7 +17,7 @@
 
 #include "mem_neon.h"
 
-static inline int16x4_t compound_convolve8_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+static INLINE int16x4_t compound_convolve8_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
                                                const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
                                                const int16x4_t s6, const int16x4_t s7, const int16x8_t filter,
                                                const int32x4_t offset_const) {
@@ -37,7 +37,7 @@ static inline int16x4_t compound_convolve8_4_v(const int16x4_t s0, const int16x4
     return vshrn_n_s32(sum, COMPOUND_ROUND1_BITS);
 }
 
-static inline int16x8_t compound_convolve8_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+static INLINE int16x8_t compound_convolve8_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
                                                const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
                                                const int16x8_t s6, const int16x8_t s7, const int16x8_t filter,
                                                const int32x4_t offset_const) {
@@ -70,7 +70,7 @@ static inline int16x8_t compound_convolve8_8_v(const int16x8_t s0, const int16x8
     return vcombine_s16(res0, res1);
 }
 
-static inline void compound_convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint16_t *dst,
+static INLINE void compound_convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint16_t *dst,
                                                           int dst_stride, int w, int h, const int16_t *y_filter,
                                                           int subpel_y_qn, int y_step_qn) {
     const int bd          = 8;
@@ -127,7 +127,7 @@ static inline void compound_convolve_vert_scale_8tap_neon(const int16_t *src, in
     }
 }
 
-static inline void compound_avg_convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
+static INLINE void compound_avg_convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
                                                               int dst8_stride, uint16_t *dst16, int dst16_stride, int w,
                                                               int h, const int16_t *y_filter, int subpel_y_qn,
                                                               int y_step_qn) {
@@ -161,7 +161,7 @@ static inline void compound_avg_convolve_vert_scale_8tap_neon(const int16_t *src
 
             uint8x8_t d0_u8 = vqrshrun_n_s16(d0_s16, 2 * FILTER_BITS - ROUND0_BITS - COMPOUND_ROUND1_BITS);
 
-            store_u8_4x1(dst8, d0_u8, 0);
+            store_u8_4x1(dst8, d0_u8);
 
             dst16 += dst16_stride;
             dst8 += dst8_stride;
@@ -205,7 +205,7 @@ static inline void compound_avg_convolve_vert_scale_8tap_neon(const int16_t *src
     }
 }
 
-static inline void compound_dist_wtd_convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
+static INLINE void compound_dist_wtd_convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
                                                                    int dst8_stride, uint16_t *dst16, int dst16_stride,
                                                                    int w, int h, const int16_t *y_filter,
                                                                    ConvolveParams *conv_params, int subpel_y_qn,
@@ -249,7 +249,7 @@ static inline void compound_dist_wtd_convolve_vert_scale_8tap_neon(const int16_t
 
             uint8x8_t d0_u8 = vqmovun_s16(vcombine_s16(d0_s16, vdup_n_s16(0)));
 
-            store_u8_4x1(dst8, d0_u8, 0);
+            store_u8_4x1(dst8, d0_u8);
 
             dst16 += dst16_stride;
             dst8 += dst8_stride;
@@ -302,7 +302,7 @@ static inline void compound_dist_wtd_convolve_vert_scale_8tap_neon(const int16_t
     }
 }
 
-static inline uint8x8_t convolve8_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
+static INLINE uint8x8_t convolve8_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
                                       const int16x4_t s4, const int16x4_t s5, const int16x4_t s6, const int16x4_t s7,
                                       const int16x8_t filter, const int32x4_t offset_const) {
     const int16x4_t filter_0_3 = vget_low_s16(filter);
@@ -323,7 +323,7 @@ static inline uint8x8_t convolve8_4_v(const int16x4_t s0, const int16x4_t s1, co
     return vqmovun_s16(vcombine_s16(res, vdup_n_s16(0)));
 }
 
-static inline uint8x8_t convolve8_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
+static INLINE uint8x8_t convolve8_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
                                       const int16x8_t s4, const int16x8_t s5, const int16x8_t s6, const int16x8_t s7,
                                       const int16x8_t filter, const int32x4_t offset_const) {
     const int16x4_t filter_0_3 = vget_low_s16(filter);
@@ -355,7 +355,7 @@ static inline uint8x8_t convolve8_8_v(const int16x8_t s0, const int16x8_t s1, co
     return vqmovun_s16(vcombine_s16(res0, res1));
 }
 
-static inline void convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint8_t *dst, int dst_stride,
+static INLINE void convolve_vert_scale_8tap_neon(const int16_t *src, int src_stride, uint8_t *dst, int dst_stride,
                                                  int w, int h, const int16_t *y_filter, int subpel_y_qn,
                                                  int y_step_qn) {
     const int bd          = 8;
@@ -377,7 +377,7 @@ static inline void convolve_vert_scale_8tap_neon(const int16_t *src, int src_str
 
             uint8x8_t d = convolve8_4_v(s0, s1, s2, s3, s4, s5, s6, s7, filter, vert_offset);
 
-            store_u8_4x1(dst, d, 0);
+            store_u8_4x1(dst, d);
 
             dst += dst_stride;
             y_qn += y_step_qn;
@@ -431,7 +431,7 @@ static inline void convolve_vert_scale_8tap_neon(const int16_t *src, int src_str
     }
 }
 
-static inline int16x4_t compound_convolve6_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+static INLINE int16x4_t compound_convolve6_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
                                                const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
                                                const int16x8_t filter, const int32x4_t offset_const) {
     const int16x4_t filter_0_3 = vget_low_s16(filter);
@@ -449,7 +449,7 @@ static inline int16x4_t compound_convolve6_4_v(const int16x4_t s0, const int16x4
     return vshrn_n_s32(sum, COMPOUND_ROUND1_BITS);
 }
 
-static inline int16x8_t compound_convolve6_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+static INLINE int16x8_t compound_convolve6_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
                                                const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
                                                const int16x8_t filter, const int32x4_t offset_const) {
     const int16x4_t filter_0_3 = vget_low_s16(filter);
@@ -478,7 +478,7 @@ static inline int16x8_t compound_convolve6_8_v(const int16x8_t s0, const int16x8
     return vcombine_s16(res0, res1);
 }
 
-static inline void compound_convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint16_t *dst,
+static INLINE void compound_convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint16_t *dst,
                                                           int dst_stride, int w, int h, const int16_t *y_filter,
                                                           int subpel_y_qn, int y_step_qn) {
     const int bd          = 8;
@@ -535,7 +535,7 @@ static inline void compound_convolve_vert_scale_6tap_neon(const int16_t *src, in
     }
 }
 
-static inline void compound_avg_convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
+static INLINE void compound_avg_convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
                                                               int dst8_stride, uint16_t *dst16, int dst16_stride, int w,
                                                               int h, const int16_t *y_filter, int subpel_y_qn,
                                                               int y_step_qn) {
@@ -569,7 +569,7 @@ static inline void compound_avg_convolve_vert_scale_6tap_neon(const int16_t *src
 
             uint8x8_t d0_u8 = vqrshrun_n_s16(d0_s16, 2 * FILTER_BITS - ROUND0_BITS - COMPOUND_ROUND1_BITS);
 
-            store_u8_4x1(dst8, d0_u8, 0);
+            store_u8_4x1(dst8, d0_u8);
 
             dst16 += dst16_stride;
             dst8 += dst8_stride;
@@ -613,7 +613,7 @@ static inline void compound_avg_convolve_vert_scale_6tap_neon(const int16_t *src
     }
 }
 
-static inline void compound_dist_wtd_convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
+static INLINE void compound_dist_wtd_convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint8_t *dst8,
                                                                    int dst8_stride, uint16_t *dst16, int dst16_stride,
                                                                    int w, int h, const int16_t *y_filter,
                                                                    ConvolveParams *conv_params, int subpel_y_qn,
@@ -657,7 +657,7 @@ static inline void compound_dist_wtd_convolve_vert_scale_6tap_neon(const int16_t
 
             uint8x8_t d0_u8 = vqmovun_s16(vcombine_s16(d0_s16, vdup_n_s16(0)));
 
-            store_u8_4x1(dst8, d0_u8, 0);
+            store_u8_4x1(dst8, d0_u8);
 
             dst16 += dst16_stride;
             dst8 += dst8_stride;
@@ -710,7 +710,7 @@ static inline void compound_dist_wtd_convolve_vert_scale_6tap_neon(const int16_t
     }
 }
 
-static inline uint8x8_t convolve6_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
+static INLINE uint8x8_t convolve6_4_v(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2, const int16x4_t s3,
                                       const int16x4_t s4, const int16x4_t s5, const int16x8_t filter,
                                       const int32x4_t offset_const) {
     const int16x4_t filter_0_3 = vget_low_s16(filter);
@@ -730,7 +730,7 @@ static inline uint8x8_t convolve6_4_v(const int16x4_t s0, const int16x4_t s1, co
     return vqmovun_s16(vcombine_s16(res, vdup_n_s16(0)));
 }
 
-static inline uint8x8_t convolve6_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
+static INLINE uint8x8_t convolve6_8_v(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2, const int16x8_t s3,
                                       const int16x8_t s4, const int16x8_t s5, const int16x8_t filter,
                                       const int32x4_t offset_const) {
     const int16x4_t filter_0_3 = vget_low_s16(filter);
@@ -759,7 +759,7 @@ static inline uint8x8_t convolve6_8_v(const int16x8_t s0, const int16x8_t s1, co
     return vqmovun_s16(vcombine_s16(res0, res1));
 }
 
-static inline void convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint8_t *dst, int dst_stride,
+static INLINE void convolve_vert_scale_6tap_neon(const int16_t *src, int src_stride, uint8_t *dst, int dst_stride,
                                                  int w, int h, const int16_t *y_filter, int subpel_y_qn,
                                                  int y_step_qn) {
     const int bd          = 8;
@@ -781,7 +781,7 @@ static inline void convolve_vert_scale_6tap_neon(const int16_t *src, int src_str
 
             uint8x8_t d = convolve6_4_v(s0, s1, s2, s3, s4, s5, filter, vert_offset);
 
-            store_u8_4x1(dst, d, 0);
+            store_u8_4x1(dst, d);
 
             dst += dst_stride;
             y_qn += y_step_qn;
diff --git a/Source/Lib/ASM_NEON/deblocking_filter_intrinsic_neon.c b/Source/Lib/ASM_NEON/deblocking_filter_intrinsic_neon.c
index a3211b6860903f2dcce87e7224bdbeba8a30c15a..d1a4166d173fbcc7939af1c4d3373a914b233301 100644
--- a/Source/Lib/ASM_NEON/deblocking_filter_intrinsic_neon.c
+++ b/Source/Lib/ASM_NEON/deblocking_filter_intrinsic_neon.c
@@ -703,7 +703,7 @@ void svt_aom_lpf_vertical_4_neon(uint8_t *src, int stride, const uint8_t *blimit
     // row1: p1 p0 | q0 q1
     // row2: p1 p0 | q0 q1
     // row3: p1 p0 | q0 q1
-    load_unaligned_u8_4x4(src - 2, stride, &p1p0, &q0q1);
+    load_u8_4x2x2(src - 2, stride, &p1p0, &q0q1);
 
     transpose_elems_inplace_u8_4x4(&p1p0, &q0q1);
 
@@ -724,47 +724,28 @@ void svt_aom_lpf_vertical_4_neon(uint8_t *src, int stride, const uint8_t *blimit
 
     transpose_elems_inplace_u8_4x4(&p1p0, &q0q1);
 
-    store_unaligned_u8_4x1(src - 2, p1p0, 0);
-    store_unaligned_u8_4x1((src - 2) + 1 * stride, q0q1, 0);
-    store_unaligned_u8_4x1((src - 2) + 2 * stride, p1p0, 1);
-    store_unaligned_u8_4x1((src - 2) + 3 * stride, q0q1, 1);
+    store_u8x4_strided_x2((src - 2) + 0 * stride, 2 * stride, p1p0);
+    store_u8x4_strided_x2((src - 2) + 1 * stride, 2 * stride, q0q1);
 }
 
 void svt_aom_lpf_horizontal_14_neon(uint8_t *src, int stride, const uint8_t *blimit, const uint8_t *limit,
                                     const uint8_t *thresh) {
-    uint8x8_t p0q0, p1q1, p2q2, p3q3, p4q4, p5q5, p6q6;
-
-    p0q0 = p1q1 = p2q2 = p3q3 = p4q4 = p5q5 = p6q6 = vdup_n_u8(0);
-
-    load_u8_4x1(src - 7 * stride, &p6q6, 0);
-    load_u8_4x1(src - 6 * stride, &p5q5, 0);
-    load_u8_4x1(src - 5 * stride, &p4q4, 0);
-    load_u8_4x1(src - 4 * stride, &p3q3, 0);
-    load_u8_4x1(src - 3 * stride, &p2q2, 0);
-    load_u8_4x1(src - 2 * stride, &p1q1, 0);
-    load_u8_4x1(src - 1 * stride, &p0q0, 0);
-    load_u8_4x1(src + 0 * stride, &p0q0, 1);
-    load_u8_4x1(src + 1 * stride, &p1q1, 1);
-    load_u8_4x1(src + 2 * stride, &p2q2, 1);
-    load_u8_4x1(src + 3 * stride, &p3q3, 1);
-    load_u8_4x1(src + 4 * stride, &p4q4, 1);
-    load_u8_4x1(src + 5 * stride, &p5q5, 1);
-    load_u8_4x1(src + 6 * stride, &p6q6, 1);
+    uint8x8_t p6q6 = load_u8x4_strided_x2(src - 7 * stride, 13 * stride);
+    uint8x8_t p5q5 = load_u8x4_strided_x2(src - 6 * stride, 11 * stride);
+    uint8x8_t p4q4 = load_u8x4_strided_x2(src - 5 * stride, 9 * stride);
+    uint8x8_t p3q3 = load_u8x4_strided_x2(src - 4 * stride, 7 * stride);
+    uint8x8_t p2q2 = load_u8x4_strided_x2(src - 3 * stride, 5 * stride);
+    uint8x8_t p1q1 = load_u8x4_strided_x2(src - 2 * stride, 3 * stride);
+    uint8x8_t p0q0 = load_u8x4_strided_x2(src - 1 * stride, 1 * stride);
 
     lpf_14_neon(&p6q6, &p5q5, &p4q4, &p3q3, &p2q2, &p1q1, &p0q0, *blimit, *limit, *thresh);
 
-    store_u8_4x1(src - 6 * stride, p5q5, 0);
-    store_u8_4x1(src - 5 * stride, p4q4, 0);
-    store_u8_4x1(src - 4 * stride, p3q3, 0);
-    store_u8_4x1(src - 3 * stride, p2q2, 0);
-    store_u8_4x1(src - 2 * stride, p1q1, 0);
-    store_u8_4x1(src - 1 * stride, p0q0, 0);
-    store_u8_4x1(src + 0 * stride, p0q0, 1);
-    store_u8_4x1(src + 1 * stride, p1q1, 1);
-    store_u8_4x1(src + 2 * stride, p2q2, 1);
-    store_u8_4x1(src + 3 * stride, p3q3, 1);
-    store_u8_4x1(src + 4 * stride, p4q4, 1);
-    store_u8_4x1(src + 5 * stride, p5q5, 1);
+    store_u8x4_strided_x2(src - 6 * stride, 11 * stride, p5q5);
+    store_u8x4_strided_x2(src - 5 * stride, 9 * stride, p4q4);
+    store_u8x4_strided_x2(src - 4 * stride, 7 * stride, p3q3);
+    store_u8x4_strided_x2(src - 3 * stride, 5 * stride, p2q2);
+    store_u8x4_strided_x2(src - 2 * stride, 3 * stride, p1q1);
+    store_u8x4_strided_x2(src - 1 * stride, 1 * stride, p0q0);
 }
 
 void svt_aom_lpf_horizontal_8_neon(uint8_t *src, int stride, const uint8_t *blimit, const uint8_t *limit,
@@ -815,19 +796,11 @@ void svt_aom_lpf_horizontal_6_neon(uint8_t *src, int stride, const uint8_t *blim
 
 void svt_aom_lpf_horizontal_4_neon(uint8_t *src, int stride, const uint8_t *blimit, const uint8_t *limit,
                                    const uint8_t *thresh) {
-    uint8x8_t p0q0, p1q1;
-
-    p0q0 = p1q1 = vdup_n_u8(0);
-
-    load_u8_4x1(src - 2 * stride, &p1q1, 0);
-    load_u8_4x1(src - 1 * stride, &p0q0, 0);
-    load_u8_4x1(src + 0 * stride, &p0q0, 1);
-    load_u8_4x1(src + 1 * stride, &p1q1, 1);
+    uint8x8_t p1q1 = load_u8x4_strided_x2(src - 2 * stride, 3 * stride);
+    uint8x8_t p0q0 = load_u8x4_strided_x2(src - 1 * stride, 1 * stride);
 
     lpf_4_neon(&p1q1, &p0q0, *blimit, *limit, *thresh);
 
-    store_u8_4x1(src - 2 * stride, p1q1, 0);
-    store_u8_4x1(src - 1 * stride, p0q0, 0);
-    store_u8_4x1(src + 0 * stride, p0q0, 1);
-    store_u8_4x1(src + 1 * stride, p1q1, 1);
+    store_u8x4_strided_x2(src - 2 * stride, 3 * stride, p1q1);
+    store_u8x4_strided_x2(src - 1 * stride, 1 * stride, p0q0);
 }
diff --git a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
index 9588770d25ab753f9844bb8a984f2c50329cfab8..fcc19564d9d9afb202cdddf7913d1e57a7a36069 100644
--- a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
@@ -74,9 +74,9 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
             } while (--h != 0);
         } else {
             do {
-                uint16x8_t m0 = vmovl_u8(load_unaligned_u8_4x2(mask, mask_stride));
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t m0 = vmovl_u8(load_u8_4x2(mask, mask_stride));
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t blend = alpha_blend_a64_u16x8(m0, s0, s1);
 
@@ -121,8 +121,8 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);
                 uint8x8_t  m2 = vld1_u8(mask + 2 * mask_stride);
                 uint8x8_t  m3 = vld1_u8(mask + 3 * mask_stride);
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);
                 uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
@@ -164,8 +164,8 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
             do {
                 uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);
                 uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));
                 uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
@@ -204,10 +204,10 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
             } while (--h != 0);
         } else {
             do {
-                uint8x8_t  m0_2 = load_unaligned_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
-                uint8x8_t  m1_3 = load_unaligned_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
-                uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);
+                uint8x8_t  m0_2 = load_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
+                uint8x8_t  m1_3 = load_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
+                uint16x8_t s0   = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1   = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0_2, m1_3));
                 uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
@@ -225,7 +225,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
 }
 
 #define HBD_BLEND_A64_D16_MASK(bd, round0_bits)                                                                       \
-    static inline uint16x8_t alpha_##bd##_blend_a64_d16_u16x8(                                                        \
+    static INLINE uint16x8_t alpha_##bd##_blend_a64_d16_u16x8(                                                        \
         uint16x8_t m, uint16x8_t a, uint16x8_t b, int32x4_t round_offset) {                                           \
         const uint16x8_t m_inv = vsubq_u16(vdupq_n_u16(AOM_BLEND_A64_MAX_ALPHA), m);                                  \
                                                                                                                       \
@@ -248,7 +248,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
         return blend_u16;                                                                                             \
     }                                                                                                                 \
                                                                                                                       \
-    static inline void highbd_##bd##_blend_a64_d16_mask_neon(uint16_t            *dst,                                \
+    static INLINE void highbd_##bd##_blend_a64_d16_mask_neon(uint16_t            *dst,                                \
                                                              uint32_t             dst_stride,                         \
                                                              const CONV_BUF_TYPE *src0,                               \
                                                              uint32_t             src0_stride,                        \
@@ -287,9 +287,9 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 } while (--h != 0);                                                                                   \
             } else {                                                                                                  \
                 do {                                                                                                  \
-                    uint16x8_t m0 = vmovl_u8(load_unaligned_u8_4x2(mask, mask_stride));                               \
-                    uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
-                    uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
+                    uint16x8_t m0 = vmovl_u8(load_u8_4x2(mask, mask_stride));                                         \
+                    uint16x8_t s0 = load_u16_4x2(src0, src0_stride);                                                  \
+                    uint16x8_t s1 = load_u16_4x2(src1, src1_stride);                                                  \
                                                                                                                       \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m0, s0, s1, offset);                          \
                                                                                                                       \
@@ -331,8 +331,8 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);                                                  \
                     uint8x8_t  m2 = vld1_u8(mask + 2 * mask_stride);                                                  \
                     uint8x8_t  m3 = vld1_u8(mask + 3 * mask_stride);                                                  \
-                    uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
-                    uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
+                    uint16x8_t s0 = load_u16_4x2(src0, src0_stride);                                                  \
+                    uint16x8_t s1 = load_u16_4x2(src1, src1_stride);                                                  \
                                                                                                                       \
                     uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);                                \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
@@ -372,8 +372,8 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 do {                                                                                                  \
                     uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);                                                  \
                     uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);                                                  \
-                    uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
-                    uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
+                    uint16x8_t s0 = load_u16_4x2(src0, src0_stride);                                                  \
+                    uint16x8_t s1 = load_u16_4x2(src1, src1_stride);                                                  \
                                                                                                                       \
                     uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));                                     \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
@@ -411,10 +411,10 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 } while (--h != 0);                                                                                   \
             } else {                                                                                                  \
                 do {                                                                                                  \
-                    uint8x8_t  m0_2 = load_unaligned_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);                 \
-                    uint8x8_t  m1_3 = load_unaligned_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);                 \
-                    uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);                                      \
-                    uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);                                      \
+                    uint8x8_t  m0_2 = load_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);                           \
+                    uint8x8_t  m1_3 = load_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);                           \
+                    uint16x8_t s0   = load_u16_4x2(src0, src0_stride);                                                \
+                    uint16x8_t s1   = load_u16_4x2(src1, src1_stride);                                                \
                                                                                                                       \
                     uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0_2, m1_3));                                               \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
@@ -503,10 +503,10 @@ void svt_aom_highbd_blend_a64_hmask_16bit_neon(uint16_t *dst, uint32_t dst_strid
             dst += dst_stride;
         } while (--h != 0);
     } else if (w == 4) {
-        const uint16x8_t m0 = vmovl_u8(load_unaligned_dup_u8_4x2(mask));
+        const uint16x8_t m0 = vmovl_u8(load_dup_u8_4x2(mask));
         do {
-            uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-            uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+            uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+            uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
             uint16x8_t blend = alpha_blend_a64_u16x8(m0, s0, s1);
 
@@ -519,10 +519,10 @@ void svt_aom_highbd_blend_a64_hmask_16bit_neon(uint16_t *dst, uint32_t dst_strid
         } while (h != 0);
     } else {
         assert(w == 2);
-        const uint16x4_t m0 = vget_low_u16(vmovl_u8(load_unaligned_dup_u8_2x4(mask)));
+        const uint16x4_t m0 = vget_low_u16(vmovl_u8(load_dup_u8_2x4(mask)));
         do {
-            uint16x4_t s0 = load_unaligned_u16_2x2(src0, src0_stride);
-            uint16x4_t s1 = load_unaligned_u16_2x2(src1, src1_stride);
+            uint16x4_t s0 = load_u16_2x2(src0, src0_stride);
+            uint16x4_t s1 = load_u16_2x2(src1, src1_stride);
 
             uint16x4_t blend = alpha_blend_a64_u16x4(m0, s0, s1);
 
@@ -575,8 +575,8 @@ void svt_aom_highbd_blend_a64_vmask_16bit_neon(uint16_t *dst, uint32_t dst_strid
             uint16x4_t m1 = vdup_n_u16((uint16_t)mask[0]);
             uint16x4_t m2 = vdup_n_u16((uint16_t)mask[1]);
             uint16x8_t m  = vcombine_u16(m1, m2);
-            uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-            uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+            uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+            uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
             uint16x8_t blend = alpha_blend_a64_u16x8(m, s0, s1);
 
@@ -595,8 +595,8 @@ void svt_aom_highbd_blend_a64_vmask_16bit_neon(uint16_t *dst, uint32_t dst_strid
             m0               = vld1_lane_u16((uint16_t *)mask, m0, 0);
             uint8x8_t m0_zip = vzip_u8(vreinterpret_u8_u16(m0), vreinterpret_u8_u16(m0)).val[0];
             m0               = vget_low_u16(vmovl_u8(m0_zip));
-            uint16x4_t s0    = load_unaligned_u16_2x2(src0, src0_stride);
-            uint16x4_t s1    = load_unaligned_u16_2x2(src1, src1_stride);
+            uint16x4_t s0    = load_u16_2x2(src0, src0_stride);
+            uint16x4_t s1    = load_u16_2x2(src1, src1_stride);
 
             uint16x4_t blend = alpha_blend_a64_u16x4(m0, s0, s1);
 
diff --git a/Source/Lib/ASM_NEON/highbd_fwd_txfm_neon.c b/Source/Lib/ASM_NEON/highbd_fwd_txfm_neon.c
index 78bb7738e944174560ae8680749bc43c672fc15c..12e911449f42d7e99ab2300a68764f2121f1c11c 100644
--- a/Source/Lib/ASM_NEON/highbd_fwd_txfm_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_fwd_txfm_neon.c
@@ -49,29 +49,29 @@ static const int32_t av1_cospi_arr_s32_data[4][66] = {
         4717, 6580, 4880, 6458, 5040, 6333, 5197, 6203, 5351, 6070, 5501, 5933, 5649, 5793, 5793,
     }};
 
-static inline const int32_t *cospi_arr_s32(int n) { return av1_cospi_arr_s32_data[n - cos_bit_min]; }
+static INLINE const int32_t *cospi_arr_s32(int n) { return av1_cospi_arr_s32_data[n - cos_bit_min]; }
 
-static inline void ud_adjust_input_and_stride(int ud_flip, int16_t **input, uint32_t *stride, int out_size) {
+static INLINE void ud_adjust_input_and_stride(int ud_flip, int16_t **input, uint32_t *stride, int out_size) {
     if (ud_flip) {
         *input  = *input + (out_size - 1) * *stride;
         *stride = -*stride;
     }
 }
 
-#define LOAD_BUFFER_4XH(h, shift)                                                                                    \
-    static AOM_FORCE_INLINE void load_buffer_4x##h##_(const int16_t *input, int32x4_t *in, int stride, int fliplr) { \
-        if (fliplr) {                                                                                                \
-            for (int i = 0; i < (h); ++i) {                                                                          \
-                int16x4_t a = vld1_s16(input + i * stride);                                                          \
-                a           = vrev64_s16(a);                                                                         \
-                in[i]       = vshll_n_s16(a, shift);                                                                 \
-            }                                                                                                        \
-        } else {                                                                                                     \
-            for (int i = 0; i < (h); ++i) {                                                                          \
-                int16x4_t a = vld1_s16(input + i * stride);                                                          \
-                in[i]       = vshll_n_s16(a, shift);                                                                 \
-            }                                                                                                        \
-        }                                                                                                            \
+#define LOAD_BUFFER_4XH(h, shift)                                                                                 \
+    static AOM_FORCE_INLINE void load_buffer_4x##h(const int16_t *input, int32x4_t *in, int stride, int fliplr) { \
+        if (fliplr) {                                                                                             \
+            for (int i = 0; i < (h); ++i) {                                                                       \
+                int16x4_t a = vld1_s16(input + i * stride);                                                       \
+                a           = vrev64_s16(a);                                                                      \
+                in[i]       = vshll_n_s16(a, shift);                                                              \
+            }                                                                                                     \
+        } else {                                                                                                  \
+            for (int i = 0; i < (h); ++i) {                                                                       \
+                int16x4_t a = vld1_s16(input + i * stride);                                                       \
+                in[i]       = vshll_n_s16(a, shift);                                                              \
+            }                                                                                                     \
+        }                                                                                                         \
     }
 
 LOAD_BUFFER_4XH(4, 2)
@@ -80,29 +80,28 @@ LOAD_BUFFER_4XH(16, 2)
 LOAD_BUFFER_4XH(32, 2)
 LOAD_BUFFER_4XH(64, 0)
 
-#define LOAD_BUFFER_WXH(w, h, shift)                                                    \
-    static AOM_FORCE_INLINE void load_buffer_##w##x##h##_(                              \
-        const int16_t *input, int32x4_t *in, int stride, int fliplr) {                  \
-        assert(w >= 8);                                                                 \
-        if (fliplr) {                                                                   \
-            for (int i = 0; i < (h); ++i) {                                             \
-                for (int j = 0; j < (w) / 8; ++j) {                                     \
-                    int16x8_t a                = vld1q_s16(input + i * stride + j * 8); \
-                    a                          = vrev64q_s16(a);                        \
-                    int j2                     = (w) / 8 - j - 1;                       \
-                    in[i + (h) * (2 * j2 + 0)] = vshll_n_s16(vget_high_s16(a), shift);  \
-                    in[i + (h) * (2 * j2 + 1)] = vshll_n_s16(vget_low_s16(a), shift);   \
-                }                                                                       \
-            }                                                                           \
-        } else {                                                                        \
-            for (int i = 0; i < (h); ++i) {                                             \
-                for (int j = 0; j < (w) / 8; ++j) {                                     \
-                    int16x8_t a               = vld1q_s16(input + i * stride + j * 8);  \
-                    in[i + (h) * (2 * j + 0)] = vshll_n_s16(vget_low_s16(a), shift);    \
-                    in[i + (h) * (2 * j + 1)] = vshll_n_s16(vget_high_s16(a), shift);   \
-                }                                                                       \
-            }                                                                           \
-        }                                                                               \
+#define LOAD_BUFFER_WXH(w, h, shift)                                                                                  \
+    static AOM_FORCE_INLINE void load_buffer_##w##x##h(const int16_t *input, int32x4_t *in, int stride, int fliplr) { \
+        assert(w >= 8);                                                                                               \
+        if (fliplr) {                                                                                                 \
+            for (int i = 0; i < (h); ++i) {                                                                           \
+                for (int j = 0; j < (w) / 8; ++j) {                                                                   \
+                    int16x8_t a                = vld1q_s16(input + i * stride + j * 8);                               \
+                    a                          = vrev64q_s16(a);                                                      \
+                    int j2                     = (w) / 8 - j - 1;                                                     \
+                    in[i + (h) * (2 * j2 + 0)] = vshll_n_s16(vget_high_s16(a), shift);                                \
+                    in[i + (h) * (2 * j2 + 1)] = vshll_n_s16(vget_low_s16(a), shift);                                 \
+                }                                                                                                     \
+            }                                                                                                         \
+        } else {                                                                                                      \
+            for (int i = 0; i < (h); ++i) {                                                                           \
+                for (int j = 0; j < (w) / 8; ++j) {                                                                   \
+                    int16x8_t a               = vld1q_s16(input + i * stride + j * 8);                                \
+                    in[i + (h) * (2 * j + 0)] = vshll_n_s16(vget_low_s16(a), shift);                                  \
+                    in[i + (h) * (2 * j + 1)] = vshll_n_s16(vget_high_s16(a), shift);                                 \
+                }                                                                                                     \
+            }                                                                                                         \
+        }                                                                                                             \
     }
 
 LOAD_BUFFER_WXH(8, 8, 2)
@@ -210,7 +209,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -218,7 +217,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case ADST_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -226,7 +225,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case DCT_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -234,7 +233,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case ADST_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -242,7 +241,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case FLIPADST_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -250,7 +249,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case DCT_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -258,7 +257,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -266,7 +265,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case ADST_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -274,7 +273,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case FLIPADST_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -282,13 +281,13 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case IDTX:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case V_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -296,7 +295,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case H_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
@@ -304,7 +303,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case V_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -312,7 +311,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case H_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
@@ -320,7 +319,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case V_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -328,7 +327,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
         store_buffer_4x4(buf, output, /*stride=*/4);
         break;
     case H_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fidentity4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -340,7 +339,7 @@ void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *output, uint32_t input
 }
 
 #define SHIFT_LOOP_HELPER(name, type, intrinsic, arg)              \
-    static inline void name(const type *in, type *out, int size) { \
+    static INLINE void name(const type *in, type *out, int size) { \
         int i = 0;                                                 \
         do { out[i] = intrinsic(in[i], arg); } while (++i < size); \
     }
@@ -606,7 +605,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -615,7 +614,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case ADST_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -624,7 +623,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case DCT_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -633,7 +632,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case ADST_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -642,7 +641,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case FLIPADST_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -651,7 +650,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case DCT_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fdct8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -660,7 +659,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -669,7 +668,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case ADST_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -678,7 +677,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case FLIPADST_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -687,14 +686,14 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case IDTX:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fidentity8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         highbd_fidentity8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case V_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -703,7 +702,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case H_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fidentity8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -712,7 +711,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case V_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -721,7 +720,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case H_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fidentity8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -730,7 +729,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case V_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -739,7 +738,7 @@ void svt_av1_fwd_txfm2d_8x8_neon(int16_t *input, int32_t *output, uint32_t strid
         store_buffer_8x8(buf0, output, /*stride=*/8);
         break;
     case H_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fidentity8_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 16);
         transpose_arrays_s32_8x8(buf0, buf1);
@@ -1020,7 +1019,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1029,7 +1028,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case ADST_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1038,7 +1037,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case DCT_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1047,7 +1046,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case ADST_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1056,7 +1055,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case FLIPADST_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1065,7 +1064,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case DCT_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fdct16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1074,7 +1073,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1083,7 +1082,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case ADST_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1092,7 +1091,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case FLIPADST_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1101,14 +1100,14 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case IDTX:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fidentity16_xn_neon(buf0, buf1, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf1, buf1, 64);
         highbd_fidentity16_xn_neon(buf1, buf0, fwd_cos_bit_row[2][2], 4);
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case V_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1117,7 +1116,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case H_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fidentity16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1126,7 +1125,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case V_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1135,7 +1134,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case H_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fidentity16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1144,7 +1143,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case V_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1153,7 +1152,7 @@ void svt_av1_fwd_txfm2d_16x16_neon(int16_t *input, int32_t *output, uint32_t str
         store_buffer_16x16(buf0, output, /*stride=*/16);
         break;
     case H_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fidentity16_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4(buf0, buf0, 64);
         transpose_arrays_s32_16x16(buf0, buf1);
@@ -1193,7 +1192,7 @@ typedef void (*fwd_transform_1d_row_many_neon)(const int32x4_t *in, int32x4_t *o
     static void highbd_##name##_col_neon(                                                \
         const int16_t *input, int32x4_t *output, int stride, int cos_bit, int lr_flip) { \
         int32x4_t buf0[n];                                                               \
-        load_buffer_4x##n##_(input, buf0, stride, lr_flip);                              \
+        load_buffer_4x##n(input, buf0, stride, lr_flip);                                 \
         highbd_##name##_x4_neon(buf0, output, cos_bit);                                  \
     }
 
@@ -1203,7 +1202,7 @@ typedef void (*fwd_transform_1d_row_many_neon)(const int32x4_t *in, int32x4_t *o
         int i = 0;                                                                                                   \
         do {                                                                                                         \
             int32x4_t buf0[n];                                                                                       \
-            load_buffer_4x##n##_(input + 4 * i, buf0, stride, lr_flip);                                              \
+            load_buffer_4x##n(input + 4 * i, buf0, stride, lr_flip);                                                 \
             highbd_##name##_x4_neon(buf0, output + i * hm_stride, cos_bit);                                          \
         } while (++i < howmany);                                                                                     \
     }
@@ -2363,7 +2362,7 @@ void svt_av1_fwd_txfm2d_16x64_neon(int16_t *input, int32_t *coeff, uint32_t stri
 
     // Column-wise transform.
     int32x4_t buf0[256];
-    load_buffer_16x64_(input, buf0, stride, lr_flip);
+    load_buffer_16x64(input, buf0, stride, lr_flip);
     for (int i = 0; i < 4; i++) { highbd_fdct64_x4_neon(buf0 + i * 64, buf0 + i * 64, bitcol); }
     shift_right_2_round_s32_x4(buf0, buf0, 256);
 
@@ -2529,7 +2528,7 @@ void svt_av1_fwd_txfm2d_32x64_neon(int16_t *input, int32_t *output, uint32_t str
 
     // Column-wise transform.
     int32x4_t buf0[512];
-    load_buffer_32x64_(input, buf0, stride, 0);
+    load_buffer_32x64(input, buf0, stride, 0);
     for (int i = 0; i < 8; i++) { highbd_fdct64_x4_neon(buf0 + i * 64, buf0 + i * 64, bitcol); }
     shift_right_2_round_s32_x4(buf0, buf0, 512);
 
@@ -2576,7 +2575,7 @@ void svt_av1_fwd_txfm2d_64x16_neon(int16_t *input, int32_t *output, uint32_t str
 
     // Column-wise transform.
     int32x4_t buf0[256];
-    load_buffer_64x16_(input, buf0, stride, lr_flip);
+    load_buffer_64x16(input, buf0, stride, lr_flip);
     highbd_fdct16_xn_neon(buf0, buf0, bitcol, 16);
     shift_right_4_round_s32_x4(buf0, buf0, 256);
 
@@ -2596,7 +2595,7 @@ void svt_av1_fwd_txfm2d_64x32_neon(int16_t *input, int32_t *output, uint32_t str
 
     // Column-wise transform.
     int32x4_t buf0[512];
-    load_buffer_64x32_(input, buf0, stride, 0);
+    load_buffer_64x32(input, buf0, stride, 0);
     for (int i = 0; i < 16; i++) { highbd_fdct32_x4_neon(buf0 + i * 32, buf0 + i * 32, bitcol); }
     shift_right_4_round_s32_x4(buf0, buf0, 512);
 
@@ -2738,63 +2737,63 @@ void svt_av1_fwd_txfm2d_4x4_N4_neon(int16_t *input, int32_t *output, uint32_t in
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case ADST_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case DCT_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case ADST_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case FLIPADST_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case DCT_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case ADST_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case FLIPADST_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -2807,39 +2806,39 @@ void svt_av1_fwd_txfm2d_4x4_N4_neon(int16_t *input, int32_t *output, uint32_t in
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case V_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         highbd_fidentity4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case H_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case V_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         highbd_fidentity4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case H_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case V_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         highbd_fidentity4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         output[0] = vgetq_lane_s32(buf[0], 0);
         break;
     case H_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fidentity4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N4_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -3005,7 +3004,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3015,7 +3014,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case ADST_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3025,7 +3024,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case DCT_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3035,7 +3034,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case ADST_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3045,7 +3044,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case FLIPADST_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3055,7 +3054,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case DCT_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fdct8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3065,7 +3064,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fadst8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3075,7 +3074,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case ADST_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fadst8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3085,7 +3084,7 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case FLIPADST_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3095,22 +3094,22 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case IDTX:
-        load_buffer_4x4_(input, buf0, stride, 0);
+        load_buffer_4x4(input, buf0, stride, 0);
         highbd_fidentity8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         highbd_fidentity8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         write_buffer_8x8_N4(vget_low_s32(buf0[0]), vget_low_s32(buf0[1]), output);
         break;
     case V_DCT:
-        load_buffer_4x8_(input, buf0, stride, 0);
+        load_buffer_4x8(input, buf0, stride, 0);
         highbd_fdct8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         highbd_fidentity8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         write_buffer_8x8_N4(vget_low_s32(buf0[0]), vget_low_s32(buf0[1]), output);
         break;
     case H_DCT:
-        load_buffer_4x4_(input + 0 * 4, buf0 + 0 * 8, stride, 0);
-        load_buffer_4x4_(input + 1 * 4, buf0 + 1 * 8, stride, 0);
+        load_buffer_4x4(input + 0 * 4, buf0 + 0 * 8, stride, 0);
+        load_buffer_4x4(input + 1 * 4, buf0 + 1 * 8, stride, 0);
         highbd_fidentity8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3120,15 +3119,15 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case V_ADST:
-        load_buffer_4x8_(input, buf0, stride, 0);
+        load_buffer_4x8(input, buf0, stride, 0);
         highbd_fadst8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         highbd_fidentity8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         write_buffer_8x8_N4(vget_low_s32(buf0[0]), vget_low_s32(buf0[1]), output);
         break;
     case H_ADST:
-        load_buffer_4x4_(input + 0 * 4, buf0 + 0 * 8, stride, 0);
-        load_buffer_4x4_(input + 1 * 4, buf0 + 1 * 8, stride, 0);
+        load_buffer_4x4(input + 0 * 4, buf0 + 0 * 8, stride, 0);
+        load_buffer_4x4(input + 1 * 4, buf0 + 1 * 8, stride, 0);
         highbd_fidentity8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3138,15 +3137,15 @@ void svt_av1_fwd_txfm2d_8x8_N4_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N4(buf.val[0], buf.val[1], output);
         break;
     case V_FLIPADST:
-        load_buffer_4x8_(input, buf0, stride, 0);
+        load_buffer_4x8(input, buf0, stride, 0);
         highbd_fadst8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         highbd_fidentity8_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         write_buffer_8x8_N4(vget_low_s32(buf0[0]), vget_low_s32(buf0[1]), output);
         break;
     case H_FLIPADST:
-        load_buffer_4x4_(input + 0 * 4, buf0 + 1 * 8, stride, 1);
-        load_buffer_4x4_(input + 1 * 4, buf0 + 0 * 8, stride, 1);
+        load_buffer_4x4(input + 0 * 4, buf0 + 1 * 8, stride, 1);
+        load_buffer_4x4(input + 1 * 4, buf0 + 0 * 8, stride, 1);
         highbd_fidentity8_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_x4(buf0, buf0, 2);
         shift_right_1_round_s32_x4(buf0 + 8, buf0 + 8, 2);
@@ -3404,17 +3403,17 @@ static INLINE void write_buffer_16x16_N4(const int32x4_t *buf, int32_t *output)
 }
 
 static INLINE void load_buffer_16x4_in_16x16(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x4_(input + 0 * 4, buf + 0 * 16, stride, 0);
-    load_buffer_4x4_(input + 1 * 4, buf + 1 * 16, stride, 0);
-    load_buffer_4x4_(input + 2 * 4, buf + 2 * 16, stride, 0);
-    load_buffer_4x4_(input + 3 * 4, buf + 3 * 16, stride, 0);
+    load_buffer_4x4(input + 0 * 4, buf + 0 * 16, stride, 0);
+    load_buffer_4x4(input + 1 * 4, buf + 1 * 16, stride, 0);
+    load_buffer_4x4(input + 2 * 4, buf + 2 * 16, stride, 0);
+    load_buffer_4x4(input + 3 * 4, buf + 3 * 16, stride, 0);
 }
 
 static INLINE void load_buffer_16x4_in_16x16_flip(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x4_(input + 0 * 4, buf + 3 * 16, stride, 1);
-    load_buffer_4x4_(input + 1 * 4, buf + 2 * 16, stride, 1);
-    load_buffer_4x4_(input + 2 * 4, buf + 1 * 16, stride, 1);
-    load_buffer_4x4_(input + 3 * 4, buf + 0 * 16, stride, 1);
+    load_buffer_4x4(input + 0 * 4, buf + 3 * 16, stride, 1);
+    load_buffer_4x4(input + 1 * 4, buf + 2 * 16, stride, 1);
+    load_buffer_4x4(input + 2 * 4, buf + 1 * 16, stride, 1);
+    load_buffer_4x4(input + 3 * 4, buf + 0 * 16, stride, 1);
 }
 
 void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t stride, TxType tx_type, uint8_t bd) {
@@ -3428,7 +3427,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3437,7 +3436,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case ADST_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3446,7 +3445,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case DCT_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3455,7 +3454,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case ADST_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3464,7 +3463,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case FLIPADST_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3473,7 +3472,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case DCT_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fdct16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3482,7 +3481,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fadst16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3491,7 +3490,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case ADST_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fadst16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3500,7 +3499,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case FLIPADST_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_x4_N4(buf0, buf0);
         transpose_16x4_in_16x16(buf0, buf1);
@@ -3509,14 +3508,14 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case IDTX:
-        load_buffer_4x4_(input, buf0, stride, 0);
+        load_buffer_4x4(input, buf0, stride, 0);
         highbd_fidentity16_N4_x4_neon(buf0, buf1, fwd_cos_bit_col[2][2]);
         shift_right_2_round_s32_x4(buf1, buf1, 4);
         highbd_fidentity16_N4_x4_neon(buf1, buf0, fwd_cos_bit_row[2][2]);
         write_buffer_16x16_N4(buf0, output);
         break;
     case V_DCT:
-        load_buffer_4x16_(input, buf0, stride, 0);
+        load_buffer_4x16(input, buf0, stride, 0);
         highbd_fdct16_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[2][2]);
         shift_right_2_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity16_N4_x4_neon(buf0, buf0, fwd_cos_bit_row[2][2]);
@@ -3532,7 +3531,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case V_ADST:
-        load_buffer_4x16_(input, buf0, stride, 0);
+        load_buffer_4x16(input, buf0, stride, 0);
         highbd_fadst16_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[2][2]);
         shift_right_2_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity16_N4_x4_neon(buf0, buf0, fwd_cos_bit_row[2][2]);
@@ -3548,7 +3547,7 @@ void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N4(buf0, output);
         break;
     case V_FLIPADST:
-        load_buffer_4x16_(input, buf0, stride, 0);
+        load_buffer_4x16(input, buf0, stride, 0);
         highbd_fadst16_N4_x4_neon(buf0, buf0, fwd_cos_bit_col[2][2]);
         shift_right_2_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity16_N4_x4_neon(buf0, buf0, fwd_cos_bit_row[2][2]);
@@ -3765,21 +3764,21 @@ static INLINE void transpose_8x8_in_32x32(const int32x4_t *in, int32x4_t *out) {
 }
 
 static INLINE void load_buffer_8x8_in_32x32(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x4_(input + 0 * stride + 0, buf, stride, 0);
-    load_buffer_4x4_(input + 0 * stride + 4, buf + 32, stride, 0);
-    load_buffer_4x4_(input + 4 * stride + 0, buf + 4, stride, 0);
-    load_buffer_4x4_(input + 4 * stride + 4, buf + 32 + 4, stride, 0);
+    load_buffer_4x4(input + 0 * stride + 0, buf, stride, 0);
+    load_buffer_4x4(input + 0 * stride + 4, buf + 32, stride, 0);
+    load_buffer_4x4(input + 4 * stride + 0, buf + 4, stride, 0);
+    load_buffer_4x4(input + 4 * stride + 4, buf + 32 + 4, stride, 0);
 }
 
 static INLINE void load_buffer_32x8_in_32x32(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x8_(input + 0 * 4, buf + 0 * 32, stride, 0);
-    load_buffer_4x8_(input + 1 * 4, buf + 1 * 32, stride, 0);
-    load_buffer_4x8_(input + 2 * 4, buf + 2 * 32, stride, 0);
-    load_buffer_4x8_(input + 3 * 4, buf + 3 * 32, stride, 0);
-    load_buffer_4x8_(input + 4 * 4, buf + 4 * 32, stride, 0);
-    load_buffer_4x8_(input + 5 * 4, buf + 5 * 32, stride, 0);
-    load_buffer_4x8_(input + 6 * 4, buf + 6 * 32, stride, 0);
-    load_buffer_4x8_(input + 7 * 4, buf + 7 * 32, stride, 0);
+    load_buffer_4x8(input + 0 * 4, buf + 0 * 32, stride, 0);
+    load_buffer_4x8(input + 1 * 4, buf + 1 * 32, stride, 0);
+    load_buffer_4x8(input + 2 * 4, buf + 2 * 32, stride, 0);
+    load_buffer_4x8(input + 3 * 4, buf + 3 * 32, stride, 0);
+    load_buffer_4x8(input + 4 * 4, buf + 4 * 32, stride, 0);
+    load_buffer_4x8(input + 5 * 4, buf + 5 * 32, stride, 0);
+    load_buffer_4x8(input + 6 * 4, buf + 6 * 32, stride, 0);
+    load_buffer_4x8(input + 7 * 4, buf + 7 * 32, stride, 0);
 }
 
 void svt_av1_fwd_txfm2d_32x32_N4_neon(int16_t *input, int32_t *output, uint32_t stride, TxType tx_type, uint8_t bd) {
@@ -3790,7 +3789,7 @@ void svt_av1_fwd_txfm2d_32x32_N4_neon(int16_t *input, int32_t *output, uint32_t
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_32x32_(input, buf0, stride, 0);
+        load_buffer_32x32(input, buf0, stride, 0);
         highbd_fdct32_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[3][3], 8);
         shift_right_4_round_s32_x4_N4(buf0, buf0);
         transpose_32x8_in_32x32(buf0, buf1);
@@ -3807,7 +3806,7 @@ void svt_av1_fwd_txfm2d_32x32_N4_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_32x32_N4(buf0, output);
         break;
     case V_DCT:
-        load_buffer_8x32_(input, buf0, stride, 0);
+        load_buffer_8x32(input, buf0, stride, 0);
         highbd_fdct32_N4_xn_neon(buf0, buf0, fwd_cos_bit_col[3][3], 2);
         shift_right_4_round_s32_x4_N4(buf0, buf0);
         highbd_fidentity32_N4_xn_neon(buf0, buf0, fwd_cos_bit_row[3][3], 2);
@@ -4282,7 +4281,7 @@ void svt_av1_fwd_txfm2d_64x64_N4_neon(int16_t *input, int32_t *output, uint32_t
     case DCT_DCT: {
         // Column-wise transform.
         int32x4_t buf0[1024];
-        load_buffer_64x64_(input, buf0, stride, 0);
+        load_buffer_64x64(input, buf0, stride, 0);
         highbd_fdct64_N4_xn_neon(buf0, buf0, 13, 16);
         shift_right_2_round_s32_64x16_N4(buf0, buf0);
 
@@ -4395,7 +4394,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4403,7 +4402,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case ADST_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4411,7 +4410,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case DCT_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4419,7 +4418,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case ADST_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4427,7 +4426,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case FLIPADST_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4435,7 +4434,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case DCT_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4443,7 +4442,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4451,7 +4450,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case ADST_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4459,7 +4458,7 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case FLIPADST_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4467,19 +4466,19 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case IDTX:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         write_buffer_4x4_N2(vget_low_s32(buf[0]), vget_low_s32(buf[1]), output);
         break;
     case V_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         write_buffer_4x4_N2(vget_low_s32(buf[0]), vget_low_s32(buf[1]), output);
         break;
     case H_DCT:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fdct4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
@@ -4487,13 +4486,13 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case V_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         write_buffer_4x4_N2(vget_low_s32(buf[0]), vget_low_s32(buf[1]), output);
         break;
     case H_ADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_col[0][0]);
@@ -4501,13 +4500,13 @@ void svt_av1_fwd_txfm2d_4x4_N2_neon(int16_t *input, int32_t *output, uint32_t in
         write_buffer_4x4_N2(buf0.val[0], buf0.val[1], output);
         break;
     case V_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 0);
+        load_buffer_4x4(input, buf, input_stride, 0);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         write_buffer_4x4_N2(vget_low_s32(buf[0]), vget_low_s32(buf[1]), output);
         break;
     case H_FLIPADST:
-        load_buffer_4x4_(input, buf, input_stride, 1);
+        load_buffer_4x4(input, buf, input_stride, 1);
         highbd_fidentity4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
         transpose_arrays_s32_4x4(buf, buf);
         highbd_fadst4_N2_x4_neon(buf, buf, fwd_cos_bit_row[0][0]);
@@ -4658,13 +4657,13 @@ static INLINE void transpose_8x4_in_8x8_N2(int32x4_t *in, int32x4_t *out) {
 }
 
 static INLINE void load_buffer_8x4_in_8x8(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x4_(input + 0 * 4, buf + 0 * 8, stride, 0);
-    load_buffer_4x4_(input + 1 * 4, buf + 1 * 8, stride, 0);
+    load_buffer_4x4(input + 0 * 4, buf + 0 * 8, stride, 0);
+    load_buffer_4x4(input + 1 * 4, buf + 1 * 8, stride, 0);
 }
 
 static INLINE void load_buffer_8x4_in_8x8_flip(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x4_(input + 0 * 4, buf + 1 * 8, stride, 1);
-    load_buffer_4x4_(input + 1 * 4, buf + 0 * 8, stride, 1);
+    load_buffer_4x4(input + 0 * 4, buf + 1 * 8, stride, 1);
+    load_buffer_4x4(input + 1 * 4, buf + 0 * 8, stride, 1);
 }
 
 static INLINE void write_buffer_8x8_N2(const int32x4_t *buf, int32_t *output) {
@@ -4690,7 +4689,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4699,7 +4698,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case ADST_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4708,7 +4707,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case DCT_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fdct8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4717,7 +4716,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case ADST_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4726,7 +4725,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case FLIPADST_DCT:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4735,7 +4734,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case DCT_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fdct8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4744,7 +4743,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fadst8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4753,7 +4752,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case ADST_FLIPADST:
-        load_buffer_8x8_(input, buf0, stride, 1);
+        load_buffer_8x8(input, buf0, stride, 1);
         highbd_fadst8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4762,7 +4761,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case FLIPADST_ADST:
-        load_buffer_8x8_(input, buf0, stride, 0);
+        load_buffer_8x8(input, buf0, stride, 0);
         highbd_fadst8_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[1][1], 2);
         shift_right_1_round_s32_8x4_N2(buf0, buf0);
         transpose_8x4_in_8x8_N2(buf0, buf1);
@@ -4771,14 +4770,14 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case IDTX:
-        load_buffer_4x4_(input, buf0, stride, 0);
+        load_buffer_4x4(input, buf0, stride, 0);
         highbd_fidentity8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         write_buffer_8x8_N2(buf0, output);
         break;
     case V_DCT:
-        load_buffer_4x8_(input, buf0, stride, 0);
+        load_buffer_4x8(input, buf0, stride, 0);
         highbd_fdct8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
@@ -4794,7 +4793,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case V_ADST:
-        load_buffer_4x8_(input, buf0, stride, 0);
+        load_buffer_4x8(input, buf0, stride, 0);
         highbd_fadst8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
@@ -4810,7 +4809,7 @@ void svt_av1_fwd_txfm2d_8x8_N2_neon(int16_t *input, int32_t *output, uint32_t st
         write_buffer_8x8_N2(buf0, output);
         break;
     case V_FLIPADST:
-        load_buffer_4x8_(input, buf0, stride, 0);
+        load_buffer_4x8(input, buf0, stride, 0);
         highbd_fadst8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
         shift_right_1_round_s32_x4(buf0, buf0, 4);
         highbd_fidentity8_N2_x4_neon(buf0, buf0, fwd_cos_bit_col[1][1]);
@@ -5124,27 +5123,27 @@ static INLINE void transpose_wx8_in_16x16(int32x4_t *in, int32x4_t *out, int wid
 }
 
 static INLINE void load_buffer_8x8_in_16x16(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x8_(input, buf, stride, 0);
-    load_buffer_4x8_(input + 4, buf + 16, stride, 0);
+    load_buffer_4x8(input, buf, stride, 0);
+    load_buffer_4x8(input + 4, buf + 16, stride, 0);
 }
 
 static INLINE void load_buffer_8x16_in_16x16(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x16_(input, buf, stride, 0);
-    load_buffer_4x16_(input + 4, buf + 16, stride, 0);
+    load_buffer_4x16(input, buf, stride, 0);
+    load_buffer_4x16(input + 4, buf + 16, stride, 0);
 }
 
 static INLINE void load_buffer_16x8_in_16x16(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x8_(input + 0 * 4, buf + 0 * 16, stride, 0);
-    load_buffer_4x8_(input + 1 * 4, buf + 1 * 16, stride, 0);
-    load_buffer_4x8_(input + 2 * 4, buf + 2 * 16, stride, 0);
-    load_buffer_4x8_(input + 3 * 4, buf + 3 * 16, stride, 0);
+    load_buffer_4x8(input + 0 * 4, buf + 0 * 16, stride, 0);
+    load_buffer_4x8(input + 1 * 4, buf + 1 * 16, stride, 0);
+    load_buffer_4x8(input + 2 * 4, buf + 2 * 16, stride, 0);
+    load_buffer_4x8(input + 3 * 4, buf + 3 * 16, stride, 0);
 }
 
 static INLINE void load_buffer_16x8_in_16x16_flip(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x8_(input + 0 * 4, buf + 3 * 16, stride, 1);
-    load_buffer_4x8_(input + 1 * 4, buf + 2 * 16, stride, 1);
-    load_buffer_4x8_(input + 2 * 4, buf + 1 * 16, stride, 1);
-    load_buffer_4x8_(input + 3 * 4, buf + 0 * 16, stride, 1);
+    load_buffer_4x8(input + 0 * 4, buf + 3 * 16, stride, 1);
+    load_buffer_4x8(input + 1 * 4, buf + 2 * 16, stride, 1);
+    load_buffer_4x8(input + 2 * 4, buf + 1 * 16, stride, 1);
+    load_buffer_4x8(input + 3 * 4, buf + 0 * 16, stride, 1);
 }
 
 void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t stride, TxType tx_type, uint8_t bd) {
@@ -5158,7 +5157,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5167,7 +5166,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case ADST_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5176,7 +5175,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case DCT_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fdct16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5185,7 +5184,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case ADST_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5194,7 +5193,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case FLIPADST_DCT:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5203,7 +5202,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case DCT_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fdct16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5212,7 +5211,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case FLIPADST_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fadst16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5221,7 +5220,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case ADST_FLIPADST:
-        load_buffer_16x16_(input, buf0, stride, 1);
+        load_buffer_16x16(input, buf0, stride, 1);
         highbd_fadst16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5230,7 +5229,7 @@ void svt_av1_fwd_txfm2d_16x16_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_16x16_N2(buf0, output);
         break;
     case FLIPADST_ADST:
-        load_buffer_16x16_(input, buf0, stride, 0);
+        load_buffer_16x16(input, buf0, stride, 0);
         highbd_fadst16_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[2][2], 4);
         shift_right_2_round_s32_16x8_N2(buf0, buf0);
         transpose_wx8_in_16x16(buf0, buf1, 4);
@@ -5520,34 +5519,34 @@ static INLINE void write_buffer_32x32_N2(const int32x4_t *in, int32_t *output) {
 }
 
 static INLINE void load_buffer_16x16_in_32x32(const int16_t *input, int32x4_t *buf, int stride) {
-    load_buffer_4x4_(input + 0 * stride + 0, buf + 0 + 0 * 32, stride, 0);
-    load_buffer_4x4_(input + 0 * stride + 4, buf + 0 + 1 * 32, stride, 0);
-    load_buffer_4x4_(input + 0 * stride + 8, buf + 0 + 2 * 32, stride, 0);
-    load_buffer_4x4_(input + 0 * stride + 12, buf + 0 + 3 * 32, stride, 0);
-    load_buffer_4x4_(input + 4 * stride + 0, buf + 4 + 0 * 32, stride, 0);
-    load_buffer_4x4_(input + 4 * stride + 4, buf + 4 + 1 * 32, stride, 0);
-    load_buffer_4x4_(input + 4 * stride + 8, buf + 4 + 2 * 32, stride, 0);
-    load_buffer_4x4_(input + 4 * stride + 12, buf + 4 + 3 * 32, stride, 0);
-    load_buffer_4x4_(input + 8 * stride + 0, buf + 8 + 0 * 32, stride, 0);
-    load_buffer_4x4_(input + 8 * stride + 4, buf + 8 + 1 * 32, stride, 0);
-    load_buffer_4x4_(input + 8 * stride + 8, buf + 8 + 2 * 32, stride, 0);
-    load_buffer_4x4_(input + 8 * stride + 12, buf + 8 + 3 * 32, stride, 0);
-    load_buffer_4x4_(input + 12 * stride + 0, buf + 12 + 0 * 32, stride, 0);
-    load_buffer_4x4_(input + 12 * stride + 4, buf + 12 + 1 * 32, stride, 0);
-    load_buffer_4x4_(input + 12 * stride + 8, buf + 12 + 2 * 32, stride, 0);
-    load_buffer_4x4_(input + 12 * stride + 12, buf + 12 + 3 * 32, stride, 0);
+    load_buffer_4x4(input + 0 * stride + 0, buf + 0 + 0 * 32, stride, 0);
+    load_buffer_4x4(input + 0 * stride + 4, buf + 0 + 1 * 32, stride, 0);
+    load_buffer_4x4(input + 0 * stride + 8, buf + 0 + 2 * 32, stride, 0);
+    load_buffer_4x4(input + 0 * stride + 12, buf + 0 + 3 * 32, stride, 0);
+    load_buffer_4x4(input + 4 * stride + 0, buf + 4 + 0 * 32, stride, 0);
+    load_buffer_4x4(input + 4 * stride + 4, buf + 4 + 1 * 32, stride, 0);
+    load_buffer_4x4(input + 4 * stride + 8, buf + 4 + 2 * 32, stride, 0);
+    load_buffer_4x4(input + 4 * stride + 12, buf + 4 + 3 * 32, stride, 0);
+    load_buffer_4x4(input + 8 * stride + 0, buf + 8 + 0 * 32, stride, 0);
+    load_buffer_4x4(input + 8 * stride + 4, buf + 8 + 1 * 32, stride, 0);
+    load_buffer_4x4(input + 8 * stride + 8, buf + 8 + 2 * 32, stride, 0);
+    load_buffer_4x4(input + 8 * stride + 12, buf + 8 + 3 * 32, stride, 0);
+    load_buffer_4x4(input + 12 * stride + 0, buf + 12 + 0 * 32, stride, 0);
+    load_buffer_4x4(input + 12 * stride + 4, buf + 12 + 1 * 32, stride, 0);
+    load_buffer_4x4(input + 12 * stride + 8, buf + 12 + 2 * 32, stride, 0);
+    load_buffer_4x4(input + 12 * stride + 12, buf + 12 + 3 * 32, stride, 0);
 }
 
 static INLINE void load_buffer_32x16_in_32x32(const int16_t *input, int32x4_t *buf, int stride) {
     for (int i = 0; i < 16; i++) {
-        load_buffer_4x4_(input + i * stride + 0, buf + i + 0 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 4, buf + i + 1 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 8, buf + i + 2 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 12, buf + i + 3 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 16, buf + i + 4 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 20, buf + i + 5 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 24, buf + i + 6 * 32, stride, 0);
-        load_buffer_4x4_(input + i * stride + 28, buf + i + 7 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 0, buf + i + 0 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 4, buf + i + 1 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 8, buf + i + 2 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 12, buf + i + 3 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 16, buf + i + 4 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 20, buf + i + 5 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 24, buf + i + 6 * 32, stride, 0);
+        load_buffer_4x4(input + i * stride + 28, buf + i + 7 * 32, stride, 0);
     }
 }
 
@@ -5559,7 +5558,7 @@ void svt_av1_fwd_txfm2d_32x32_N2_neon(int16_t *input, int32_t *output, uint32_t
 
     switch (tx_type) {
     case DCT_DCT:
-        load_buffer_32x32_(input, buf0, stride, 0);
+        load_buffer_32x32(input, buf0, stride, 0);
         highbd_fdct32_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[3][3], 8);
         shift_right_4_round_s32_x4_N2(buf0, buf0);
         transpose_wx16_in_32x32(buf0, buf1, 8);
@@ -5578,7 +5577,7 @@ void svt_av1_fwd_txfm2d_32x32_N2_neon(int16_t *input, int32_t *output, uint32_t
         write_buffer_32x32_N2(buf0, output);
         break;
     case V_DCT:
-        load_buffer_16x32_(input, buf0, stride, 0);
+        load_buffer_16x32(input, buf0, stride, 0);
         highbd_fdct32_N2_xn_neon(buf0, buf0, fwd_cos_bit_col[3][3], 4);
         shift_right_4_round_s32_x4(buf0 + 0 * 32, buf0 + 0 * 32, 16);
         shift_right_4_round_s32_x4(buf0 + 1 * 32, buf0 + 1 * 32, 16);
@@ -6008,7 +6007,7 @@ void svt_av1_fwd_txfm2d_64x64_N2_neon(int16_t *input, int32_t *output, uint32_t
     case DCT_DCT: {
         // Column-wise transform.
         int32x4_t buf0[1024];
-        load_buffer_64x64_(input, buf0, stride, 0);
+        load_buffer_64x64(input, buf0, stride, 0);
         highbd_fdct64_N2_xn_neon(buf0, buf0, 13, 16);
         shift_right_2_round_s32_wx32_N2(buf0, buf0, 16);
 
@@ -6024,7 +6023,7 @@ void svt_av1_fwd_txfm2d_64x64_N2_neon(int16_t *input, int32_t *output, uint32_t
     }
     case IDTX: {
         int32x4_t buf0[1024];
-        load_buffer_64x64_(input, buf0, stride, 0);
+        load_buffer_64x64(input, buf0, stride, 0);
 
         // Column-wise transform.
         int32x4_t buf1[1024];
@@ -6698,7 +6697,7 @@ void svt_av1_fwd_txfm2d_16x64_N2_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[256];
-    load_buffer_16x64_(input, buf0, stride, lr_flip);
+    load_buffer_16x64(input, buf0, stride, lr_flip);
     highbd_fdct64_N2_xn_neon(buf0, buf0, bitcol, 4);
     for (int i = 0; i < 4; i++) { shift_right_2_round_s32_x4(buf0 + i * 64, buf0 + i * 64, 32); }
 
@@ -6828,7 +6827,7 @@ void svt_av1_fwd_txfm2d_32x64_N2_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[512];
-    load_buffer_32x64_(input, buf0, stride, 0);
+    load_buffer_32x64(input, buf0, stride, 0);
     highbd_fdct64_N2_xn_neon(buf0, buf0, bitcol, 8);
     for (int i = 0; i < 8; i++) { shift_right_2_round_s32_x4(buf0 + i * 64, buf0 + i * 64, 32); }
 
@@ -6871,7 +6870,7 @@ void svt_av1_fwd_txfm2d_64x16_N2_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[256];
-    load_buffer_64x16_(input, buf0, stride, lr_flip);
+    load_buffer_64x16(input, buf0, stride, lr_flip);
     highbd_fdct16_N2_xn_neon(buf0, buf0, bitcol, 16);
     for (int i = 0; i < 16; i++) { shift_right_4_round_s32_x4(buf0 + i * 16, buf0 + i * 16, 8); }
 
@@ -6892,7 +6891,7 @@ void svt_av1_fwd_txfm2d_64x32_N2_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[512];
-    load_buffer_64x32_(input, buf0, stride, 0);
+    load_buffer_64x32(input, buf0, stride, 0);
     highbd_fdct32_N2_xn_neon(buf0, buf0, bitcol, 16);
     for (int i = 0; i < 16; i++) { shift_right_4_round_s32_x4(buf0 + i * 32, buf0 + i * 32, 16); }
 
@@ -7528,7 +7527,7 @@ void svt_av1_fwd_txfm2d_16x64_N4_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[256];
-    load_buffer_16x64_(input, buf0, stride, lr_flip);
+    load_buffer_16x64(input, buf0, stride, lr_flip);
     highbd_fdct64_N4_xn_neon(buf0, buf0, bitcol, 4);
     for (int i = 0; i < 4; i++) { shift_right_2_round_s32_x4(buf0 + i * 64, buf0 + i * 64, 16); }
 
@@ -7655,7 +7654,7 @@ void svt_av1_fwd_txfm2d_32x64_N4_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[512];
-    load_buffer_32x64_(input, buf0, stride, 0);
+    load_buffer_32x64(input, buf0, stride, 0);
     highbd_fdct64_N4_xn_neon(buf0, buf0, bitcol, 8);
     for (int i = 0; i < 8; i++) { shift_right_2_round_s32_x4(buf0 + i * 64, buf0 + i * 64, 16); }
 
@@ -7690,7 +7689,7 @@ void svt_av1_fwd_txfm2d_64x16_N4_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[256];
-    load_buffer_64x16_(input, buf0, stride, lr_flip);
+    load_buffer_64x16(input, buf0, stride, lr_flip);
     highbd_fdct16_N4_xn_neon(buf0, buf0, bitcol, 16);
     for (int i = 0; i < 16; i++) { shift_right_4_round_s32_x4(buf0 + i * 16, buf0 + i * 16, 4); }
 
@@ -7711,7 +7710,7 @@ void svt_av1_fwd_txfm2d_64x32_N4_neon(int16_t *input, int32_t *output, uint32_t
 
     // Column-wise transform.
     int32x4_t buf0[512];
-    load_buffer_64x32_(input, buf0, stride, 0);
+    load_buffer_64x32(input, buf0, stride, 0);
     highbd_fdct32_N2_xn_neon(buf0, buf0, bitcol, 16);
     for (int i = 0; i < 16; i++) { shift_right_4_round_s32_x4(buf0 + i * 32, buf0 + i * 32, 8); }
 
diff --git a/Source/Lib/ASM_NEON/highbd_inter_prediction_neon.c b/Source/Lib/ASM_NEON/highbd_inter_prediction_neon.c
index 69257aa6bd81ec78e77938dd906ec5bf5f86fd21..c4ddbf7b428b9b68367680d30053d42c6a9eb4d7 100644
--- a/Source/Lib/ASM_NEON/highbd_inter_prediction_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_inter_prediction_neon.c
@@ -95,8 +95,8 @@ static INLINE void diffwtd_mask_highbd_neon(uint8_t *mask, bool inverse, const u
             } while (--h != 0);
         } else if (w == 4) {
             do {
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t diff_u16 = vabdq_u16(s0, s1);
                 uint8x8_t  diff_u8  = vshrn_n_u16(diff_u16, DIFF_FACTOR_LOG2);
@@ -184,8 +184,8 @@ static INLINE void diffwtd_mask_highbd_neon(uint8_t *mask, bool inverse, const u
             } while (--h != 0);
         } else if (w == 4) {
             do {
-                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
-                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+                uint16x8_t s0 = load_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_u16_4x2(src1, src1_stride);
 
                 uint16x8_t diff_u16 = vabdq_u16(s0, s1);
                 uint8x8_t  diff_u8  = vshrn_n_u16(diff_u16, 2 + DIFF_FACTOR_LOG2);
diff --git a/Source/Lib/ASM_NEON/highbd_intra_prediction_neon.c b/Source/Lib/ASM_NEON/highbd_intra_prediction_neon.c
index 642ba2550fc0ff354b0d731201f971dad07c3022..de2f046d9b86c59393d1e7d44bc53a35f90621dd 100644
--- a/Source/Lib/ASM_NEON/highbd_intra_prediction_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_intra_prediction_neon.c
@@ -309,7 +309,7 @@ HIGHBD_SMOOTH_V_NXM_WIDE(64, 64)
 #undef HIGHBD_SMOOTH_V_NXM_WIDE
 
 // 256 - v = vneg_s8(v)
-static inline uint16x4_t negate_s8(const uint16x4_t v) { return vreinterpret_u16_s8(vneg_s8(vreinterpret_s8_u16(v))); }
+static INLINE uint16x4_t negate_s8(const uint16x4_t v) { return vreinterpret_u16_s8(vneg_s8(vreinterpret_s8_u16(v))); }
 
 static INLINE void highbd_smooth_h_4xh_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *const top_row,
                                             const uint16_t *left_column, int height) {
@@ -1385,14 +1385,14 @@ static const uint8_t kLoadMaxShuffles[] = {
 };
 // clang-format on
 
-static inline uint16x8_t zn_load_masked_neon(const uint16_t *ptr, int shuffle_idx) {
+static INLINE uint16x8_t zn_load_masked_neon(const uint16_t *ptr, int shuffle_idx) {
     uint8x16_t shuffle = vld1q_u8(&kLoadMaxShuffles[16 * shuffle_idx]);
     uint8x16_t src     = vreinterpretq_u8_u16(vld1q_u16(ptr));
     return vreinterpretq_u16_u8(vqtbl1q_u8(src, shuffle));
 }
 
-static void highbd_dr_prediction_z1_upsample0_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
-                                                   const uint16_t *above, int dx) {
+static INLINE void highbd_dr_prediction_z1_upsample0_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
+                                                          const uint16_t *above, int dx) {
     assert(bw % 4 == 0);
     assert(bh % 4 == 0);
     assert(dx > 0);
@@ -1460,8 +1460,8 @@ static void highbd_dr_prediction_z1_upsample0_neon(uint16_t *dst, ptrdiff_t stri
     } while (++r < bh);
 }
 
-static void highbd_dr_prediction_z1_upsample1_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
-                                                   const uint16_t *above, int dx) {
+static INLINE void highbd_dr_prediction_z1_upsample1_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
+                                                          const uint16_t *above, int dx) {
     assert(bw % 4 == 0);
     assert(bh % 4 == 0);
     assert(dx > 0);
@@ -1666,13 +1666,13 @@ static AOM_FORCE_INLINE uint16x4x2_t highbd_dr_prediction_z2_gather_left_x4(cons
 
     // At time of writing both Clang and GCC produced better code with these
     // nested if-statements compared to a switch statement with fallthrough.
-    load_unaligned_u32_2x1_lane(ret0_u32, left + idx0, 0);
+    load_u32_2x1_lane(ret0_u32, left + idx0, 0);
     if (n > 1) {
-        load_unaligned_u32_2x1_lane(ret0_u32, left + idx1, 1);
+        load_u32_2x1_lane(ret0_u32, left + idx1, 1);
         if (n > 2) {
-            load_unaligned_u32_2x1_lane(ret1_u32, left + idx2, 0);
+            load_u32_2x1_lane(ret1_u32, left + idx2, 0);
             if (n > 3) {
-                load_unaligned_u32_2x1_lane(ret1_u32, left + idx3, 1);
+                load_u32_2x1_lane(ret1_u32, left + idx3, 1);
             }
         }
     }
@@ -1703,21 +1703,21 @@ static AOM_FORCE_INLINE uint16x8x2_t highbd_dr_prediction_z2_gather_left_x8(cons
 
     // At time of writing both Clang and GCC produced better code with these
     // nested if-statements compared to a switch statement with fallthrough.
-    load_unaligned_u32_4x1_lane(ret0_u32, left + idx0, 0);
+    load_u32_4x1_lane(ret0_u32, left + idx0, 0);
     if (n > 1) {
-        load_unaligned_u32_4x1_lane(ret0_u32, left + idx1, 1);
+        load_u32_4x1_lane(ret0_u32, left + idx1, 1);
         if (n > 2) {
-            load_unaligned_u32_4x1_lane(ret0_u32, left + idx2, 2);
+            load_u32_4x1_lane(ret0_u32, left + idx2, 2);
             if (n > 3) {
-                load_unaligned_u32_4x1_lane(ret0_u32, left + idx3, 3);
+                load_u32_4x1_lane(ret0_u32, left + idx3, 3);
                 if (n > 4) {
-                    load_unaligned_u32_4x1_lane(ret1_u32, left + idx4, 0);
+                    load_u32_4x1_lane(ret1_u32, left + idx4, 0);
                     if (n > 5) {
-                        load_unaligned_u32_4x1_lane(ret1_u32, left + idx5, 1);
+                        load_u32_4x1_lane(ret1_u32, left + idx5, 1);
                         if (n > 6) {
-                            load_unaligned_u32_4x1_lane(ret1_u32, left + idx6, 2);
+                            load_u32_4x1_lane(ret1_u32, left + idx6, 2);
                             if (n > 7) {
-                                load_unaligned_u32_4x1_lane(ret1_u32, left + idx7, 3);
+                                load_u32_4x1_lane(ret1_u32, left + idx7, 3);
                             }
                         }
                     }
@@ -1855,15 +1855,15 @@ static AOM_FORCE_INLINE uint16x8_t highbd_dr_prediction_z2_step_x8(const uint16_
 // Left array is accessed from -1 through `bh - 1` inclusive.
 // Above array is accessed from -1 through `bw - 1` inclusive.
 #define HIGHBD_DR_PREDICTOR_Z2_WXH(bw, bh)                                                                       \
-    static void highbd_dr_prediction_z2_##bw##x##bh##_neon(uint16_t       *dst,                                  \
-                                                           ptrdiff_t       stride,                               \
-                                                           const uint16_t *above,                                \
-                                                           const uint16_t *left,                                 \
-                                                           int             upsample_above,                       \
-                                                           int             upsample_left,                        \
-                                                           int             dx,                                   \
-                                                           int             dy,                                   \
-                                                           int             bd) {                                             \
+    static INLINE void highbd_dr_prediction_z2_##bw##x##bh##_neon(uint16_t       *dst,                           \
+                                                                  ptrdiff_t       stride,                        \
+                                                                  const uint16_t *above,                         \
+                                                                  const uint16_t *left,                          \
+                                                                  int             upsample_above,                \
+                                                                  int             upsample_left,                 \
+                                                                  int             dx,                            \
+                                                                  int             dy,                            \
+                                                                  int             bd) {                                      \
         (void)bd;                                                                                                \
         (void)upsample_above;                                                                                    \
         (void)upsample_left;                                                                                     \
@@ -1925,9 +1925,9 @@ typedef void (*highbd_dr_prediction_z2_ptr)(uint16_t *dst, ptrdiff_t stride, con
                                             const uint16_t *left, int upsample_above, int upsample_left, int dx, int dy,
                                             int bd);
 
-static void highbd_dr_prediction_z2_4x4_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
-                                             const uint16_t *left, int upsample_above, int upsample_left, int dx,
-                                             int dy, int bd) {
+static INLINE void highbd_dr_prediction_z2_4x4_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
+                                                    const uint16_t *left, int upsample_above, int upsample_left, int dx,
+                                                    int dy, int bd) {
     (void)bd;
     assert(dx > 0);
     assert(dy > 0);
@@ -2016,9 +2016,9 @@ static void highbd_dr_prediction_z2_4x4_neon(uint16_t *dst, ptrdiff_t stride, co
     }
 }
 
-static void highbd_dr_prediction_z2_4x8_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
-                                             const uint16_t *left, int upsample_above, int upsample_left, int dx,
-                                             int dy, int bd) {
+static INLINE void highbd_dr_prediction_z2_4x8_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
+                                                    const uint16_t *left, int upsample_above, int upsample_left, int dx,
+                                                    int dy, int bd) {
     (void)bd;
     assert(dx > 0);
     assert(dy > 0);
@@ -2111,9 +2111,9 @@ static void highbd_dr_prediction_z2_4x8_neon(uint16_t *dst, ptrdiff_t stride, co
     }
 }
 
-static void highbd_dr_prediction_z2_8x4_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
-                                             const uint16_t *left, int upsample_above, int upsample_left, int dx,
-                                             int dy, int bd) {
+static INLINE void highbd_dr_prediction_z2_8x4_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
+                                                    const uint16_t *left, int upsample_above, int upsample_left, int dx,
+                                                    int dy, int bd) {
     (void)bd;
     assert(dx > 0);
     assert(dy > 0);
@@ -2206,9 +2206,9 @@ static void highbd_dr_prediction_z2_8x4_neon(uint16_t *dst, ptrdiff_t stride, co
     }
 }
 
-static void highbd_dr_prediction_z2_8x8_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
-                                             const uint16_t *left, int upsample_above, int upsample_left, int dx,
-                                             int dy, int bd) {
+static INLINE void highbd_dr_prediction_z2_8x8_neon(uint16_t *dst, ptrdiff_t stride, const uint16_t *above,
+                                                    const uint16_t *left, int upsample_above, int upsample_left, int dx,
+                                                    int dy, int bd) {
     (void)bd;
     assert(dx > 0);
     assert(dy > 0);
@@ -2372,7 +2372,7 @@ void svt_av1_highbd_dr_prediction_z2_neon(uint16_t *dst, ptrdiff_t stride, int b
         *(out)            = vcombine_u16(vrshrn_n_u32(val_lo, (shift)), vrshrn_n_u32(val_hi, (shift))); \
     } while (0)
 
-static inline uint16x8x2_t z3_load_left_neon(const uint16_t *left0, int ofs, int max_ofs) {
+static INLINE uint16x8x2_t z3_load_left_neon(const uint16_t *left0, int ofs, int max_ofs) {
     uint16x8_t r0;
     uint16x8_t r1;
     if (ofs + 7 >= max_ofs) {
@@ -2390,8 +2390,8 @@ static inline uint16x8x2_t z3_load_left_neon(const uint16_t *left0, int ofs, int
     return (uint16x8x2_t){{r0, r1}};
 }
 
-static void highbd_dr_prediction_z3_upsample0_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
-                                                   const uint16_t *left, int dy) {
+static INLINE void highbd_dr_prediction_z3_upsample0_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
+                                                          const uint16_t *left, int dy) {
     assert(bw % 4 == 0);
     assert(bh % 4 == 0);
     assert(dy > 0);
@@ -2514,8 +2514,8 @@ static void highbd_dr_prediction_z3_upsample0_neon(uint16_t *dst, ptrdiff_t stri
     }
 }
 
-static void highbd_dr_prediction_z3_upsample1_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
-                                                   const uint16_t *left, int dy) {
+static INLINE void highbd_dr_prediction_z3_upsample1_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh,
+                                                          const uint16_t *left, int dy) {
     assert(bw % 4 == 0);
     assert(bh % 4 == 0);
     assert(dy > 0);
diff --git a/Source/Lib/ASM_NEON/highbd_inv_txfm_neon.c b/Source/Lib/ASM_NEON/highbd_inv_txfm_neon.c
index 9218b96bc188fdffd0875d59f12987b2b5ec6b5f..df9e69a11df2ecd7f1104c2f4d8d35c6b33b497b 100644
--- a/Source/Lib/ASM_NEON/highbd_inv_txfm_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_inv_txfm_neon.c
@@ -914,7 +914,7 @@ static void idct64_x4_neon(int32x4_t in[], int32x4_t out[], int32_t bit) {
     for (int32_t i = 0; i < 32; i++) { addsub_neon(v[i], v[63 - i], &out[i], &out[63 - i]); }
 }
 
-static void idct64_xn_neon(int32x4_t in[], int32x4_t out[], int32_t bit, int howmany) {
+static INLINE void idct64_xn_neon(int32x4_t in[], int32x4_t out[], int32_t bit, int howmany) {
     const int stride = 64;
     int       i      = 0;
     do { idct64_x4_neon(in + i * stride, out + i * stride, bit); } while (++i < howmany);
@@ -2218,7 +2218,7 @@ void svt_av1_inv_txfm2d_add_4x4_neon(const int32_t *input, uint16_t *output_r, i
     }
 }
 
-static void identity8_x4_neon(int32x4_t *in, int32x4_t *out, int bit) {
+static INLINE void identity8_x4_neon(int32x4_t *in, int32x4_t *out, int bit) {
     (void)bit;
     out[0] = vaddq_s32(in[0], in[0]);
     out[1] = vaddq_s32(in[1], in[1]);
@@ -2236,7 +2236,7 @@ static INLINE void iidentity8_xn_neon(int32x4_t *in, int32x4_t *out, int bit, in
     do { identity8_x4_neon(in + i * stride, out + i * stride, bit); } while (++i < howmany);
 }
 
-static void identity32_x4_neon(int32x4_t *in, int32x4_t *out, int bit) {
+static INLINE void identity32_x4_neon(int32x4_t *in, int32x4_t *out, int bit) {
     (void)bit;
     for (int i = 0; i < 32; i++) { out[i] = vshlq_n_s32(in[i], 2); }
 }
@@ -2247,14 +2247,14 @@ static INLINE void iidentity32_xn_neon(int32x4_t *in, int32x4_t *out, int bit, i
     do { identity32_x4_neon(in + i * stride, out + i * stride, bit); } while (++i < howmany);
 }
 
-static inline void round_shift_rect_array_32_neon(int32x4_t *input, int32x4_t *output, const int size) {
+static INLINE void round_shift_rect_array_32_neon(int32x4_t *input, int32x4_t *output, const int size) {
     for (int i = 0; i < size; i++) {
         const int32x4_t r0 = vmulq_n_s32(input[i], new_inv_sqrt2);
         output[i]          = vrshrq_n_s32(r0, new_sqrt2_bits);
     }
 }
 
-static inline uint16x8_t highbd_get_recon_4xn_neon(int16x4_t pred0, int16x4_t pred1, int32x4_t res0, int32x4_t res1,
+static INLINE uint16x8_t highbd_get_recon_4xn_neon(int16x4_t pred0, int16x4_t pred1, int32x4_t res0, int32x4_t res1,
                                                    const int bd) {
     uint16x4_t x0_ = vqmovun_s32(vaddw_s16(res0, pred0));
     uint16x4_t x1_ = vqmovun_s32(vaddw_s16(res1, pred1));
@@ -2262,7 +2262,7 @@ static inline uint16x8_t highbd_get_recon_4xn_neon(int16x4_t pred0, int16x4_t pr
     return vminq_u16(x, vdupq_n_u16((1 << bd) - 1));
 }
 
-static inline void highbd_write_buffer_4xn_neon(int32x4_t *in, uint16_t *output_r, int stride_r, uint16_t *output_w,
+static INLINE void highbd_write_buffer_4xn_neon(int32x4_t *in, uint16_t *output_r, int stride_r, uint16_t *output_w,
                                                 int stride_w, int flipud, int height, const int bd) {
     int       j    = flipud ? (height - 1) : 0;
     const int step = flipud ? -1 : 1;
@@ -2426,7 +2426,7 @@ void svt_av1_inv_txfm2d_add_4x16_neon(const int32_t *input, uint16_t *output_r,
     highbd_write_buffer_4xn_neon(buf1, output_r, stride_r, output_w, stride_w, ud_flip, txfm_size_row, bd);
 }
 
-static inline uint16x8_t highbd_get_recon_8x8_neon(const int16x8_t pred, int32x4_t res0, int32x4_t res1, const int bd) {
+static INLINE uint16x8_t highbd_get_recon_8x8_neon(const int16x8_t pred, int32x4_t res0, int32x4_t res1, const int bd) {
     int32x4_t max_clip_val = vdupq_n_s32((1 << bd) - 1);
     res0                   = vaddw_s16(res0, vget_low_s16(pred));
     res1                   = vaddw_s16(res1, vget_high_s16(pred));
@@ -2436,7 +2436,7 @@ static inline uint16x8_t highbd_get_recon_8x8_neon(const int16x8_t pred, int32x4
     return vcombine_u16(vqmovun_s32(res0), vqmovun_s32(res1));
 }
 
-static inline void highbd_write_buffer_8xn_neon(int32x4_t *in, uint16_t *output_r, int32_t stride_r, uint16_t *output_w,
+static INLINE void highbd_write_buffer_8xn_neon(int32x4_t *in, uint16_t *output_r, int32_t stride_r, uint16_t *output_w,
                                                 int32_t stride_w, int flipud, int height, const int bd) {
     int       j    = flipud ? (height - 1) : 0;
     const int step = flipud ? -1 : 1;
diff --git a/Source/Lib/ASM_NEON/highbd_pickrst_neon.c b/Source/Lib/ASM_NEON/highbd_pickrst_neon.c
index 31d2a9b89620cf3ca9001fe945fb768ff743a173..0ce3f8dbd734e1523a836b22a8334bb6df57d804 100644
--- a/Source/Lib/ASM_NEON/highbd_pickrst_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_pickrst_neon.c
@@ -771,10 +771,10 @@ static INLINE void compute_stats_win5_highbd_neon(const int16_t *const d, const
             int32x4_t deltas[WIENER_WIN_CHROMA * 2] = {vdupq_n_s32(0)};
             int16x8_t ds[WIENER_WIN_CHROMA * 2];
 
-            ds[0] = load_unaligned_s16_4x2(d_t + 0 * d_stride, width);
-            ds[1] = load_unaligned_s16_4x2(d_t + 1 * d_stride, width);
-            ds[2] = load_unaligned_s16_4x2(d_t + 2 * d_stride, width);
-            ds[3] = load_unaligned_s16_4x2(d_t + 3 * d_stride, width);
+            ds[0] = load_s16_4x2(d_t + 0 * d_stride, width);
+            ds[1] = load_s16_4x2(d_t + 1 * d_stride, width);
+            ds[2] = load_s16_4x2(d_t + 2 * d_stride, width);
+            ds[3] = load_s16_4x2(d_t + 3 * d_stride, width);
 
             step3_win5_neon(d_t + 4 * d_stride, d_stride, width, height, ds, deltas);
 
diff --git a/Source/Lib/ASM_NEON/highbd_variance_neon.c b/Source/Lib/ASM_NEON/highbd_variance_neon.c
index 3a0c28bdfc043b0c2771198fe9e545721d2a01bb..3faa74b00db730a2b8eebbe28f9d10f0ea9275bf 100644
--- a/Source/Lib/ASM_NEON/highbd_variance_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_variance_neon.c
@@ -50,8 +50,8 @@ static INLINE void highbd_variance_4xh_neon(const uint16_t *src_ptr, int src_str
 
     int i = h;
     do {
-        const uint16x8_t s = load_unaligned_u16_4x2(src_ptr, src_stride);
-        const uint16x8_t r = load_unaligned_u16_4x2(ref_ptr, ref_stride);
+        const uint16x8_t s = load_u16_4x2(src_ptr, src_stride);
+        const uint16x8_t r = load_u16_4x2(ref_ptr, ref_stride);
 
         int16x8_t diff = vreinterpretq_s16_u16(vsubq_u16(s, r));
         sum_s16        = vaddq_s16(sum_s16, diff);
diff --git a/Source/Lib/ASM_NEON/inter_prediction_neon.c b/Source/Lib/ASM_NEON/inter_prediction_neon.c
index 8e02afe55ed4bced13553aefb89be96183ea11a1..83054cff9aab1da7dfdfa5d501ac8a0ff193609b 100644
--- a/Source/Lib/ASM_NEON/inter_prediction_neon.c
+++ b/Source/Lib/ASM_NEON/inter_prediction_neon.c
@@ -166,8 +166,8 @@ static INLINE void diffwtd_mask_neon(uint8_t *mask, const bool inverse, const ui
     } else if (w == 4) {
         int i = 0;
         do {
-            uint8x16_t s0 = load_unaligned_u8q(src0, src0_stride);
-            uint8x16_t s1 = load_unaligned_u8q(src1, src1_stride);
+            uint8x16_t s0 = load_u8_4x4(src0, src0_stride);
+            uint8x16_t s1 = load_u8_4x4(src1, src1_stride);
 
             uint8x16_t diff = vshrq_n_u8(vabdq_u8(s0, s1), DIFF_FACTOR_LOG2);
             uint8x16_t m;
@@ -274,7 +274,7 @@ void svt_av1_calc_target_weighted_pred_left_neon(uint8_t is16bit, MacroBlockD *x
         // MI_SIZE = 4 so it's fine to do 4 rows at a time.
         int row = nb_mi_height * MI_SIZE;
         do {
-            uint8x16_t tmp_u8 = vcombine_u8(load_unaligned_u8_2x4(tmp, ctxt->tmp_stride), vdup_n_u8(0));
+            uint8x16_t tmp_u8 = vcombine_u8(load_u8_2x4(tmp, ctxt->tmp_stride), vdup_n_u8(0));
 
             int32x4_t tmp_lo = vreinterpretq_s32_u8(vqtbl1q_u8(tmp_u8, pre_idx0));
             int32x4_t tmp_hi = vreinterpretq_s32_u8(vqtbl1q_u8(tmp_u8, pre_idx1));
@@ -322,7 +322,7 @@ void svt_av1_calc_target_weighted_pred_left_neon(uint8_t is16bit, MacroBlockD *x
         int32x4_t m1  = vld1q_s32(mask1d1);
         int       row = nb_mi_height * MI_SIZE;
         do {
-            uint8x16_t tmp_u8 = load_unaligned_u8q(tmp, ctxt->tmp_stride);
+            uint8x16_t tmp_u8 = load_u8_4x4(tmp, ctxt->tmp_stride);
 
             int32x4_t tmp_s32[2];
             tmp_s32[0] = vreinterpretq_s32_u8(vqtbl1q_u8(tmp_u8, pre_idx0));
diff --git a/Source/Lib/ASM_NEON/intra_prediction_neon.c b/Source/Lib/ASM_NEON/intra_prediction_neon.c
index cf77057cef56db6836493b4d0fee5ffbbe8e2d04..f05c8e2516a8964b61e01d3312a5522979c4d654 100644
--- a/Source/Lib/ASM_NEON/intra_prediction_neon.c
+++ b/Source/Lib/ASM_NEON/intra_prediction_neon.c
@@ -1671,14 +1671,14 @@ void svt_av1_filter_intra_predictor_neon(uint8_t *dst, ptrdiff_t stride, TxSize
 /* ---------------------DC PREDICTOR--------------------------- */
 /* DC 4x4 */
 static INLINE uint16x8_t dc_load_sum_4(const uint8_t *in) {
-    const uint8x8_t  a  = load_u8_4x1_lane0(in);
+    const uint8x8_t  a  = load_u8_4x1(in);
     const uint16x4_t p0 = vpaddl_u8(a);
     const uint16x4_t p1 = vpadd_u16(p0, p0);
     return vcombine_u16(p1, vdup_n_u16(0));
 }
 
 static INLINE void dc_store_4xh(uint8_t *dst, ptrdiff_t stride, int h, uint8x8_t dc) {
-    for (int i = 0; i < h; ++i) { store_u8_4x1(dst + i * stride, dc, 0); }
+    for (int i = 0; i < h; ++i) { store_u8_4x1(dst + i * stride, dc); }
 }
 
 void svt_aom_dc_predictor_4x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
@@ -1920,7 +1920,7 @@ static INLINE int calculate_dc_from_sum(int bw, int bh, uint32_t sum, int shift1
 }
 
 void svt_aom_dc_predictor_4x8_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
-    uint8x8_t a   = load_u8_4x1_lane0(above);
+    uint8x8_t a   = load_u8_4x1(above);
     uint8x8_t l   = vld1_u8(left);
     uint32_t  sum = vaddlvq_u16(vaddl_u8(a, l));
     uint32_t  dc  = calculate_dc_from_sum(4, 8, sum, 2, DC_MULTIPLIER_1X2);
@@ -1929,14 +1929,14 @@ void svt_aom_dc_predictor_4x8_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t
 
 void svt_aom_dc_predictor_8x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
     uint8x8_t a   = vld1_u8(above);
-    uint8x8_t l   = load_u8_4x1_lane0(left);
+    uint8x8_t l   = load_u8_4x1(left);
     uint32_t  sum = vaddlvq_u16(vaddl_u8(a, l));
     uint32_t  dc  = calculate_dc_from_sum(8, 4, sum, 2, DC_MULTIPLIER_1X2);
     dc_store_8xh(dst, stride, 4, vdup_n_u8(dc));
 }
 
 void svt_aom_dc_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
-    uint8x8_t  a      = load_u8_4x1_lane0(above);
+    uint8x8_t  a      = load_u8_4x1(above);
     uint8x16_t l      = vld1q_u8(left);
     uint16x8_t sum_al = vaddw_u8(vpaddlq_u8(l), a);
     uint32_t   sum    = vaddlvq_u16(sum_al);
@@ -1946,7 +1946,7 @@ void svt_aom_dc_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride, const uint8_
 
 void svt_aom_dc_predictor_16x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
     uint8x16_t a      = vld1q_u8(above);
-    uint8x8_t  l      = load_u8_4x1_lane0(left);
+    uint8x8_t  l      = load_u8_4x1(left);
     uint16x8_t sum_al = vaddw_u8(vpaddlq_u8(a), l);
     uint32_t   sum    = vaddlvq_u16(sum_al);
     uint32_t   dc     = calculate_dc_from_sum(16, 4, sum, 2, DC_MULTIPLIER_1X4);
@@ -2118,18 +2118,16 @@ DC_PREDICTOR_TOP(64, 32, 6, q)
 /* 256 - v = vneg_s8(v) */
 static INLINE uint8x8_t negate_s8(const uint8x8_t v) { return vreinterpret_u8_s8(vneg_s8(vreinterpret_s8_u8(v))); }
 
-static void smooth_4xh_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *const top_row,
-                            const uint8_t *const left_column, const int height) {
-    uint8x8_t            top_v       = vdup_n_u8(0);
-    uint8x8_t            weights_x_v = vdup_n_u8(0);
+static INLINE void smooth_4xh_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *const top_row,
+                                   const uint8_t *const left_column, const int height) {
     const uint8_t        top_right   = top_row[3];
     const uint8_t        bottom_left = left_column[height - 1];
     const uint8_t *const weights_y   = sm_weight_arrays + height;
 
-    load_u8_4x1(top_row, &top_v, 0);
-    const uint8x8_t top_right_v   = vdup_n_u8(top_right);
-    const uint8x8_t bottom_left_v = vdup_n_u8(bottom_left);
-    load_u8_4x1(sm_weight_arrays + 4, &weights_x_v, 0);
+    uint8x8_t        top_v            = load_u8_4x1(top_row);
+    const uint8x8_t  top_right_v      = vdup_n_u8(top_right);
+    const uint8x8_t  bottom_left_v    = vdup_n_u8(bottom_left);
+    uint8x8_t        weights_x_v      = load_u8_4x1(sm_weight_arrays + 4);
     const uint8x8_t  scaled_weights_x = negate_s8(weights_x_v);
     const uint16x8_t weighted_tr      = vmull_u8(scaled_weights_x, top_right_v);
 
@@ -2167,8 +2165,8 @@ static INLINE uint8x8_t calculate_weights_and_pred(const uint8x8_t top, const ui
     return calculate_pred(weighted_top_bl, weighted_left_tr);
 }
 
-static void smooth_8xh_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *const top_row,
-                            const uint8_t *const left_column, const int height) {
+static INLINE void smooth_8xh_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *const top_row,
+                                   const uint8_t *const left_column, const int height) {
     const uint8_t        top_right   = top_row[7];
     const uint8_t        bottom_left = left_column[height - 1];
     const uint8_t *const weights_y   = sm_weight_arrays + height;
@@ -2232,12 +2230,12 @@ static INLINE uint8x16_t negate_s8q(const uint8x16_t v) {
 
 /* For width 16 and above. */
 #define SMOOTH_PREDICTOR_WIDE(W)                                                                                    \
-    static void smooth_##W##xh_wide_neon(uint8_t             *dst,                                                  \
-                                         ptrdiff_t            stride,                                               \
-                                         const uint8_t *const top_row,                                              \
-                                         const uint8_t *const left_column,                                          \
-                                         const int            height) {                                                        \
-        const uint8_t        top_right   = top_row[(W)-1];                                                          \
+    static INLINE void smooth_##W##xh_wide_neon(uint8_t             *dst,                                           \
+                                                ptrdiff_t            stride,                                        \
+                                                const uint8_t *const top_row,                                       \
+                                                const uint8_t *const left_column,                                   \
+                                                const int            height) {                                                 \
+        const uint8_t        top_right   = top_row[(W) - 1];                                                        \
         const uint8_t        bottom_left = left_column[height - 1];                                                 \
         const uint8_t *const weights_y   = sm_weight_arrays + height;                                               \
                                                                                                                     \
@@ -2343,12 +2341,12 @@ SMOOTH_NXM_WIDE(64, 64)
 
 /* For widths 4 and 8. */
 #define SMOOTH_H_PREDICTOR(W)                                                                                    \
-    static void smooth_h_##W##xh_neon(uint8_t             *dst,                                                  \
-                                      ptrdiff_t            stride,                                               \
-                                      const uint8_t *const top_row,                                              \
-                                      const uint8_t *const left_column,                                          \
-                                      const int            height) {                                                        \
-        const uint8_t top_right = top_row[(W)-1];                                                                \
+    static INLINE void smooth_h_##W##xh_neon(uint8_t             *dst,                                           \
+                                             ptrdiff_t            stride,                                        \
+                                             const uint8_t *const top_row,                                       \
+                                             const uint8_t *const left_column,                                   \
+                                             const int            height) {                                                 \
+        const uint8_t top_right = top_row[(W) - 1];                                                              \
                                                                                                                  \
         const uint8x8_t top_right_v = vdup_n_u8(top_right);                                                      \
         /* Over-reads for 4xN but still within the array. */                                                     \
@@ -2405,12 +2403,12 @@ static INLINE uint8x16_t calculate_horizontal_weights_and_pred(const uint8x8_t l
 
 /* For width 16 and above. */
 #define SMOOTH_H_PREDICTOR_WIDE(W)                                                   \
-    static void smooth_h_##W##xh_wide_neon(uint8_t             *dst,                 \
-                                           ptrdiff_t            stride,              \
-                                           const uint8_t *const top_row,             \
-                                           const uint8_t *const left_column,         \
-                                           const int            height) {                       \
-        const uint8_t top_right = top_row[(W)-1];                                    \
+    static INLINE void smooth_h_##W##xh_wide_neon(uint8_t             *dst,          \
+                                                  ptrdiff_t            stride,       \
+                                                  const uint8_t *const top_row,      \
+                                                  const uint8_t *const left_column,  \
+                                                  const int            height) {                \
+        const uint8_t top_right = top_row[(W) - 1];                                  \
                                                                                      \
         const uint8x8_t top_right_v = vdup_n_u8(top_right);                          \
                                                                                      \
@@ -2487,17 +2485,17 @@ SMOOTH_H_NXM_WIDE(64, 64)
 
 /* For widths 4 and 8. */
 #define SMOOTH_V_PREDICTOR(W)                                                                                   \
-    static void smooth_v_##W##xh_neon(uint8_t             *dst,                                                 \
-                                      ptrdiff_t            stride,                                              \
-                                      const uint8_t *const top_row,                                             \
-                                      const uint8_t *const left_column,                                         \
-                                      const int            height) {                                                       \
-        uint8x8_t            top_v       = vdup_n_u8(0);                                                        \
+    static INLINE void smooth_v_##W##xh_neon(uint8_t             *dst,                                          \
+                                             ptrdiff_t            stride,                                       \
+                                             const uint8_t *const top_row,                                      \
+                                             const uint8_t *const left_column,                                  \
+                                             const int            height) {                                                \
+        uint8x8_t            top_v;                                                                             \
         const uint8_t        bottom_left = left_column[height - 1];                                             \
         const uint8_t *const weights_y   = sm_weight_arrays + height;                                           \
                                                                                                                 \
         if ((W) == 4) {                                                                                         \
-            load_u8_4x1(top_row, &top_v, 0);                                                                    \
+            top_v = load_u8_4x1(top_row);                                                                       \
         } else { /* width == 8 */                                                                               \
             top_v = vld1_u8(top_row);                                                                           \
         }                                                                                                       \
@@ -2551,11 +2549,11 @@ static INLINE uint8x16_t calculate_vertical_weights_and_pred(const uint8x16_t to
 
 /* For width 16 and above. */
 #define SMOOTH_V_PREDICTOR_WIDE(W)                                                                                     \
-    static void smooth_v_##W##xh_wide_neon(uint8_t             *dst,                                                   \
-                                           ptrdiff_t            stride,                                                \
-                                           const uint8_t *const top_row,                                               \
-                                           const uint8_t *const left_column,                                           \
-                                           const int            height) {                                                         \
+    static INLINE void smooth_v_##W##xh_wide_neon(uint8_t             *dst,                                            \
+                                                  ptrdiff_t            stride,                                         \
+                                                  const uint8_t *const top_row,                                        \
+                                                  const uint8_t *const left_column,                                    \
+                                                  const int            height) {                                                  \
         const uint8_t        bottom_left = left_column[height - 1];                                                    \
         const uint8_t *const weights_y   = sm_weight_arrays + height;                                                  \
                                                                                                                        \
@@ -2623,7 +2621,7 @@ SMOOTH_V_NXM_WIDE(64, 64)
 
 /* ---------------------V PREDICTOR--------------------------- */
 static INLINE void v_store_4xh(uint8_t *dst, ptrdiff_t stride, int h, uint8x8_t d0) {
-    for (int i = 0; i < h; ++i) { store_u8_4x1(dst + i * stride, d0, 0); }
+    for (int i = 0; i < h; ++i) { store_u8_4x1(dst + i * stride, d0); }
 }
 
 static INLINE void v_store_8xh(uint8_t *dst, ptrdiff_t stride, int h, uint8x8_t d0) {
@@ -2655,7 +2653,7 @@ static INLINE void v_store_64xh(uint8_t *dst, ptrdiff_t stride, int h, uint8x16_
 
 void svt_aom_v_predictor_4x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
     (void)left;
-    v_store_4xh(dst, stride, 4, load_u8_4x1_lane0(above));
+    v_store_4xh(dst, stride, 4, load_u8_4x1(above));
 }
 
 void svt_aom_v_predictor_8x8_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
@@ -2677,12 +2675,12 @@ void svt_aom_v_predictor_32x32_neon(uint8_t *dst, ptrdiff_t stride, const uint8_
 
 void svt_aom_v_predictor_4x8_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
     (void)left;
-    v_store_4xh(dst, stride, 8, load_u8_4x1_lane0(above));
+    v_store_4xh(dst, stride, 8, load_u8_4x1(above));
 }
 
 void svt_aom_v_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
     (void)left;
-    v_store_4xh(dst, stride, 16, load_u8_4x1_lane0(above));
+    v_store_4xh(dst, stride, 16, load_u8_4x1(above));
 }
 
 void svt_aom_v_predictor_8x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
@@ -2770,14 +2768,14 @@ void svt_aom_v_predictor_64x64_neon(uint8_t *dst, ptrdiff_t stride, const uint8_
 
 /* ---------------------H PREDICTOR--------------------------- */
 static INLINE void h_store_4x8(uint8_t *dst, ptrdiff_t stride, uint8x8_t d0) {
-    store_u8_4x1(dst + 0 * stride, vdup_lane_u8(d0, 0), 0);
-    store_u8_4x1(dst + 1 * stride, vdup_lane_u8(d0, 1), 0);
-    store_u8_4x1(dst + 2 * stride, vdup_lane_u8(d0, 2), 0);
-    store_u8_4x1(dst + 3 * stride, vdup_lane_u8(d0, 3), 0);
-    store_u8_4x1(dst + 4 * stride, vdup_lane_u8(d0, 4), 0);
-    store_u8_4x1(dst + 5 * stride, vdup_lane_u8(d0, 5), 0);
-    store_u8_4x1(dst + 6 * stride, vdup_lane_u8(d0, 6), 0);
-    store_u8_4x1(dst + 7 * stride, vdup_lane_u8(d0, 7), 0);
+    store_u8_4x1(dst + 0 * stride, vdup_lane_u8(d0, 0));
+    store_u8_4x1(dst + 1 * stride, vdup_lane_u8(d0, 1));
+    store_u8_4x1(dst + 2 * stride, vdup_lane_u8(d0, 2));
+    store_u8_4x1(dst + 3 * stride, vdup_lane_u8(d0, 3));
+    store_u8_4x1(dst + 4 * stride, vdup_lane_u8(d0, 4));
+    store_u8_4x1(dst + 5 * stride, vdup_lane_u8(d0, 5));
+    store_u8_4x1(dst + 6 * stride, vdup_lane_u8(d0, 6));
+    store_u8_4x1(dst + 7 * stride, vdup_lane_u8(d0, 7));
 }
 
 static INLINE void h_store_8x8(uint8_t *dst, ptrdiff_t stride, uint8x8_t d0) {
@@ -2871,12 +2869,12 @@ static INLINE void h_store_64x8(uint8_t *dst, ptrdiff_t stride, uint8x8_t d0) {
 }
 
 void svt_aom_h_predictor_4x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
-    const uint8x8_t d0 = load_u8_4x1_lane0(left);
+    const uint8x8_t d0 = load_u8_4x1(left);
     (void)above;
-    store_u8_4x1(dst + 0 * stride, vdup_lane_u8(d0, 0), 0);
-    store_u8_4x1(dst + 1 * stride, vdup_lane_u8(d0, 1), 0);
-    store_u8_4x1(dst + 2 * stride, vdup_lane_u8(d0, 2), 0);
-    store_u8_4x1(dst + 3 * stride, vdup_lane_u8(d0, 3), 0);
+    store_u8_4x1(dst + 0 * stride, vdup_lane_u8(d0, 0));
+    store_u8_4x1(dst + 1 * stride, vdup_lane_u8(d0, 1));
+    store_u8_4x1(dst + 2 * stride, vdup_lane_u8(d0, 2));
+    store_u8_4x1(dst + 3 * stride, vdup_lane_u8(d0, 3));
 }
 
 void svt_aom_h_predictor_8x8_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
@@ -2916,7 +2914,7 @@ void svt_aom_h_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t
 }
 
 void svt_aom_h_predictor_8x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
-    const uint8x8_t d0 = load_u8_4x1_lane0(left);
+    const uint8x8_t d0 = load_u8_4x1(left);
     (void)above;
     vst1_u8(dst + 0 * stride, vdup_lane_u8(d0, 0));
     vst1_u8(dst + 1 * stride, vdup_lane_u8(d0, 1));
@@ -2942,7 +2940,7 @@ void svt_aom_h_predictor_8x32_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t
 }
 
 void svt_aom_h_predictor_16x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left) {
-    const uint8x8_t d0 = load_u8_4x1_lane0(left);
+    const uint8x8_t d0 = load_u8_4x1(left);
     (void)above;
     vst1q_u8(dst + 0 * stride, vdupq_lane_u8(d0, 0));
     vst1q_u8(dst + 1 * stride, vdupq_lane_u8(d0, 1));
@@ -3047,7 +3045,7 @@ static INLINE void paeth_4or8_x_h_neon(uint8_t *dest, ptrdiff_t stride, const ui
     const uint8x8_t  top_left    = vdup_n_u8(top_row[-1]);
     const uint16x8_t top_left_x2 = vdupq_n_u16(top_row[-1] + top_row[-1]);
     if (width == 4) {
-        load_u8_4x1(top_row, &top, 0);
+        top = load_u8_4x1(top_row);
     } else {
         top = vld1_u8(top_row);
     }
@@ -3071,7 +3069,7 @@ static INLINE void paeth_4or8_x_h_neon(uint8_t *dest, ptrdiff_t stride, const ui
         result                           = vbsl_u8(left_or_top_mask, result, top_left);
 
         if (width == 4) {
-            store_unaligned_u8_4x1(dest, result, 0);
+            store_u8_4x1(dest, result);
         } else {
             vst1_u8(dest, result);
         }
diff --git a/Source/Lib/ASM_NEON/mem_neon.h b/Source/Lib/ASM_NEON/mem_neon.h
index 014384e251ae7ef564640f638a8a5334c601d572..3bb26b883b250e7933163a0de8fed42343b8953c 100644
--- a/Source/Lib/ASM_NEON/mem_neon.h
+++ b/Source/Lib/ASM_NEON/mem_neon.h
@@ -46,18 +46,27 @@ static INLINE uint8x16_t load_u8_8x2(const uint8_t *s, ptrdiff_t p) { return vco
 
 /* These intrinsics require immediate values, so we must use #defines
    to enforce that. */
-#define load_u8_4x1(s, s0, lane) \
+#define load_u8_4x1_lane(s, s0, lane) \
     do { *(s0) = vreinterpret_u8_u32(vld1_lane_u32((uint32_t *)(s), vreinterpret_u32_u8(*(s0)), lane)); } while (0)
-#define load_u16_2x1(s, s0, lane) \
-    do { *(s0) = vreinterpret_u16_u32(vld1_lane_u32((uint32_t *)(s), vreinterpret_u32_u16(*(s0)), lane)); } while (0)
 
 // Load four bytes into the low half of a uint8x8_t, zero the upper half.
-static INLINE uint8x8_t load_u8_4x1_lane0(const uint8_t *p) {
+static INLINE uint8x8_t load_u8_4x1(const uint8_t *p) {
     uint8x8_t ret = vdup_n_u8(0);
-    load_u8_4x1(p, &ret, 0);
+    load_u8_4x1_lane(p, &ret, 0);
     return ret;
 }
 
+// Load two blocks of 32-bits into a single vector.
+static INLINE uint8x8_t load_u8x4_strided_x2(uint8_t *src, ptrdiff_t stride) {
+    uint8x8_t ret = vdup_n_u8(0);
+    load_u8_4x1_lane(src, &ret, 0);
+    src += stride;
+    load_u8_4x1_lane(src, &ret, 1);
+    return ret;
+}
+
+#undef load_u8_4x1_lane
+
 static INLINE void load_u8_8x8(const uint8_t *s, ptrdiff_t p, uint8x8_t *const s0, uint8x8_t *const s1,
                                uint8x8_t *const s2, uint8x8_t *const s3, uint8x8_t *const s4, uint8x8_t *const s5,
                                uint8x8_t *const s6, uint8x8_t *const s7) {
@@ -450,14 +459,6 @@ static INLINE void load_s16_4x4(const int16_t *s, ptrdiff_t p, int16x4_t *const
     *s3 = vld1_s16(s);
 }
 
-/* These intrinsics require immediate values, so we must use #defines
-   to enforce that. */
-#define store_u8_2x1(s, s0, lane) \
-    do { vst1_lane_u16((uint16_t *)(s), vreinterpret_u16_u8(s0), lane); } while (0)
-
-#define store_u8_4x1(s, s0, lane) \
-    do { vst1_lane_u32((uint32_t *)(s), vreinterpret_u32_u8(s0), lane); } while (0)
-
 static INLINE void store_u8_8x8(uint8_t *s, ptrdiff_t p, const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2,
                                 const uint8x8_t s3, const uint8x8_t s4, const uint8x8_t s5, const uint8x8_t s6,
                                 const uint8x8_t s7) {
@@ -943,14 +944,13 @@ static INLINE void load_s16_8x3(const int16_t *s, ptrdiff_t p, int16x8_t *const
     *s2 = vld1q_s16(s);
 }
 
-#define load_unaligned_u32_2x1_lane(v, p, lane) \
+#define load_u32_2x1_lane(v, p, lane) \
     do { (v) = vld1_lane_u32((const uint32_t *)(p), (v), (lane)); } while (0)
-
-#define load_unaligned_u32_4x1_lane(v, p, lane) \
+#define load_u32_4x1_lane(v, p, lane) \
     do { (v) = vld1q_lane_u32((const uint32_t *)(p), (v), (lane)); } while (0)
 
-// Load 2 sets of 4 bytes when alignment is not guaranteed.
-static INLINE uint8x8_t load_unaligned_u8(const uint8_t *buf, int stride) {
+// Load 2 sets of 4 bytes.
+static INLINE uint8x8_t load_u8_4x2(const uint8_t *buf, int stride) {
     uint32_t a;
     memcpy(&a, buf, 4);
     buf += stride;
@@ -960,8 +960,8 @@ static INLINE uint8x8_t load_unaligned_u8(const uint8_t *buf, int stride) {
     return vreinterpret_u8_u32(a_u32);
 }
 
-// Load 4 sets of 4 bytes when alignment is not guaranteed.
-static INLINE uint8x16_t load_unaligned_u8q(const uint8_t *buf, int stride) {
+// Load 4 sets of 4 bytes.
+static INLINE uint8x16_t load_u8_4x4(const uint8_t *buf, int stride) {
     uint32_t   a;
     uint32x4_t a_u32;
     if (stride == 4)
@@ -980,7 +980,7 @@ static INLINE uint8x16_t load_unaligned_u8q(const uint8_t *buf, int stride) {
     return vreinterpretq_u8_u32(a_u32);
 }
 
-static INLINE uint8x8_t load_unaligned_u8_2x2(const uint8_t *buf, int stride) {
+static INLINE uint8x8_t load_u8_2x2(const uint8_t *buf, int stride) {
     uint16_t   a;
     uint16x4_t a_u16;
 
@@ -992,7 +992,7 @@ static INLINE uint8x8_t load_unaligned_u8_2x2(const uint8_t *buf, int stride) {
     return vreinterpret_u8_u16(a_u16);
 }
 
-static INLINE uint8x8_t load_unaligned_u8_2x4(const uint8_t *buf, int stride) {
+static INLINE uint8x8_t load_u8_2x4(const uint8_t *buf, int stride) {
     uint16_t   a;
     uint16x4_t a_u16;
 
@@ -1010,17 +1010,7 @@ static INLINE uint8x8_t load_unaligned_u8_2x4(const uint8_t *buf, int stride) {
     return vreinterpret_u8_u16(a_u16);
 }
 
-static INLINE uint8x8_t load_unaligned_u8_4x1(const uint8_t *buf) {
-    uint32_t   a;
-    uint32x2_t a_u32;
-
-    memcpy(&a, buf, 4);
-    a_u32 = vdup_n_u32(0);
-    a_u32 = vset_lane_u32(a, a_u32, 0);
-    return vreinterpret_u8_u32(a_u32);
-}
-
-static INLINE uint8x8_t load_unaligned_dup_u8_4x2(const uint8_t *buf) {
+static INLINE uint8x8_t load_dup_u8_4x2(const uint8_t *buf) {
     uint32_t   a;
     uint32x2_t a_u32;
 
@@ -1029,7 +1019,7 @@ static INLINE uint8x8_t load_unaligned_dup_u8_4x2(const uint8_t *buf) {
     return vreinterpret_u8_u32(a_u32);
 }
 
-static INLINE uint8x8_t load_unaligned_dup_u8_2x4(const uint8_t *buf) {
+static INLINE uint8x8_t load_dup_u8_2x4(const uint8_t *buf) {
     uint16_t   a;
     uint16x4_t a_u32;
 
@@ -1038,68 +1028,12 @@ static INLINE uint8x8_t load_unaligned_dup_u8_2x4(const uint8_t *buf) {
     return vreinterpret_u8_u16(a_u32);
 }
 
-static INLINE uint8x8_t load_unaligned_u8_4x2(const uint8_t *buf, int stride) {
-    uint32_t   a;
-    uint32x2_t a_u32;
-
-    memcpy(&a, buf, 4);
-    buf += stride;
-    a_u32 = vdup_n_u32(a);
-    memcpy(&a, buf, 4);
-    a_u32 = vset_lane_u32(a, a_u32, 1);
-    return vreinterpret_u8_u32(a_u32);
-}
-
-static INLINE void load_unaligned_u8_4x4(const uint8_t *buf, int stride, uint8x8_t *tu0, uint8x8_t *tu1) {
-    *tu0 = load_unaligned_u8_4x2(buf, stride);
+static INLINE void load_u8_4x2x2(const uint8_t *buf, int stride, uint8x8_t *tu0, uint8x8_t *tu1) {
+    *tu0 = load_u8_4x2(buf, stride);
     buf += 2 * stride;
-    *tu1 = load_unaligned_u8_4x2(buf, stride);
-}
-
-static INLINE void load_unaligned_u8_3x8(const uint8_t *buf, int stride, uint8x8_t *tu0, uint8x8_t *tu1,
-                                         uint8x8_t *tu2) {
-    load_unaligned_u8_4x4(buf, stride, tu0, tu1);
-    buf += 4 * stride;
-    *tu2 = load_unaligned_u8_4x2(buf, stride);
-}
-
-static INLINE void load_unaligned_u8_4x8(const uint8_t *buf, int stride, uint8x8_t *tu0, uint8x8_t *tu1, uint8x8_t *tu2,
-                                         uint8x8_t *tu3) {
-    load_unaligned_u8_4x4(buf, stride, tu0, tu1);
-    buf += 4 * stride;
-    load_unaligned_u8_4x4(buf, stride, tu2, tu3);
+    *tu1 = load_u8_4x2(buf, stride);
 }
 
-/* These intrinsics require immediate values, so we must use #defines
-   to enforce that. */
-#define store_unaligned_u8_4x1(dst, src, lane)             \
-    do {                                                   \
-        uint32_t a;                                        \
-        a = vget_lane_u32(vreinterpret_u32_u8(src), lane); \
-        memcpy(dst, &a, 4);                                \
-    } while (0)
-
-#define store_unaligned_u8_2x1(dst, src, lane)             \
-    do {                                                   \
-        uint16_t a;                                        \
-        a = vget_lane_u16(vreinterpret_u16_u8(src), lane); \
-        memcpy(dst, &a, 2);                                \
-    } while (0)
-
-#define store_unaligned_u16_2x1(dst, src, lane)             \
-    do {                                                    \
-        uint32_t a;                                         \
-        a = vget_lane_u32(vreinterpret_u32_u16(src), lane); \
-        memcpy(dst, &a, 4);                                 \
-    } while (0)
-
-#define store_unaligned_u16_4x1(dst, src, lane)               \
-    do {                                                      \
-        uint64_t a;                                           \
-        a = vgetq_lane_u64(vreinterpretq_u64_u16(src), lane); \
-        memcpy(dst, &a, 8);                                   \
-    } while (0)
-
 static INLINE void load_u8_16x8(const uint8_t *s, ptrdiff_t p, uint8x16_t *const s0, uint8x16_t *const s1,
                                 uint8x16_t *const s2, uint8x16_t *const s3, uint8x16_t *const s4, uint8x16_t *const s5,
                                 uint8x16_t *const s6, uint8x16_t *const s7) {
@@ -1188,7 +1122,7 @@ static INLINE void load_u16_16x4(const uint16_t *s, ptrdiff_t p, uint16x8_t *con
     *s7 = vld1q_u16(s + 8);
 }
 
-static INLINE uint16x4_t load_unaligned_u16_2x2(const uint16_t *buf, int stride) {
+static INLINE uint16x4_t load_u16_2x2(const uint16_t *buf, int stride) {
     uint32_t   a;
     uint32x2_t a_u32;
 
@@ -1200,15 +1134,7 @@ static INLINE uint16x4_t load_unaligned_u16_2x2(const uint16_t *buf, int stride)
     return vreinterpret_u16_u32(a_u32);
 }
 
-static INLINE uint16x4_t load_unaligned_u16_4x1(const uint16_t *buf) {
-    uint64_t   a;
-    uint64x1_t a_u64 = vdup_n_u64(0);
-    memcpy(&a, buf, 8);
-    a_u64 = vset_lane_u64(a, a_u64, 0);
-    return vreinterpret_u16_u64(a_u64);
-}
-
-static INLINE uint16x8_t load_unaligned_u16_4x2(const uint16_t *buf, uint32_t stride) {
+static INLINE uint16x8_t load_u16_4x2(const uint16_t *buf, uint32_t stride) {
     uint64_t   a;
     uint64x2_t a_u64;
 
@@ -1222,7 +1148,7 @@ static INLINE uint16x8_t load_unaligned_u16_4x2(const uint16_t *buf, uint32_t st
     return vreinterpretq_u16_u64(a_u64);
 }
 
-static INLINE int16x8_t load_unaligned_s16_4x2(const int16_t *buf, uint32_t stride) {
+static INLINE int16x8_t load_s16_4x2(const int16_t *buf, uint32_t stride) {
     int64_t   a;
     int64x2_t a_s64;
 
@@ -1236,12 +1162,6 @@ static INLINE int16x8_t load_unaligned_s16_4x2(const int16_t *buf, uint32_t stri
     return vreinterpretq_s16_s64(a_s64);
 }
 
-static INLINE void load_unaligned_u16_4x4(const uint16_t *buf, uint32_t stride, uint16x8_t *tu0, uint16x8_t *tu1) {
-    *tu0 = load_unaligned_u16_4x2(buf, stride);
-    buf += 2 * stride;
-    *tu1 = load_unaligned_u16_4x2(buf, stride);
-}
-
 static INLINE int32x4_t load_s32_2x2(int32_t *s, int stride) { return vcombine_s32(vld1_s32(s), vld1_s32(s + stride)); }
 
 static INLINE void load_s32_4x2(int32_t *s, int32_t p, int32x4_t *s1, int32x4_t *s2) {
@@ -1355,30 +1275,6 @@ static INLINE void store_s16_to_tran_low(tran_low_t *buf, const int16x4_t a) {
     vst1q_s32(buf, v0);
 }
 
-static INLINE void store_unaligned_u8_2x2(uint8_t *dst, uint32_t dst_stride, uint8x8_t src) {
-    store_unaligned_u8_2x1(dst, src, 0);
-    dst += dst_stride;
-    store_unaligned_u8_2x1(dst, src, 1);
-}
-
-static INLINE void store_unaligned_u8_4x2(uint8_t *dst, uint32_t dst_stride, uint8x8_t src) {
-    store_unaligned_u8_4x1(dst, src, 0);
-    dst += dst_stride;
-    store_unaligned_u8_4x1(dst, src, 1);
-}
-
-static INLINE void store_unaligned_u16_2x2(uint16_t *dst, uint32_t dst_stride, uint16x4_t src) {
-    store_unaligned_u16_2x1(dst, src, 0);
-    dst += dst_stride;
-    store_unaligned_u16_2x1(dst, src, 1);
-}
-
-static INLINE void store_unaligned_u16_4x2(uint16_t *dst, uint32_t dst_stride, uint16x8_t src) {
-    store_unaligned_u16_4x1(dst, src, 0);
-    dst += dst_stride;
-    store_unaligned_u16_4x1(dst, src, 1);
-}
-
 // The `lane` parameter here must be an immediate.
 #define store_u8_2x1_lane(dst, src, lane)                           \
     do {                                                            \
@@ -1392,6 +1288,12 @@ static INLINE void store_unaligned_u16_4x2(uint16_t *dst, uint32_t dst_stride, u
         memcpy(dst, &a, 4);                                         \
     } while (0)
 
+// Store the low 16-bits from a single vector.
+static INLINE void store_u8_2x1(uint8_t *dst, const uint8x8_t src) { store_u8_2x1_lane(dst, src, 0); }
+
+// Store the low 32-bits from a single vector.
+static INLINE void store_u8_4x1(uint8_t *dst, const uint8x8_t src) { store_u8_4x1_lane(dst, src, 0); }
+
 // Store two blocks of 32-bits from a single vector.
 static INLINE void store_u8x4_strided_x2(uint8_t *dst, ptrdiff_t stride, uint8x8_t src) {
     store_u8_4x1_lane(dst, src, 0);
@@ -1400,7 +1302,7 @@ static INLINE void store_u8x4_strided_x2(uint8_t *dst, ptrdiff_t stride, uint8x8
 }
 
 // Store two blocks of 16-bits from a single vector.
-static inline void store_u8x2_strided_x2(uint8_t *dst, uint32_t dst_stride, uint8x8_t src) {
+static INLINE void store_u8x2_strided_x2(uint8_t *dst, uint32_t dst_stride, uint8x8_t src) {
     store_u8_2x1_lane(dst, src, 0);
     dst += dst_stride;
     store_u8_2x1_lane(dst, src, 1);
@@ -1417,14 +1319,14 @@ static INLINE void store_s16x4_strided_x2(int16_t *dst, int32_t dst_stride, int1
 }
 
 // Store two blocks of 32-bits from a single vector.
-static inline void store_u16x2_strided_x2(uint16_t *dst, uint32_t dst_stride, uint16x4_t src) {
+static INLINE void store_u16x2_strided_x2(uint16_t *dst, uint32_t dst_stride, uint16x4_t src) {
     store_u16_2x1_lane(dst, src, 0);
     dst += dst_stride;
     store_u16_2x1_lane(dst, src, 1);
 }
 
 // Store two blocks of 32-bits from a single vector.
-static inline void store_s16x2_strided_x2(int16_t *dst, uint32_t dst_stride, int16x4_t src) {
+static INLINE void store_s16x2_strided_x2(int16_t *dst, uint32_t dst_stride, int16x4_t src) {
     store_s16_2x1_lane(dst, src, 0);
     dst += dst_stride;
     store_s16_2x1_lane(dst, src, 1);
diff --git a/Source/Lib/ASM_NEON/obmc_sad_neon.c b/Source/Lib/ASM_NEON/obmc_sad_neon.c
index ae742bc778a283bf563ee60988b7b37c7b847ddf..de5633ff1ffd44340471df6d8a55296329a42958 100644
--- a/Source/Lib/ASM_NEON/obmc_sad_neon.c
+++ b/Source/Lib/ASM_NEON/obmc_sad_neon.c
@@ -134,7 +134,7 @@ static INLINE unsigned int obmc_sad_4xh_neon(const uint8_t *ref, int ref_stride,
 
     int h = height / 2;
     do {
-        uint8x8_t r = load_unaligned_u8(ref, ref_stride);
+        uint8x8_t r = load_u8_4x2(ref, ref_stride);
 
         int16x8_t ref_s16 = vreinterpretq_s16_u16(vmovl_u8(r));
         obmc_sad_8x1_s16_neon(ref_s16, mask, wsrc, &sum);
diff --git a/Source/Lib/ASM_NEON/obmc_variance_neon.c b/Source/Lib/ASM_NEON/obmc_variance_neon.c
index e9e5362fa0d8f3b8e139dab1b4bc1a99caef081b..30992fe773d8d598d960cbb5693d682c2aafb781 100644
--- a/Source/Lib/ASM_NEON/obmc_variance_neon.c
+++ b/Source/Lib/ASM_NEON/obmc_variance_neon.c
@@ -172,7 +172,7 @@ static INLINE void obmc_variance_neon_4xh(const uint8_t *pre, int pre_stride, co
     int32x4_t sumv = vdupq_n_s32(0);
 
     do {
-        uint8x8_t pre_u8  = load_unaligned_u8(pre, pre_stride);
+        uint8x8_t pre_u8  = load_u8_4x2(pre, pre_stride);
         int16x8_t pre_s16 = vreinterpretq_s16_u16(vmovl_u8(pre_u8));
 
         obmc_variance_8x1_s16_neon(pre_s16, wsrc, mask, &ssev, &sumv);
diff --git a/Source/Lib/ASM_NEON/pickrst_neon.c b/Source/Lib/ASM_NEON/pickrst_neon.c
index 2b7f36b57b95acc349be53271c6bbb4bcd6e262f..3cd0314c085ed7dbb7f94d46366532fede76fbaa 100644
--- a/Source/Lib/ASM_NEON/pickrst_neon.c
+++ b/Source/Lib/ASM_NEON/pickrst_neon.c
@@ -652,10 +652,10 @@ static INLINE void compute_stats_win5_neon(const int16_t *const d, const int32_t
             int32x4_t deltas[WIENER_WIN_CHROMA * 2] = {vdupq_n_s32(0)};
             int16x8_t ds[WIENER_WIN_CHROMA * 2];
 
-            ds[0] = load_unaligned_s16_4x2(d_t + 0 * d_stride, width);
-            ds[1] = load_unaligned_s16_4x2(d_t + 1 * d_stride, width);
-            ds[2] = load_unaligned_s16_4x2(d_t + 2 * d_stride, width);
-            ds[3] = load_unaligned_s16_4x2(d_t + 3 * d_stride, width);
+            ds[0] = load_s16_4x2(d_t + 0 * d_stride, width);
+            ds[1] = load_s16_4x2(d_t + 1 * d_stride, width);
+            ds[2] = load_s16_4x2(d_t + 2 * d_stride, width);
+            ds[3] = load_s16_4x2(d_t + 3 * d_stride, width);
 
             step3_win5_neon(d_t + 4 * d_stride, d_stride, width, height, ds, deltas);
 
diff --git a/Source/Lib/ASM_NEON/pickrst_neon.h b/Source/Lib/ASM_NEON/pickrst_neon.h
index 38e2570c3bbaf5b841ab1543b0f7bae65a743d82..a36c24b9ae8b40f56eba902117cb855cc5c8bc82 100644
--- a/Source/Lib/ASM_NEON/pickrst_neon.h
+++ b/Source/Lib/ASM_NEON/pickrst_neon.h
@@ -406,8 +406,8 @@ static INLINE void step3_win5_neon(const int16_t *d, const int32_t d_stride, con
                                    int16x8_t *ds, int32x4_t *deltas) {
     int32_t y = height;
     do {
-        ds[4] = load_unaligned_s16_4x2(d + 0 * d_stride, width);
-        ds[5] = load_unaligned_s16_4x2(d + 1 * d_stride, width);
+        ds[4] = load_s16_4x2(d + 0 * d_stride, width);
+        ds[5] = load_s16_4x2(d + 1 * d_stride, width);
 
         compute_delta_step3_two_lines(&deltas[0], ds[0], ds[0]);
         compute_delta_step3_two_lines(&deltas[1], ds[0], ds[1]);
diff --git a/Source/Lib/ASM_NEON/sad_neon.c b/Source/Lib/ASM_NEON/sad_neon.c
index ccfc8b130a60265aace417f5d454087b204f4ca6..0104e3496d6c75a5e0b5e6c90fcf940de19749fd 100644
--- a/Source/Lib/ASM_NEON/sad_neon.c
+++ b/Source/Lib/ASM_NEON/sad_neon.c
@@ -56,14 +56,13 @@ static AOM_FORCE_INLINE uint32x4_t compute8xh_sad_kernel_dual_neon(uint8_t *rest
 }
 
 /*******************************************
-Calculate SAD for 16x16 and its 8x8 sublcoks
-and check if there is improvment, if yes keep
-the best SAD+MV
+Calculate SAD for 16x16 and its 8x8 sublocks and check if there is an
+improvement, if yes keep the best SAD+MV.
 *******************************************/
-void svt_ext_sad_calculation_8x8_16x16_neon_intrin(uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride,
-                                                   uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16,
-                                                   uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16, uint32_t mv,
-                                                   uint32_t *p_sad16x16, uint32_t *p_sad8x8, bool sub_sad) {
+void svt_ext_sad_calculation_8x8_16x16_neon(uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride,
+                                            uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16,
+                                            uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16, uint32_t mv,
+                                            uint32_t *p_sad16x16, uint32_t *p_sad8x8, bool sub_sad) {
     uint32_t   sad16x16;
     uint32x4_t sad;
     uint32x4_t best_sad_vec = vld1q_u32(p_best_sad_8x8);
@@ -106,12 +105,10 @@ void svt_ext_sad_calculation_8x8_16x16_neon_intrin(uint8_t *src, uint32_t src_st
 /*******************************************
  * svt_ext_eight_sad_calculation_8x8_16x16_neon
  *******************************************/
-static void svt_ext_eight_sad_calculation_8x8_16x16_neon(uint8_t *src, uint32_t src_stride, uint8_t *ref,
-                                                         uint32_t ref_stride, uint32_t mv, uint32_t start_16x16_pos,
-                                                         uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16,
-                                                         uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
-                                                         uint32_t p_eight_sad16x16[16][8],
-                                                         uint32_t p_eight_sad8x8[64][8], bool sub_sad) {
+static INLINE void svt_ext_eight_sad_calculation_8x8_16x16_neon(
+    uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride, uint32_t mv, uint32_t start_16x16_pos,
+    uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16, uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
+    uint32_t p_eight_sad16x16[16][8], uint32_t p_eight_sad8x8[64][8], bool sub_sad) {
     (void)p_eight_sad8x8;
     const uint32_t start_8x8_pos = 4 * start_16x16_pos;
 
diff --git a/Source/Lib/ASM_NEON/sse_neon.c b/Source/Lib/ASM_NEON/sse_neon.c
index 20cd000169c42b2adb1cf81fec30383eab0f9449..2773a60604a4ecdf512d5571ad034dee5e455ddb 100644
--- a/Source/Lib/ASM_NEON/sse_neon.c
+++ b/Source/Lib/ASM_NEON/sse_neon.c
@@ -38,8 +38,8 @@ static INLINE void sse_8x1_neon(const uint8_t *src, const uint8_t *ref, uint32x4
 
 static INLINE void sse_4x2_neon(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride,
                                 uint32x4_t *sse) {
-    uint8x8_t s = load_unaligned_u8(src, src_stride);
-    uint8x8_t r = load_unaligned_u8(ref, ref_stride);
+    uint8x8_t s = load_u8_4x2(src, src_stride);
+    uint8x8_t r = load_u8_4x2(ref, ref_stride);
 
     uint8x8_t abs_diff = vabd_u8(s, r);
 
diff --git a/Source/Lib/ASM_NEON/transpose_neon.h b/Source/Lib/ASM_NEON/transpose_neon.h
index de0f7305f7ceb373395011fc72ef0b6622b1bc7e..8740f9f191d5d93b19d23ccd0d8015a936298c66 100644
--- a/Source/Lib/ASM_NEON/transpose_neon.h
+++ b/Source/Lib/ASM_NEON/transpose_neon.h
@@ -15,7 +15,7 @@
 #include <arm_neon.h>
 #include "definitions.h"
 
-static inline void transpose_elems_u8_8x8(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2, uint8x8_t a3, uint8x8_t a4,
+static INLINE void transpose_elems_u8_8x8(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2, uint8x8_t a3, uint8x8_t a4,
                                           uint8x8_t a5, uint8x8_t a6, uint8x8_t a7, uint8x8_t *o0, uint8x8_t *o1,
                                           uint8x8_t *o2, uint8x8_t *o3, uint8x8_t *o4, uint8x8_t *o5, uint8x8_t *o6,
                                           uint8x8_t *o7) {
@@ -141,7 +141,7 @@ static INLINE void transpose_elems_inplace_u8_8x4(uint8x8_t *a0, uint8x8_t *a1,
     *a3 = vreinterpret_u8_u16(c1.val[1]);
 }
 
-static inline void transpose_elems_inplace_s16_8x4(int16x8_t *a0, int16x8_t *a1, int16x8_t *a2, int16x8_t *a3) {
+static INLINE void transpose_elems_inplace_s16_8x4(int16x8_t *a0, int16x8_t *a1, int16x8_t *a2, int16x8_t *a3) {
     // Swap 16 bit elements. Goes from:
     // a0: 00 01 02 03 04 05 06 07
     // a1: 10 11 12 13 14 15 16 17
@@ -171,7 +171,7 @@ static inline void transpose_elems_inplace_s16_8x4(int16x8_t *a0, int16x8_t *a1,
     *a3 = vreinterpretq_s16_s32(c1.val[1]);
 }
 
-static inline void transpose_elems_inplace_u8_16x4(uint8x16_t *a0, uint8x16_t *a1, uint8x16_t *a2, uint8x16_t *a3) {
+static INLINE void transpose_elems_inplace_u8_16x4(uint8x16_t *a0, uint8x16_t *a1, uint8x16_t *a2, uint8x16_t *a3) {
     // Swap 8 bit elements. Goes from:
     // a0: 00 01 02 03 04 05 06 07 08 09 010 011 012 013 014 015
     // a1: 10 11 12 13 14 15 16 17 18 19 110 111 112 113 114 115
diff --git a/Source/Lib/ASM_NEON/var_filter_neon.h b/Source/Lib/ASM_NEON/var_filter_neon.h
index 43dd7e41a99aa322fd6f61301c45ea03d03bd5ff..99658697f2e3b650925e87ec8dd7e90e30d1c01e 100644
--- a/Source/Lib/ASM_NEON/var_filter_neon.h
+++ b/Source/Lib/ASM_NEON/var_filter_neon.h
@@ -17,15 +17,15 @@
 #include "sum_neon.h"
 #include "aom_dsp_rtcd.h"
 
-static void var_filter_block2d_bil_w4(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                      int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_w4(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                             int dst_height, int filter_offset) {
     const uint8x8_t f0 = vdup_n_u8(8 - filter_offset);
     const uint8x8_t f1 = vdup_n_u8(filter_offset);
 
     int i = dst_height;
     do {
-        uint8x8_t  s0      = load_unaligned_u8(src_ptr, src_stride);
-        uint8x8_t  s1      = load_unaligned_u8(src_ptr + pixel_step, src_stride);
+        uint8x8_t  s0      = load_u8_4x2(src_ptr, src_stride);
+        uint8x8_t  s1      = load_u8_4x2(src_ptr + pixel_step, src_stride);
         uint16x8_t blend   = vmull_u8(s0, f0);
         blend              = vmlal_u8(blend, s1, f1);
         uint8x8_t blend_u8 = vrshrn_n_u16(blend, 3);
@@ -37,8 +37,8 @@ static void var_filter_block2d_bil_w4(const uint8_t *src_ptr, uint8_t *dst_ptr,
     } while (i != 0);
 }
 
-static void var_filter_block2d_bil_w8(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                      int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_w8(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                             int dst_height, int filter_offset) {
     const uint8x8_t f0 = vdup_n_u8(8 - filter_offset);
     const uint8x8_t f1 = vdup_n_u8(filter_offset);
 
@@ -56,8 +56,8 @@ static void var_filter_block2d_bil_w8(const uint8_t *src_ptr, uint8_t *dst_ptr,
     } while (--i != 0);
 }
 
-static void var_filter_block2d_bil_large(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                         int dst_width, int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_large(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride,
+                                                int pixel_step, int dst_width, int dst_height, int filter_offset) {
     const uint8x8_t f0 = vdup_n_u8(8 - filter_offset);
     const uint8x8_t f1 = vdup_n_u8(filter_offset);
 
@@ -82,28 +82,28 @@ static void var_filter_block2d_bil_large(const uint8_t *src_ptr, uint8_t *dst_pt
     } while (--i != 0);
 }
 
-static void var_filter_block2d_bil_w16(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                       int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_w16(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                              int dst_height, int filter_offset) {
     var_filter_block2d_bil_large(src_ptr, dst_ptr, src_stride, pixel_step, 16, dst_height, filter_offset);
 }
 
-static void var_filter_block2d_bil_w32(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                       int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_w32(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                              int dst_height, int filter_offset) {
     var_filter_block2d_bil_large(src_ptr, dst_ptr, src_stride, pixel_step, 32, dst_height, filter_offset);
 }
 
-static void var_filter_block2d_bil_w64(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                       int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_w64(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                              int dst_height, int filter_offset) {
     var_filter_block2d_bil_large(src_ptr, dst_ptr, src_stride, pixel_step, 64, dst_height, filter_offset);
 }
 
-static void var_filter_block2d_bil_w128(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                        int dst_height, int filter_offset) {
+static INLINE void var_filter_block2d_bil_w128(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                               int dst_height, int filter_offset) {
     var_filter_block2d_bil_large(src_ptr, dst_ptr, src_stride, pixel_step, 128, dst_height, filter_offset);
 }
 
-static void var_filter_block2d_avg(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
-                                   int dst_width, int dst_height) {
+static INLINE void var_filter_block2d_avg(const uint8_t *src_ptr, uint8_t *dst_ptr, int src_stride, int pixel_step,
+                                          int dst_width, int dst_height) {
     // We only specialise on the filter values for large block sizes (>= 16x16.)
     assert(dst_width >= 16 && dst_width % 16 == 0);
 
diff --git a/Source/Lib/ASM_NEON/variance_neon.c b/Source/Lib/ASM_NEON/variance_neon.c
index ecaa0d2730dcb16f40ffc9f9dd3fc5c5166f076d..0af0e54796baf8a63b57ed806539f8358043cf32 100644
--- a/Source/Lib/ASM_NEON/variance_neon.c
+++ b/Source/Lib/ASM_NEON/variance_neon.c
@@ -27,8 +27,8 @@ static INLINE void variance_4xh_neon(const uint8_t *src, int src_stride, const u
 
     int i = h;
     do {
-        uint8x8_t s    = load_unaligned_u8(src, src_stride);
-        uint8x8_t r    = load_unaligned_u8(ref, ref_stride);
+        uint8x8_t s    = load_u8_4x2(src, src_stride);
+        uint8x8_t r    = load_u8_4x2(ref, ref_stride);
         int16x8_t diff = vreinterpretq_s16_u16(vsubl_u8(s, r));
 
         sum_s16 = vaddq_s16(sum_s16, diff);
diff --git a/Source/Lib/ASM_NEON_DOTPROD/convolve_scale_neon_dotprod.c b/Source/Lib/ASM_NEON_DOTPROD/convolve_scale_neon_dotprod.c
index 608352b5012e473280e727d011b8d771afe36454..dc3e050a145bd8f16d8b07a1801e34b688506977 100644
--- a/Source/Lib/ASM_NEON_DOTPROD/convolve_scale_neon_dotprod.c
+++ b/Source/Lib/ASM_NEON_DOTPROD/convolve_scale_neon_dotprod.c
@@ -27,7 +27,7 @@ DECLARE_ALIGNED(16, static const uint8_t, kScale2DotProdPermuteTbl[32]) = {
 };
 // clang-format on
 
-static inline int16x4_t convolve8_4_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
+static INLINE int16x4_t convolve8_4_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
                                       const int8x8_t filter, const int32x4_t horiz_const) {
     const int8x16_t filters = vcombine_s8(filter, filter);
 
@@ -47,7 +47,7 @@ static inline int16x4_t convolve8_4_h(const uint8x8_t s0, const uint8x8_t s1, co
     return vshrn_n_s32(sum, ROUND0_BITS - 1);
 }
 
-static inline int16x8_t convolve8_8_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
+static INLINE int16x8_t convolve8_8_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
                                       const uint8x8_t s4, const uint8x8_t s5, const uint8x8_t s6, const uint8x8_t s7,
                                       const int8x8_t filter, const int32x4_t horiz_const) {
     const int8x16_t filters = vcombine_s8(filter, filter);
@@ -75,7 +75,7 @@ static inline int16x8_t convolve8_8_h(const uint8x8_t s0, const uint8x8_t s1, co
     return vcombine_s16(vshrn_n_s32(sum0123, ROUND0_BITS - 1), vshrn_n_s32(sum4567, ROUND0_BITS - 1));
 }
 
-static inline void convolve_horiz_scale_neon_dotprod(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_neon_dotprod(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                      int w, int h, const int16_t *x_filter, const int subpel_x_qn,
                                                      const int x_step_qn) {
     DECLARE_ALIGNED(16, int16_t, temp[8 * 8]);
@@ -168,7 +168,7 @@ static inline void convolve_horiz_scale_neon_dotprod(const uint8_t *src, int src
     }
 }
 
-static inline int16x4_t convolve8_4_h_scale_2(uint8x16_t samples, const int8x8_t filters, const int32x4_t horiz_const,
+static INLINE int16x4_t convolve8_4_h_scale_2(uint8x16_t samples, const int8x8_t filters, const int32x4_t horiz_const,
                                               const uint8x16x2_t permute_tbl) {
     // Transform sample range to [-128, 127] for 8-bit signed dot product.
     int8x16_t samples_128 = vreinterpretq_s8_u8(vsubq_u8(samples, vdupq_n_u8(128)));
@@ -186,7 +186,7 @@ static inline int16x4_t convolve8_4_h_scale_2(uint8x16_t samples, const int8x8_t
     return vshrn_n_s32(sum, ROUND0_BITS - 1);
 }
 
-static inline int16x8_t convolve8_8_h_scale_2(uint8x16_t samples[2], const int8x8_t filters,
+static INLINE int16x8_t convolve8_8_h_scale_2(uint8x16_t samples[2], const int8x8_t filters,
                                               const int32x4_t horiz_const, const uint8x16x2_t permute_tbl) {
     // Transform sample range to [-128, 127] for 8-bit signed dot product.
     int8x16_t samples0_128 = vreinterpretq_s8_u8(vsubq_u8(samples[0], vdupq_n_u8(128)));
@@ -210,7 +210,7 @@ static inline int16x8_t convolve8_8_h_scale_2(uint8x16_t samples[2], const int8x
     return vcombine_s16(vshrn_n_s32(sum0123, ROUND0_BITS - 1), vshrn_n_s32(sum4567, ROUND0_BITS - 1));
 }
 
-static inline void convolve_horiz_scale_2_neon_dotprod(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_2_neon_dotprod(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                        int w, int h, const int16_t *x_filter) {
     const int bd = 8;
     // A shim of 1 << (ROUND0_BITS - 1) enables us to use non-rounding
diff --git a/Source/Lib/ASM_NEON_DOTPROD/sad_neon_dotprod.c b/Source/Lib/ASM_NEON_DOTPROD/sad_neon_dotprod.c
index 19e0572bf3c03e42ff2b1ab2ee45dfc7719e4d2e..aff4442353688ae3aba447f54d5bc3e3465bac41 100644
--- a/Source/Lib/ASM_NEON_DOTPROD/sad_neon_dotprod.c
+++ b/Source/Lib/ASM_NEON_DOTPROD/sad_neon_dotprod.c
@@ -57,7 +57,7 @@ void svt_ext_sad_calculation_8x8_16x16_neon_dotprod(uint8_t *src, uint32_t src_s
     vst1q_u32(p_best_mv8x8, best_mv_vec);
 }
 
-static void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_1_neon_dotprod(
+static INLINE void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_1_neon_dotprod(
     uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride, uint32_t mv, uint32_t start_16x16_pos,
     uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16, uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
     uint32_t p_eight_sad16x16[16][8]) {
@@ -100,7 +100,7 @@ static void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_1_neon_dotprod(
     vst1q_u32(p_best_mv8x8, best_mv_vec);
 }
 
-static void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_0_neon_dotprod(
+static INLINE void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_0_neon_dotprod(
     uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride, uint32_t mv, uint32_t start_16x16_pos,
     uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16, uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
     uint32_t p_eight_sad16x16[16][8]) {
diff --git a/Source/Lib/ASM_NEON_DOTPROD/sse_neon_dotprod.c b/Source/Lib/ASM_NEON_DOTPROD/sse_neon_dotprod.c
index 9d2b74b4a894f17469b3c6e64596994990cf6c51..6dba8535c66e1e0536f5a5d4c70c4c5c500d0e5d 100644
--- a/Source/Lib/ASM_NEON_DOTPROD/sse_neon_dotprod.c
+++ b/Source/Lib/ASM_NEON_DOTPROD/sse_neon_dotprod.c
@@ -34,8 +34,8 @@ static INLINE void sse_8x1_neon_dotprod(const uint8_t *src, const uint8_t *ref,
 
 static INLINE void sse_4x2_neon_dotprod(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride,
                                         uint32x2_t *sse) {
-    uint8x8_t s = load_unaligned_u8(src, src_stride);
-    uint8x8_t r = load_unaligned_u8(ref, ref_stride);
+    uint8x8_t s = load_u8_4x2(src, src_stride);
+    uint8x8_t r = load_u8_4x2(ref, ref_stride);
 
     uint8x8_t abs_diff = vabd_u8(s, r);
 
diff --git a/Source/Lib/ASM_NEON_DOTPROD/variance_neon_dotprod.c b/Source/Lib/ASM_NEON_DOTPROD/variance_neon_dotprod.c
index c211295f7ffb9534add8d41e27c84741a8900f4b..8d92da70b3478ddd4ea625bdfca35b1c2197ca99 100644
--- a/Source/Lib/ASM_NEON_DOTPROD/variance_neon_dotprod.c
+++ b/Source/Lib/ASM_NEON_DOTPROD/variance_neon_dotprod.c
@@ -21,8 +21,8 @@ static INLINE void variance_4xh_neon_dotprod(const uint8_t *src, int src_stride,
     uint32x4_t sse_u32 = vdupq_n_u32(0);
 
     do {
-        uint8x16_t s = load_unaligned_u8q(src, src_stride);
-        uint8x16_t r = load_unaligned_u8q(ref, ref_stride);
+        uint8x16_t s = load_u8_4x4(src, src_stride);
+        uint8x16_t r = load_u8_4x4(ref, ref_stride);
 
         src_sum = vdotq_u32(src_sum, s, vdupq_n_u8(1));
         ref_sum = vdotq_u32(ref_sum, r, vdupq_n_u8(1));
diff --git a/Source/Lib/ASM_NEON_I8MM/aom_convolve8_neon_i8mm.c b/Source/Lib/ASM_NEON_I8MM/aom_convolve8_neon_i8mm.c
index 79c47c57b53a537c26f45e6ce0fed37c2a5d8e12..370afdde9f3d6261125f152ca6487a2f540ff2da 100644
--- a/Source/Lib/ASM_NEON_I8MM/aom_convolve8_neon_i8mm.c
+++ b/Source/Lib/ASM_NEON_I8MM/aom_convolve8_neon_i8mm.c
@@ -242,7 +242,7 @@ void svt_aom_convolve8_horiz_neon_i8mm(const uint8_t *src, ptrdiff_t src_stride,
     }
 }
 
-static inline void transpose_concat_4x4(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2, uint8x8_t a3, uint8x16_t *b) {
+static INLINE void transpose_concat_4x4(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2, uint8x8_t a3, uint8x16_t *b) {
     // Transpose 8-bit elements and concatenate result rows as follows:
     // a0: 00, 01, 02, 03, XX, XX, XX, XX
     // a1: 10, 11, 12, 13, XX, XX, XX, XX
@@ -262,7 +262,7 @@ static inline void transpose_concat_4x4(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2
     *b = vzipq_u8(a02, a13).val[0];
 }
 
-static inline void transpose_concat_8x4(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2, uint8x8_t a3, uint8x16_t *b0,
+static INLINE void transpose_concat_8x4(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2, uint8x8_t a3, uint8x16_t *b0,
                                         uint8x16_t *b1) {
     // Transpose 8-bit elements and concatenate result rows as follows:
     // a0: 00, 01, 02, 03, 04, 05, 06, 07
@@ -287,7 +287,7 @@ static inline void transpose_concat_8x4(uint8x8_t a0, uint8x8_t a1, uint8x8_t a2
     *b1 = a0123.val[1];
 }
 
-static inline int16x4_t convolve8_4_v(const uint8x16_t samples_lo, const uint8x16_t samples_hi,
+static INLINE int16x4_t convolve8_4_v(const uint8x16_t samples_lo, const uint8x16_t samples_hi,
                                       const int8x8_t filters) {
     // Sample permutation is performed by the caller.
     int32x4_t sum = vusdotq_lane_s32(vdupq_n_s32(0), samples_lo, filters, 0);
@@ -297,7 +297,7 @@ static inline int16x4_t convolve8_4_v(const uint8x16_t samples_lo, const uint8x1
     return vqmovn_s32(sum);
 }
 
-static inline uint8x8_t convolve8_8_v(const uint8x16_t samples0_lo, const uint8x16_t samples0_hi,
+static INLINE uint8x8_t convolve8_8_v(const uint8x16_t samples0_lo, const uint8x16_t samples0_hi,
                                       const uint8x16_t samples1_lo, const uint8x16_t samples1_hi,
                                       const int8x8_t filters) {
     // Sample permutation is performed by the caller.
@@ -314,7 +314,7 @@ static inline uint8x8_t convolve8_8_v(const uint8x16_t samples0_lo, const uint8x
     return vqrshrun_n_s16(sum, FILTER_BITS);
 }
 
-static inline void convolve8_vert_8tap_neon_i8mm(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+static INLINE void convolve8_vert_8tap_neon_i8mm(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
                                                  ptrdiff_t dst_stride, const int16_t *filter_y, int w, int h) {
     const int8x8_t     filter          = vmovn_s16(vld1q_s16(filter_y));
     const uint8x16x3_t merge_block_tbl = vld1q_u8_x3(kDotProdMergeBlockTbl);
diff --git a/Source/Lib/ASM_NEON_I8MM/convolve_scale_neon_i8mm.c b/Source/Lib/ASM_NEON_I8MM/convolve_scale_neon_i8mm.c
index d229fd308483ed593299b1176b8e360b1b0393dc..d202dd986e2dcd8d47dd428ee95eb5676f063dd6 100644
--- a/Source/Lib/ASM_NEON_I8MM/convolve_scale_neon_i8mm.c
+++ b/Source/Lib/ASM_NEON_I8MM/convolve_scale_neon_i8mm.c
@@ -27,7 +27,7 @@ DECLARE_ALIGNED(16, static const uint8_t, kScale2DotProdPermuteTbl[32]) = {
 };
 // clang-format on
 
-static inline int16x4_t convolve8_4_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
+static INLINE int16x4_t convolve8_4_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
                                       const int8x8_t filter, const int32x4_t horiz_const) {
     const int8x16_t filters = vcombine_s8(filter, filter);
 
@@ -43,7 +43,7 @@ static inline int16x4_t convolve8_4_h(const uint8x8_t s0, const uint8x8_t s1, co
     return vshrn_n_s32(sum, ROUND0_BITS - 1);
 }
 
-static inline int16x8_t convolve8_8_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
+static INLINE int16x8_t convolve8_8_h(const uint8x8_t s0, const uint8x8_t s1, const uint8x8_t s2, const uint8x8_t s3,
                                       const uint8x8_t s4, const uint8x8_t s5, const uint8x8_t s6, const uint8x8_t s7,
                                       const int8x8_t filter, const int32x4_t horiz_const) {
     const int8x16_t filters = vcombine_s8(filter, filter);
@@ -65,7 +65,7 @@ static inline int16x8_t convolve8_8_h(const uint8x8_t s0, const uint8x8_t s1, co
     return vcombine_s16(vshrn_n_s32(sum0123, ROUND0_BITS - 1), vshrn_n_s32(sum4567, ROUND0_BITS - 1));
 }
 
-static inline void convolve_horiz_scale_neon_i8mm(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_neon_i8mm(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                   int w, int h, const int16_t *x_filter, const int subpel_x_qn,
                                                   const int x_step_qn) {
     DECLARE_ALIGNED(16, int16_t, temp[8 * 8]);
@@ -153,7 +153,7 @@ static inline void convolve_horiz_scale_neon_i8mm(const uint8_t *src, int src_st
     }
 }
 
-static inline int16x4_t convolve8_4_h_scale_2(uint8x16_t samples, const int8x8_t filters, const int32x4_t horiz_const,
+static INLINE int16x4_t convolve8_4_h_scale_2(uint8x16_t samples, const int8x8_t filters, const int32x4_t horiz_const,
                                               const uint8x16x2_t permute_tbl) {
     // Permute samples ready for dot product.
     // { 0, 1, 2, 3, 2, 3, 4, 5, 4, 5,  6,  7,  6,  7,  8,  9 }
@@ -167,7 +167,7 @@ static inline int16x4_t convolve8_4_h_scale_2(uint8x16_t samples, const int8x8_t
     return vshrn_n_s32(sum, ROUND0_BITS - 1);
 }
 
-static inline int16x8_t convolve8_8_h_scale_2(uint8x16_t samples[2], const int8x8_t filters,
+static INLINE int16x8_t convolve8_8_h_scale_2(uint8x16_t samples[2], const int8x8_t filters,
                                               const int32x4_t horiz_const, const uint8x16x2_t permute_tbl) {
     // Permute samples ready for dot product.
     // { 0, 1, 2, 3, 2, 3, 4, 5, 4, 5,  6,  7,  6,  7,  8,  9 }
@@ -187,7 +187,7 @@ static inline int16x8_t convolve8_8_h_scale_2(uint8x16_t samples[2], const int8x
     return vcombine_s16(vshrn_n_s32(sum0123, ROUND0_BITS - 1), vshrn_n_s32(sum4567, ROUND0_BITS - 1));
 }
 
-static inline void convolve_horiz_scale_2_neon_i8mm(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
+static INLINE void convolve_horiz_scale_2_neon_i8mm(const uint8_t *src, int src_stride, int16_t *dst, int dst_stride,
                                                     int w, int h, const int16_t *x_filter) {
     const int bd = 8;
     // A shim of 1 << (ROUND0_BITS - 1) enables us to use non-rounding
diff --git a/Source/Lib/ASM_SVE/highbd_sse_sve.c b/Source/Lib/ASM_SVE/highbd_sse_sve.c
index 8b180f9d846f67dfa89560b3a247e3906df7b6ca..aced98f302d4cf6df9b600f80d86d52ae9430c93 100644
--- a/Source/Lib/ASM_SVE/highbd_sse_sve.c
+++ b/Source/Lib/ASM_SVE/highbd_sse_sve.c
@@ -137,8 +137,8 @@ static INLINE int64_t highbd_sse_4xh_sve(const uint16_t *src, int src_stride, co
     uint64x2_t sse = vdupq_n_u64(0);
 
     do {
-        uint16x8_t s = load_unaligned_u16_4x2(src, src_stride);
-        uint16x8_t r = load_unaligned_u16_4x2(ref, ref_stride);
+        uint16x8_t s = load_u16_4x2(src, src_stride);
+        uint16x8_t r = load_u16_4x2(ref, ref_stride);
 
         uint16x8_t abs_diff = vabdq_u16(s, r);
         sse                 = svt_udotq_u16(sse, abs_diff, abs_diff);
diff --git a/Source/Lib/ASM_SVE/highbd_variance_sve.c b/Source/Lib/ASM_SVE/highbd_variance_sve.c
index bb95aa0dd70764a81dd7613807df69f0bd2f02f8..6d4a31fc47735a6ece90eb2e752e4cd3eb6b2455 100644
--- a/Source/Lib/ASM_SVE/highbd_variance_sve.c
+++ b/Source/Lib/ASM_SVE/highbd_variance_sve.c
@@ -21,8 +21,8 @@ static INLINE void highbd_variance_4xh_sve(const uint16_t *src_ptr, int src_stri
     int64x2_t sse_s64 = vdupq_n_s64(0);
 
     do {
-        const uint16x8_t s = load_unaligned_u16_4x2(src_ptr, src_stride);
-        const uint16x8_t r = load_unaligned_u16_4x2(ref_ptr, ref_stride);
+        const uint16x8_t s = load_u16_4x2(src_ptr, src_stride);
+        const uint16x8_t r = load_u16_4x2(ref_ptr, ref_stride);
 
         int16x8_t diff = vreinterpretq_s16_u16(vsubq_u16(s, r));
         sum_s16        = vaddq_s16(sum_s16, diff);
diff --git a/Source/Lib/ASM_SVE/pickrst_sve.c b/Source/Lib/ASM_SVE/pickrst_sve.c
index a5877859dc372c845886b6cfeb291b9121507e41..3441fa95668fa3e6ec5f2779f1b8ef2fa489239c 100644
--- a/Source/Lib/ASM_SVE/pickrst_sve.c
+++ b/Source/Lib/ASM_SVE/pickrst_sve.c
@@ -24,7 +24,7 @@
 #include "transpose_neon.h"
 #include "utility.h"
 
-static inline uint8_t find_average_sve(const uint8_t *src, int src_stride, int width, int height) {
+static INLINE uint8_t find_average_sve(const uint8_t *src, int src_stride, int width, int height) {
     uint32x4_t avg_u32 = vdupq_n_u32(0);
     uint8x16_t ones    = vdupq_n_u8(1);
 
@@ -50,7 +50,7 @@ static inline uint8_t find_average_sve(const uint8_t *src, int src_stride, int w
     return (uint8_t)(vaddlvq_u32(avg_u32) / (width * height));
 }
 
-static inline void compute_sub_avg(const uint8_t *buf, int buf_stride, int avg, int16_t *buf_avg, int buf_avg_stride,
+static INLINE void compute_sub_avg(const uint8_t *buf, int buf_stride, int avg, int16_t *buf_avg, int buf_avg_stride,
                                    int width, int height) {
     uint8x8_t avg_u8 = vdup_n_u8(avg);
 
diff --git a/Source/Lib/ASM_SVE/pickrst_sve.h b/Source/Lib/ASM_SVE/pickrst_sve.h
index 176c9aa18291c0175a4f5f757e256bcafe694886..ffdb48029b3b3f46b2e2f067918f60d6f560b733 100644
--- a/Source/Lib/ASM_SVE/pickrst_sve.h
+++ b/Source/Lib/ASM_SVE/pickrst_sve.h
@@ -877,10 +877,10 @@ static INLINE void compute_stats_win5_sve(const int16_t *const d, const int32_t
             int32x4_t deltas[WIENER_WIN_CHROMA * 2] = {vdupq_n_s32(0)};
             int16x8_t ds[WIENER_WIN_CHROMA * 2];
 
-            ds[0] = load_unaligned_s16_4x2(d_t + 0 * d_stride, width);
-            ds[1] = load_unaligned_s16_4x2(d_t + 1 * d_stride, width);
-            ds[2] = load_unaligned_s16_4x2(d_t + 2 * d_stride, width);
-            ds[3] = load_unaligned_s16_4x2(d_t + 3 * d_stride, width);
+            ds[0] = load_s16_4x2(d_t + 0 * d_stride, width);
+            ds[1] = load_s16_4x2(d_t + 1 * d_stride, width);
+            ds[2] = load_s16_4x2(d_t + 2 * d_stride, width);
+            ds[3] = load_s16_4x2(d_t + 3 * d_stride, width);
 
             step3_win5_neon(d_t + 4 * d_stride, d_stride, width, height, ds, deltas);
 
diff --git a/Source/Lib/ASM_SVE/sad_sve.c b/Source/Lib/ASM_SVE/sad_sve.c
index f58b654cbd42d6c30d6e6b0566a95b7ec90f649f..69e2dda7dd2ad124e7a01493a5d8bf8a5564e5d7 100644
--- a/Source/Lib/ASM_SVE/sad_sve.c
+++ b/Source/Lib/ASM_SVE/sad_sve.c
@@ -18,7 +18,7 @@
 #include "sad_neon_dotprod.h"
 #include "sum_neon.h"
 
-static void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_1_neon_dotprod(
+static INLINE void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_1_neon_dotprod(
     uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride, uint32_t mv, uint32_t start_16x16_pos,
     uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16, uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
     uint32_t p_eight_sad16x16[16][8]) {
@@ -107,7 +107,7 @@ static void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_1_neon_dotprod(
     vst1q_u32(p_best_mv8x8, best_mv_vec);
 }
 
-static void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_0_neon_dotprod(
+static INLINE void svt_ext_eight_sad_calculation_8x8_16x16_sub_sad_0_neon_dotprod(
     uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride, uint32_t mv, uint32_t start_16x16_pos,
     uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16, uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
     uint32_t p_eight_sad16x16[16][8]) {
diff --git a/Source/Lib/ASM_SVE2/highbd_convolve_sve2.c b/Source/Lib/ASM_SVE2/highbd_convolve_sve2.c
index e7676fc61e9ab2713b88f1466b4f32f587f09307..47ce7f27ca85b24fbf2a45cb3a70463b05b4da03 100644
--- a/Source/Lib/ASM_SVE2/highbd_convolve_sve2.c
+++ b/Source/Lib/ASM_SVE2/highbd_convolve_sve2.c
@@ -55,9 +55,9 @@ static INLINE uint16x8_t highbd_convolve8_8_y(int16x8_t samples_lo[4], int16x8_t
     return vminq_u16(res, max);
 }
 
-static void highbd_convolve_y_sr_8tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
-                                           ptrdiff_t dst_stride, int width, int height, const int16_t *filter_y,
-                                           int bd) {
+static INLINE void highbd_convolve_y_sr_8tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
+                                                  ptrdiff_t dst_stride, int width, int height, const int16_t *filter_y,
+                                                  int bd) {
     assert(width >= 4 && height >= 4);
 
     const int16x8_t y_filter = vld1q_s16(filter_y);
@@ -215,9 +215,9 @@ static INLINE uint16x8_t highbd_convolve4_8_y(int16x8_t samples[4], int16x8_t fi
     return vminq_u16(res, max);
 }
 
-static void highbd_convolve_y_sr_4tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
-                                           ptrdiff_t dst_stride, int width, int height, const int16_t *filter_y,
-                                           int bd) {
+static INLINE void highbd_convolve_y_sr_4tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
+                                                  ptrdiff_t dst_stride, int width, int height, const int16_t *filter_y,
+                                                  int bd) {
     assert(width >= 4 && height >= 4);
 
     const int16x8_t y_filter = vcombine_s16(vld1_s16(filter_y + 2), vdup_n_s16(0));
@@ -534,9 +534,9 @@ static INLINE uint16x8_t highbd_convolve8_8_2d_v(int16x8_t samples_lo[4], int16x
     return vminq_u16(res, max);
 }
 
-static void highbd_convolve_2d_sr_vert_8tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
-                                                 ptrdiff_t dst_stride, int width, int height, const int16_t *filter_y,
-                                                 int bd, const int y_offset) {
+static INLINE void highbd_convolve_2d_sr_vert_8tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
+                                                        ptrdiff_t dst_stride, int width, int height,
+                                                        const int16_t *filter_y, int bd, const int y_offset) {
     assert(width >= 4 && height >= 4);
     const int64x2_t offset   = vdupq_n_s64(y_offset);
     const int16x8_t y_filter = vld1q_s16(filter_y);
@@ -700,9 +700,9 @@ static INLINE uint16x8_t highbd_convolve4_8_2d_v(int16x8_t samples[4], int16x8_t
     return vminq_u16(res, max);
 }
 
-static void highbd_convolve_2d_sr_vert_4tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
-                                                 ptrdiff_t dst_stride, int width, int height, const int16_t *filter_y,
-                                                 int bd, const int y_offset) {
+static INLINE void highbd_convolve_2d_sr_vert_4tap_sve2(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst,
+                                                        ptrdiff_t dst_stride, int width, int height,
+                                                        const int16_t *filter_y, int bd, const int y_offset) {
     assert(width >= 4 && height >= 4);
     const int64x2_t offset = vdupq_n_s64(y_offset);
 
diff --git a/Source/Lib/Codec/aom_dsp_rtcd.c b/Source/Lib/Codec/aom_dsp_rtcd.c
index 3203138991dda48cf566c8f918628d68bb364b0b..0d4ff4b114a3515f799c7b83f3a6708ebb684a0c 100644
--- a/Source/Lib/Codec/aom_dsp_rtcd.c
+++ b/Source/Lib/Codec/aom_dsp_rtcd.c
@@ -859,7 +859,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(apply_filtering_central, svt_aom_apply_filtering_central_c, svt_aom_apply_filtering_central_neon);
     SET_NEON(apply_filtering_central_highbd, svt_aom_apply_filtering_central_highbd_c, svt_aom_apply_filtering_central_highbd_neon);
     SET_NEON(downsample_2d, svt_aom_downsample_2d_c, svt_aom_downsample_2d_neon);
-    SET_NEON_NEON_DOTPROD(svt_ext_sad_calculation_8x8_16x16, svt_ext_sad_calculation_8x8_16x16_c, svt_ext_sad_calculation_8x8_16x16_neon_intrin, svt_ext_sad_calculation_8x8_16x16_neon_dotprod);
+    SET_NEON_NEON_DOTPROD(svt_ext_sad_calculation_8x8_16x16, svt_ext_sad_calculation_8x8_16x16_c, svt_ext_sad_calculation_8x8_16x16_neon, svt_ext_sad_calculation_8x8_16x16_neon_dotprod);
     SET_NEON(svt_ext_sad_calculation_32x32_64x64, svt_ext_sad_calculation_32x32_64x64_c, svt_ext_sad_calculation_32x32_64x64_neon);
     SET_NEON_NEON_DOTPROD_SVE(svt_ext_all_sad_calculation_8x8_16x16, svt_ext_all_sad_calculation_8x8_16x16_c, svt_ext_all_sad_calculation_8x8_16x16_neon, svt_ext_all_sad_calculation_8x8_16x16_neon_dotprod, svt_ext_all_sad_calculation_8x8_16x16_sve);
     SET_NEON(svt_ext_eight_sad_calculation_32x32_64x64, svt_ext_eight_sad_calculation_32x32_64x64_c, svt_ext_eight_sad_calculation_32x32_64x64_neon);
diff --git a/Source/Lib/Codec/aom_dsp_rtcd.h b/Source/Lib/Codec/aom_dsp_rtcd.h
index 2900749f1c35dcfdc0ecf1ab94d11469fe8d91bd..d945d55f426a72601958f3e06c566240b7ddb842 100644
--- a/Source/Lib/Codec/aom_dsp_rtcd.h
+++ b/Source/Lib/Codec/aom_dsp_rtcd.h
@@ -919,7 +919,7 @@ extern "C" {
     void svt_av1_compute_stats_sve(int32_t wiener_win, const uint8_t *dgd8, const uint8_t *src8, int32_t h_start, int32_t h_end, int32_t v_start, int32_t v_end, int32_t dgd_stride, int32_t src_stride, int64_t *M, int64_t *H);
     void svt_compute_interm_var_four8x8_neon(uint8_t *input_samples, uint16_t input_stride, uint64_t *mean_of8x8_blocks, uint64_t *mean_of_squared8x8_blocks);
     void svt_compute_interm_var_four8x8_neon_dotprod(uint8_t *input_samples, uint16_t input_stride, uint64_t *mean_of8x8_blocks, uint64_t *mean_of_squared8x8_blocks);
-    void svt_ext_sad_calculation_8x8_16x16_neon_intrin(uint8_t *src, uint32_t src_stride, uint8_t *ref,
+    void svt_ext_sad_calculation_8x8_16x16_neon(uint8_t *src, uint32_t src_stride, uint8_t *ref,
         uint32_t ref_stride, uint32_t *p_best_sad_8x8,
         uint32_t *p_best_sad_16x16, uint32_t *p_best_mv8x8,
         uint32_t *p_best_mv16x16, uint32_t mv,
diff --git a/test/SadTest.cc b/test/SadTest.cc
index adb7b7b23fd00618797a3a1987148336d2119758..fa74f896ad1c5e4b5ea956bb679697abf59836d6 100644
--- a/test/SadTest.cc
+++ b/test/SadTest.cc
@@ -1125,7 +1125,7 @@ INSTANTIATE_TEST_SUITE_P(
     ::testing::Combine(
         ::testing::ValuesIn(TEST_PATTERNS),
         ::testing::ValuesIn(TEST_SAD_PATTERNS),
-        ::testing::Values(svt_ext_sad_calculation_8x8_16x16_neon_intrin)));
+        ::testing::Values(svt_ext_sad_calculation_8x8_16x16_neon)));
 
 #if HAVE_NEON_DOTPROD
 INSTANTIATE_TEST_SUITE_P(
